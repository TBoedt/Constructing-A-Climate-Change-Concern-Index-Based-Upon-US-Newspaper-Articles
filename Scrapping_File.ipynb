{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ead0048c",
   "metadata": {},
   "source": [
    "# 0. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf5116da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "from datetime import date, timedelta\n",
    "import random\n",
    "import threading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07db45c8",
   "metadata": {},
   "source": [
    "# 1. Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6145d3",
   "metadata": {},
   "source": [
    "## 1.1. Loop Through Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0251376",
   "metadata": {},
   "outputs": [],
   "source": [
    "def daterange(start_date, end_date):\n",
    "    for n in range(int((end_date - start_date).days) + 1):\n",
    "        yield start_date + timedelta(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5b3e66",
   "metadata": {},
   "source": [
    "# 2. Scraping News Media"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ea0b00",
   "metadata": {},
   "source": [
    "## 2.1. Wall Street Journal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e6cb2f",
   "metadata": {},
   "source": [
    "### 2.1.1. Scraping Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3a991733",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWallStreetJournal(username, password, start_date, end_date):\n",
    "    #put dates in date format\n",
    "    start_date = datetime.strptime(start_date, \"%d/%m/%Y\")\n",
    "    end_date = datetime.strptime(end_date, \"%d/%m/%Y\")\n",
    "    \n",
    "    #Define list of variables to store \n",
    "    WSJ_article_content = []\n",
    "    WSJ_date = []\n",
    "    WSJ_link = []\n",
    "    WSJ_title = []\n",
    "    \n",
    "    #Go to the website\n",
    "    fireFoxOptions = webdriver.FirefoxOptions()\n",
    "    fireFoxOptions.add_argument('--ignore-certificate-errors')\n",
    "    fireFoxOptions.add_argument('--allow-running-insecure-content')\n",
    "    fireFoxOptions.add_argument(\"--headless\")\n",
    "    fireFoxOptions.add_argument(\"--disable-gpu\")\n",
    "    fireFoxOptions.add_argument(\"--no-sandbox\")\n",
    "    #fireFoxOptions.add_argument(\"-width=1800\");\n",
    "    #fireFoxOptions.add_argument(\"-height=6000\");\n",
    "    user_agent = 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.50 Safari/537.36'\n",
    "    fireFoxOptions.add_argument(f'user-agent={user_agent}')\n",
    "    driver_WSJ = webdriver.Firefox(options=fireFoxOptions)\n",
    "    #driver_WSJ = webdriver.Firefox()\n",
    "    driver_WSJ.get(\"https://sso.accounts.dowjones.com/login-page?op=localop&scope=openid%20idp_id%20roles%20email%20given_name%20family_name%20djid%20djUsername%20djStatus%20trackid%20tags%20prts%20suuid%20createTimestamp&client_id=5hssEAdMy0mJTICnJNvC9TXEw3Va7jfO&response_type=code&redirect_uri=https%3A%2F%2Faccounts.wsj.com%2Fauth%2Fsso%2Flogin&nonce=ab6a473a-cfa6-4714-8fad-b6dff98f5f18&ui_locales=en-us-x-wsj-223-2&mars=-1&ns=prod%2Faccounts-wsj&state=8rChOTDzC_Y_AK-i.TJAixN_XjsWxwUEEPoHg2OPCaX6qRBu4nGSk5fqLliY4H0B5F7gj_57-XH-YBWGS&protocol=oauth2&client=5hssEAdMy0mJTICnJNvC9TXEw3Va7jfO#!/signin\")\n",
    "    \n",
    "    driver_WSJ.get_screenshot_as_file(\"login2.png\")\n",
    "    \n",
    "    #put in user_name:\n",
    "    time.sleep(3) #makes sure field is fully loaded\n",
    "    username_field = driver_WSJ.find_element(By.XPATH, \"/html/body/div[1]/div[2]/div/div/div/div[1]/div[1]/form/div[2]/div[1]/div[2]/input\")\n",
    "    username_field.send_keys(username)\n",
    "    \n",
    "    #continue to password:\n",
    "    driver_WSJ.find_element(By.XPATH, \"/html/body/div[1]/div[2]/div/div/div/div[1]/div[1]/form/div[2]/div[6]/div[1]/button[2]\").click()\n",
    "\n",
    "    #input password:\n",
    "    time.sleep(3) #makes sure field is fully loaded\n",
    "    password_field = driver_WSJ.find_element(By.XPATH, \"//*[@id='password-login-password']\")\n",
    "    password_field.send_keys(password)\n",
    "\n",
    "    #click on sign in:\n",
    "    driver_WSJ.find_element(By.XPATH, \"//*[@id='password-login']/div/form/div/div[5]/div[1]/button\").click()\n",
    "    \n",
    "    try:\n",
    "        #accept cookies\n",
    "        time.sleep(10)\n",
    "        driver_WSJ.switch_to.frame(WebDriverWait(driver_WSJ,30).until(EC.presence_of_element_located((By.ID, 'sp_message_iframe_718122'))))\n",
    "        driver_WSJ.find_element(By.CSS_SELECTOR, \"button.message-component:nth-child(2)\").click()\n",
    "        driver_WSJ.switch_to.default_content() \n",
    "        time.sleep(3)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    scrape_time = time.time() #starting the timing\n",
    "    \n",
    "    for single_date in daterange(start_date, end_date):\n",
    "        driver_WSJ.get_screenshot_as_file(\"articles_page2.png\")\n",
    "        date = single_date.strftime(\"%d/%m/%Y\")\n",
    "        print(date)\n",
    "        year = \"%02d\" % (int(date.split('/')[2]),)\n",
    "        month = \"%02d\" % (int(date.split('/')[1]),)\n",
    "        day = \"%02d\" % (int(date.split('/')[0]),)\n",
    "        page = \"%02d\" % (1,) #we assume there is always at least one page\n",
    "        url = f\"https://www.wsj.com/news/archive/{year}/{month}/{day}?page={page}\"\n",
    "        driver_WSJ.get(url) #go to the page in the WSJ archive for the given date\n",
    "        \n",
    "        #collect the article urls for the articles on the first page\n",
    "        articles = WebDriverWait(driver_WSJ,10).until(EC.presence_of_all_elements_located((By.TAG_NAME, \"article\")))\n",
    "        for a in range(0,len(articles)):\n",
    "            WSJ_link.append(articles[a].find_element(By.TAG_NAME, \"a\").get_attribute('href'))\n",
    "            WSJ_date.append(date)\n",
    "\n",
    "        #check if there are multiple pages and if so, visit these as well\n",
    "        pages = WebDriverWait(driver_WSJ, 10).until(EC.presence_of_element_located((By.CLASS_NAME, \"WSJTheme--pagepicker-total--Kl350I1l \")))\n",
    "        page_total = re.findall(r'\\d+', pages.text)\n",
    "        nr_pages = int(page_total[0])\n",
    "\n",
    "        for p in range(1,nr_pages):\n",
    "            page = \"%02d\" % (p,)\n",
    "            url = f\"https://www.wsj.com/news/archive/{year}/{month}/{day}?page={page}\"\n",
    "            trys = 0\n",
    "            while(trys < 100):\n",
    "                try:\n",
    "                    driver_WSJ.get(url) #go to the page in the WSJ archive for the given date\n",
    "                    break\n",
    "                except:\n",
    "                    trys =+ 1\n",
    "\n",
    "            #get the article urls for the articles on the other pages, if there are more than 1\n",
    "            articles = WebDriverWait(driver_WSJ,10).until(EC.presence_of_all_elements_located((By.TAG_NAME, \"article\")))\n",
    "            for a in range(0,len(articles)):\n",
    "                WSJ_link.append(articles[a].find_element(By.TAG_NAME, \"a\").get_attribute('href'))\n",
    "                WSJ_date.append(date)\n",
    "\n",
    "    for a in range(0,len(WSJ_link)):\n",
    "        trys = 0\n",
    "        while(trys < 100):\n",
    "            try:\n",
    "                #go to every article\n",
    "                driver_WSJ.get(WSJ_link[a])\n",
    "                break\n",
    "            except:\n",
    "                trys += 1\n",
    "\n",
    "        try:\n",
    "            #extract the content and add to a variables\n",
    "            time.sleep(10)\n",
    "            driver_WSJ.get_screenshot_as_file(\"article2.png\")\n",
    "            content = WebDriverWait(driver_WSJ, 10).until(EC.presence_of_element_located((By.CLASS_NAME, 'article-content  ')))\n",
    "            text = content.find_elements(By.TAG_NAME, 'p')\n",
    "            article_text = \"\"\n",
    "            for t in range(0,len(text)):\n",
    "                article_text += \" \" + text[t].text\n",
    "            \n",
    "            WSJ_article_content.append(article_text) #add text to the list\n",
    "            #WSJ_article_content.append(content.text) #add text to the list\n",
    "\n",
    "        except: #we don't have access to the article\n",
    "            WSJ_article_content.append(\"NO ACCESS\")\n",
    "            pass #go back to page with all articles\n",
    "\n",
    "        try:\n",
    "            #collect the title as well\n",
    "            title = WebDriverWait(driver_WSJ,10).until(EC.presence_of_element_located((By.CLASS_NAME, \"wsj-article-headline\"))).text\n",
    "            WSJ_title.append(title)\n",
    "        except:\n",
    "            try:\n",
    "                #there are 2 main formats in which the titles are present in the HTML\n",
    "                title = WebDriverWait(driver_WSJ,10).until(EC.presence_of_element_located((By.CLASS_NAME, \"bigTop__hed\"))).text\n",
    "                WSJ_title.append(title)\n",
    "\n",
    "            except:\n",
    "                #add error if there is some unexpected layout -> this way our arrays will be of the same length and we will be able\n",
    "                #to construct a dataframe in the end\n",
    "                WSJ_title.append(\"ERROR\")\n",
    "                pass\n",
    "\n",
    "    print(\"--- %s seconds ---\" % (time.time() - scrape_time)) #how long did we scrape? \n",
    "\n",
    "    driver_WSJ.quit()\n",
    "    #Make the dataframe\n",
    "    WSJ = [\"Wall Street Journal\"] * len(WSJ_article_content)\n",
    "    data = {\"Title\" : WSJ_title, \"Text\" : WSJ_article_content, \"Date\" : WSJ_date, \"News Paper\" : WSJ, \"Link\" : WSJ_link}\n",
    "    Wall_Street_Journal = pd.DataFrame(data)\n",
    "    \n",
    "    \n",
    "    start_date = start_date.strftime(\"%d%m%Y\")\n",
    "    end_date = end_date.strftime(\"%d%m%Y\")\n",
    "    path = f\"Data/Wall_Street_Journal_{start_date}_{end_date}.csv\"\n",
    "    Wall_Street_Journal.to_csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d7d668",
   "metadata": {},
   "source": [
    "## 2.2. Washington Post"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008216b0",
   "metadata": {},
   "source": [
    "### 2.2.1. Scraping Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9825dbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#search_term = term you want to search articles on (\" \" as input should render all articles)\n",
    "#start_date = \n",
    "#end_Date = \n",
    "#username = the username of you account \n",
    "#password = password of you account\n",
    "#-> account will be needed to have access to the articles \n",
    "\n",
    "\n",
    "def getWashingtonPost(search_term, start_date, end_date, username, password):\n",
    "    #put dates in date format\n",
    "    start_date = datetime.strptime(start_date, \"%d/%m/%Y\")   \n",
    "    end_date = datetime.strptime(end_date, \"%d/%m/%Y\")\n",
    "\n",
    "    \n",
    "    #put them in correct format\n",
    "    \n",
    "    \n",
    "    #Declare all list variables for the output\n",
    "    WP_title = []\n",
    "    WP_article_content = []\n",
    "    WP_date = []\n",
    "    WP_link = []\n",
    "\n",
    "    #Declare the driver\n",
    "    #fireFoxOptions = webdriver.FirefoxOptions()\n",
    "    #fireFoxOptions.add_argument(\"--headless\")\n",
    "    #driver_WSJ = webdriver.Firefox(options=fireFoxOptions)\n",
    "    driver_WSJ = webdriver.Firefox()\n",
    "    driver.get(\"https://sso.accounts.dowjones.com/login-page?op=localop&scope=openid%20idp_id%20roles%20email%20given_name%20family_name%20djid%20djUsername%20djStatus%20trackid%20tags%20prts%20suuid%20createTimestamp&client_id=5hssEAdMy0mJTICnJNvC9TXEw3Va7jfO&response_type=code&redirect_uri=https%3A%2F%2Faccounts.wsj.com%2Fauth%2Fsso%2Flogin&nonce=ab6a473a-cfa6-4714-8fad-b6dff98f5f18&ui_locales=en-us-x-wsj-223-2&mars=-1&ns=prod%2Faccounts-wsj&state=8rChOTDzC_Y_AK-i.TJAixN_XjsWxwUEEPoHg2OPCaX6qRBu4nGSk5fqLliY4H0B5F7gj_57-XH-YBWGS&protocol=oauth2&client=5hssEAdMy0mJTICnJNvC9TXEw3Va7jfO#!/signin\")\n",
    "\n",
    "    #Go the the searchpage of the Washington Post\n",
    "    driver.get(\"https://www.washingtonpost.com/search/?query=+&facets=%7B%22time%22%3A%22all%22%2C%22sort%22%3A%22relevancy%22%2C%22section%22%3A%5B%5D%2C%22author%22%3A%5B%5D%7D\")\n",
    "\n",
    "    #Accept Cookies\n",
    "    WebDriverWait(driver,10).until(EC.presence_of_element_located((By.ID, \"onetrust-accept-btn-handler\"))).click()\n",
    "\n",
    "    #Log-in into Washington Post account\n",
    "    try: #it is possible that account is already logged in! \n",
    "\n",
    "        #Sign in into Washington Post account\n",
    "        #Click on Sign In\n",
    "        WebDriverWait(driver,10).until(EC.presence_of_element_located((By.XPATH, \"//*[@id='__next']/div[1]/nav/div[4]/div[2]/a/p\"))).click()\n",
    "\n",
    "        #located username field\n",
    "        time.sleep(2) #field needs to completely load\n",
    "        username_field = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.XPATH, \"//*[@id='username']\")))\n",
    "        username_field.send_keys(username) #send username\n",
    "        username_field.send_keys(Keys.RETURN) #press enter\n",
    "\n",
    "        #Located password field\n",
    "        time.sleep(2) #field needs to completely load\n",
    "        password_field = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.XPATH, \"//*[@id='password']\")))\n",
    "        password_field.send_keys(password)\n",
    "        password_field.send_keys(Keys.ENTER)\n",
    "\n",
    "        #Go back to the search page\n",
    "        WebDriverWait(driver,10).until(EC.presence_of_element_located((By.CLASS_NAME, \"wpds-c-jlBemH \"))).click()\n",
    "        go_to_search = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.ID, \"query\")))\n",
    "        go_to_search.send_keys(search_term)\n",
    "        go_to_search.send_keys(Keys.ENTER)\n",
    "\n",
    "    except:\n",
    "        #you already logged-in -> continue \n",
    "        pass\n",
    "    \n",
    "    driver.get(\"https://www.washingtonpost.com/search/?query=+&facets=%7B%22time%22%3A%22all%22%2C%22sort%22%3A%22relevancy%22%2C%22section%22%3A%5B%5D%2C%22author%22%3A%5B%5D%7D\")\n",
    "    scrape_time = time.time()\n",
    "    for single_date in daterange(start_date, end_date):\n",
    "        date = single_date.strftime(\"%m/%d/%y\")\n",
    "\n",
    "        #Locate the searchbar, send searchterm and press enter\n",
    "        searchbar = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.CLASS_NAME, \"aa-Input\")))\n",
    "        searchbar.send_keys(\" \")\n",
    "        searchbar.send_keys(Keys.ENTER)\n",
    "\n",
    "        #Select time period\n",
    "        #select periode specification\n",
    "        WebDriverWait(driver,10).until(EC.presence_of_element_located((By.XPATH, \"//*[@id='main-content']/div[1]/section[2]/div[1]/button/div/div[2]/span\"))).click()\n",
    "\n",
    "        #select custom time\n",
    "        WebDriverWait(driver,10).until(EC.presence_of_element_located((By.XPATH, \"//*[@id='main-content']/div[1]/section[2]/div[1]/div/div/div[1]/button/div/div[2]\"))).click()\n",
    "        time.sleep(1)\n",
    "\n",
    "        #Send start date\n",
    "        WebDriverWait(driver,10).until(EC.presence_of_element_located((By.XPATH, \"//*[@id='main-content']/div[1]/section[2]/div[1]/div/div/div[2]/div[1]/div/div/div[1]/input\"))).send_keys(date)\n",
    "\n",
    "        #Empty automatic end date\n",
    "        WebDriverWait(driver,10).until(EC.presence_of_element_located((By.XPATH, \"//*[@id='main-content']/div[1]/section[2]/div[1]/div/div/div[2]/div[2]/div/div/div[1]/input\"))).clear()\n",
    "\n",
    "        #Send end date\n",
    "        WebDriverWait(driver,10).until(EC.presence_of_element_located((By.XPATH, \"//*[@id='main-content']/div[1]/section[2]/div[1]/div/div/div[2]/div[2]/div/div/div[1]/input\"))).send_keys(date)\n",
    "\n",
    "        #Press Apply\n",
    "        WebDriverWait(driver,10).until(EC.presence_of_element_located((By.XPATH, \"//*[@id='main-content']/div[1]/section[2]/div[1]/div/div/div[2]/button[2]\"))).click()\n",
    "        time.sleep(5)\n",
    "        \n",
    "        #click to load more on the page untill no longer possible\n",
    "        loading = True\n",
    "        while(loading):\n",
    "            try:\n",
    "                WebDriverWait(driver,10).until(EC.presence_of_element_located((By.XPATH, \"//*[@id='main-content']/div[1]/section[3]/button\"))).click()\n",
    "            except:\n",
    "                loading = False\n",
    "\n",
    "        articles = WebDriverWait(driver,10).until(EC.presence_of_all_elements_located((By.TAG_NAME, \"article\")))\n",
    "\n",
    "        for a in range(0,len(articles)):\n",
    "            WP_link.append(articles[a].find_element(By.TAG_NAME, \"a\").get_attribute('href'))\n",
    "            WP_date.append(single_date.strftime(\"%d/%m/%Y\"))\n",
    "    \n",
    "\n",
    "    #go through all the articles and scrape the content using their url\n",
    "    #use while loop to prevent one time page error\n",
    "    for u in range(0, len(WP_link)):\n",
    "        trys = 0\n",
    "        while(trys < 100):\n",
    "            try:\n",
    "                #go to every article\n",
    "                driver.get(WP_link[u])\n",
    "                break\n",
    "            except:\n",
    "                trys += 1\n",
    "\n",
    "        text = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.TAG_NAME, \"html\"))).text\n",
    "        try:\n",
    "            text = text[text.index(\"Share\")+len(\"Share\")+1:text.rindex(\"Gift Article\")]\n",
    "            WP_article_content.append(text)\n",
    "        except:\n",
    "            try:\n",
    "                text = text[text.index(\"Share\")+len(\"Share\")+1:text.rindex(\"Comments\")]\n",
    "                WP_article_content.append(text)\n",
    "            except:\n",
    "                try:\n",
    "                    text = text[text.index(\"tb.boedttibo\")+len(\"tb.boedttibo\")+1:text.rindex(\"Comments\")]\n",
    "                    WP_article_content.append(text)\n",
    "                except:\n",
    "                    WP_article_content.append(\"ERROR\")\n",
    "                    pass\n",
    "        #idea is: We take the full text and before we have the article content the last pice of text is \"share\" and immediatly after the content\n",
    "        #we have \"comments\" this way we are able to only extract the article content\n",
    "        #the reasons for this is that each article has a more or less different html dependent on their \"type\"\n",
    "\n",
    "        #there are different formats of titles, in this order all titles are located\n",
    "        try:\n",
    "            title = driver.find_element(By.CSS_SELECTOR, \"#main-content > span:nth-child(2)\").text\n",
    "            WP_title.append(title)\n",
    "        except:\n",
    "            try:\n",
    "                title = driver.find_element(By.CSS_SELECTOR, \"#main-content\").text\n",
    "                WP_title.append(title)\n",
    "            except:\n",
    "                try:\n",
    "                    title = driver.find_element(By.CSS_SELECTOR, \"#main-content > span\").text\n",
    "                    WP_title.append(title)\n",
    "                except:\n",
    "                    WP_title.append(\"ERROR\")\n",
    "                    pass\n",
    "\n",
    "    #go back to the search page\n",
    "    driver.get(\"https://www.washingtonpost.com/search/?query=+&facets=%7B%22time%22%3A%22all%22%2C%22sort%22%3A%22relevancy%22%2C%22section%22%3A%5B%5D%2C%22author%22%3A%5B%5D%7D\")\n",
    "\n",
    "    #Close driver\n",
    "    driver.quit()\n",
    "\n",
    "    #How long did the scraping took? \n",
    "    print(\"--- %s seconds ---\" % (time.time() - scrape_time))\n",
    "\n",
    "    #Create output file\n",
    "    #Make the dataframe\n",
    "    WP = [\"Washington Post\"] * len(WP_article_content)\n",
    "    data = {\"Title\" : WP_title, \"Text\" : WP_article_content, \"Date\" : WP_date, \"News Paper\" : WP, \"Link\" : WP_link}\n",
    "    WashingtonPost = pd.DataFrame(data)\n",
    "    \n",
    "    start_date = start_date.strftime(\"%d%m%Y\")\n",
    "    end_date = end_date.strftime(\"%d%m%Y\")\n",
    "    path = f\"Washington_Post_{start_date}_{end_date}.csv\"\n",
    "    WashingtonPost.to_csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccde776",
   "metadata": {},
   "source": [
    "## 2.3. New York Times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b538717",
   "metadata": {},
   "source": [
    "### 2.3.1. Scraping Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "245022ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNewYorkTimes(start_date, end_date, username, password):\n",
    "    #put dates in date format\n",
    "    start_date = datetime.strptime(start_date, \"%d/%m/%Y\") \n",
    "    end_date = datetime.strptime(end_date, \"%d/%m/%Y\")\n",
    "\n",
    "    #Declare all list variables for the output\n",
    "    NYT_title = []\n",
    "    NYT_article_content = []\n",
    "    NYT_date = []\n",
    "    NYT_link = []\n",
    "\n",
    "    #set up driver\n",
    "    #driver = webdriver.Chrome()\n",
    "    driver = webdriver.Firefox()\n",
    "    #go to log-in screen of NYT\n",
    "    driver.get(\"https://myaccount.nytimes.com/auth/login?response_type=cookie&client_id=vi&redirect_uri=https%3A%2F%2Fwww.nytimes.com%2Fsubscription%2Fonboarding-offer%3FcampaignId%3D7JFJX%26EXIT_URI%3Dhttps%253A%252F%252Fwww.nytimes.com%252F&asset=masthead\")\n",
    "    driver.get(\"https://myaccount.nytimes.com/auth/login?response_type=cookie&client_id=vi&redirect_uri=https%3A%2F%2Fwww.nytimes.com%2Fsubscription%2Fonboarding-offer%3FcampaignId%3D7JFJX%26EXIT_URI%3Dhttps%253A%252F%252Fwww.nytimes.com%252F&asset=masthead\")\n",
    "    driver.get(\"https://myaccount.nytimes.com/auth/login?response_type=cookie&client_id=vi&redirect_uri=https%3A%2F%2Fwww.nytimes.com%2Fsubscription%2Fonboarding-offer%3FcampaignId%3D7JFJX%26EXIT_URI%3Dhttps%253A%252F%252Fwww.nytimes.com%252F&asset=masthead\")\n",
    "\n",
    "    #not a robot\n",
    "    input(\"Press Enter to continue...\")\n",
    "\n",
    "    #Select log-in field\n",
    "    login = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.ID, \"email\")))\n",
    "\n",
    "    #Send username\n",
    "    login.send_keys(username)\n",
    "\n",
    "    #Press continue\n",
    "    WebDriverWait(driver,10).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"#myAccountAuth > div > div > div > form > div > div.css-bho3kg-buttonWrapper-buttonStyles-Button > button\"))).click()\n",
    "\n",
    "    #Select password field\n",
    "    password_field = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.ID, \"password\")))\n",
    "\n",
    "    #send password\n",
    "    password_field.send_keys(password)\n",
    "\n",
    "    #Press continue\n",
    "    WebDriverWait(driver,10).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"#myAccountAuth > div > div > form > div > div.css-1nkv26b-buttonWrapper-buttonStyles-Button > button\"))).click()\n",
    "\n",
    "    time.sleep(1)\n",
    "\n",
    "    #go to search page\n",
    "    driver.get(\"https://www.nytimes.com/search?dropmab=false&query=&sort=best\")\n",
    "\n",
    "    #Accept Cookies\n",
    "    try:\n",
    "        WebDriverWait(driver,5).until(EC.presence_of_element_located((By.CLASS_NAME, \"banner__container__cta--accept\"))).click()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    scrape_time = time.time()\n",
    "    for single_date in daterange(start_date, end_date):\n",
    "        date = single_date.strftime(\"%Y%m%d\")\n",
    "\n",
    "        url = f\"https://www.nytimes.com/search?dropmab=false&endDate={date}&query=%20&sort=best&startDate={date}\"\n",
    "        driver.get(url)\n",
    "\n",
    "        #click to load more on the page untill no longer possible\n",
    "        loading = True\n",
    "        while(loading):\n",
    "            try:\n",
    "                #time.sleep(random.randint(2,4))\n",
    "                WebDriverWait(driver,10).until(EC.presence_of_element_located((By.XPATH, \"//*[@id='site-content']/div/div[2]/div[2]/div/button\"))).click()\n",
    "            except:\n",
    "                loading = False\n",
    "\n",
    "        articles = WebDriverWait(driver,10).until(EC.presence_of_all_elements_located((By.CLASS_NAME, \"css-1l4w6pd\")))\n",
    "        dates = WebDriverWait(driver,10).until(EC.presence_of_all_elements_located((By.CLASS_NAME, \"css-17ubb9w\")))\n",
    "\n",
    "        date_list = WebDriverWait(driver,10).until(EC.presence_of_all_elements_located((By.CLASS_NAME, \"css-17ubb9w\")))\n",
    "        true_date = date_list[0].text\n",
    "        for a in range(0, len(articles)):\n",
    "            if(true_date == date_list[a].text):\n",
    "                NYT_link.append(articles[a].find_element(By.TAG_NAME, \"a\").get_attribute('href'))\n",
    "                NYT_date.append(single_date.strftime(\"%d/%m/%Y\"))\n",
    "    \n",
    "    #go through all the articles urls\n",
    "    for u in range(0, len(NYT_link)):\n",
    "        driver.get(NYT_link[u])\n",
    "        try:\n",
    "            WebDriverWait(driver,3).until(EC.presence_of_element_located((By.CLASS_NAME, \"banner__container__cta--accept\"))).click()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            WebDriverWait(driver,1).until(EC.presence_of_element_located((By.CLASS_NAME, \"loginButton\"))).click()\n",
    "            driver.switch_to.window( driver.window_handles[1])\n",
    "\n",
    "            #Select log-in field\n",
    "            login = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.ID, \"email\")))\n",
    "            login.send_keys(username)\n",
    "\n",
    "            #Press continue\n",
    "            WebDriverWait(driver,10).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"#myAccountAuth > div > div > div > form > div > div.css-bho3kg-buttonWrapper-buttonStyles-Button > button\"))).click()\n",
    "\n",
    "            #Select password field\n",
    "            password_field = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.ID, \"password\")))\n",
    "\n",
    "            #send password\n",
    "            password_field.send_keys(password)\n",
    "\n",
    "            #Press continue\n",
    "            WebDriverWait(driver,10).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"#myAccountAuth > div > div > form > div > div.css-1nkv26b-buttonWrapper-buttonStyles-Button > button\"))).click()            \n",
    "\n",
    "            time.sleep(3)\n",
    "            driver.close()\n",
    "            driver.switch_to.window(driver.window_handles[0])\n",
    "            driver.get(article_urls[u])\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "        #get the article title\n",
    "        try:\n",
    "            title = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.TAG_NAME, \"h1\"))).text\n",
    "            NYT_title.append(title)\n",
    "        except:\n",
    "            NYT_title.append(\"ERROR\")\n",
    "\n",
    "        #get article content\n",
    "        try:\n",
    "            content = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.TAG_NAME, \"article\")))\n",
    "            content = content.find_elements(By.TAG_NAME, \"p\")\n",
    "            text = \"\"\n",
    "            for t in range(0,len(content)):\n",
    "                text += content[t].text\n",
    "\n",
    "            NYT_article_content.append(text)\n",
    "        except:\n",
    "            NYT_article_content.append(\"ERROR\")\n",
    "\n",
    "    #How long did the scraping took? \n",
    "    print(\"--- %s seconds ---\" % (time.time() - scrape_time))\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    NYT = [\"New York Times\"] * len(NYT_article_content)\n",
    "    data = {\"Title\" : NYT_title, \"Text\" : NYT_article_content, \"Date\" : NYT_date, \"News Paper\" : NYT, \"Link\" : NYT_link}\n",
    "    NewYorkTimes = pd.DataFrame(data)\n",
    "    \n",
    "    start_date = start_date.strftime(\"%d%m%Y\")\n",
    "    end_date = end_date.strftime(\"%d%m%Y\")\n",
    "    path = f\"Data/New_York_Times_{start_date}_{end_date}.csv\"\n",
    "    NewYorkTimes.to_parquet(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc367c6b",
   "metadata": {},
   "source": [
    "## 2.4. Politico"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdc1975",
   "metadata": {},
   "source": [
    "### 2.4.1. Scraping Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a337dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#start_date and end_date should be in format m/d/y as a string\n",
    "\n",
    "def getPolitico(start_date, end_date):\n",
    "    #put dates in date format\n",
    "    end_date = datetime.strptime(end_date, \"%d/%m/%Y\")\n",
    "    start_date = datetime.strptime(start_date, \"%d/%m/%Y\")\n",
    "    \n",
    "    #Declare all list variables for the output\n",
    "    P_title = []\n",
    "    P_article_content = []\n",
    "    P_date = []\n",
    "    P_link = []\n",
    "\n",
    "    driver = webdriver.Firefox()\n",
    "\n",
    "    #go to the website\n",
    "    driver.get(\"https://www.politico.com/search?adv=true\")\n",
    "\n",
    "    #accept cookies\n",
    "    try:\n",
    "        WebDriverWait(driver,10).until(EC.presence_of_element_located((By.ID, \"onetrust-accept-btn-handler\"))).click()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    scrape_time = time.time()\n",
    "    for single_date in daterange(start_date, end_date):\n",
    "        date = single_date.strftime(\"%m/%d/%Y\")\n",
    "        driver.get(f\"https://www.politico.com/search/1?adv=true&start={date}&end={date}\")\n",
    "        articles = WebDriverWait(driver,10).until(EC.presence_of_all_elements_located((By.TAG_NAME, \"article\")))\n",
    "        for a in range(0, len(articles)):\n",
    "            P_link.append(articles[a].find_element(By.TAG_NAME, \"a\").get_attribute('href'))\n",
    "            P_date.append(date)\n",
    "\n",
    "        #get the number of pages to know how much you should click \n",
    "        try:\n",
    "            nr_pages = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.CSS_SELECTOR, \".pagination > ol:nth-child(2) > li:nth-child(6) > a:nth-child(1)\"))).text\n",
    "        except:\n",
    "            nr_pages = 1\n",
    "            pass\n",
    "\n",
    "        for p in range(1, int(nr_pages)):\n",
    "            url = f\"https://www.politico.com/search/{p+1}?adv=true&start={date}&end={date}\"\n",
    "            driver.get(url)\n",
    "\n",
    "        articles = WebDriverWait(driver,10).until(EC.presence_of_all_elements_located((By.TAG_NAME, \"article\")))\n",
    "        for a in range(0, len(articles)):\n",
    "            P_link.append(articles[a].find_element(By.TAG_NAME, \"a\").get_attribute('href'))\n",
    "            P_date.append(single_date.strftime(\"%d/%m/%Y\"))\n",
    "\n",
    "    #loop through every article\n",
    "    for u in range(0,len(P_link)):\n",
    "        driver.get(P_link[u])\n",
    "\n",
    "        #get the title\n",
    "        try:\n",
    "            title = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.CLASS_NAME, \"headline\"))).text\n",
    "            P_title.append(title)\n",
    "        except:\n",
    "            P_title.append(\"PRO\")\n",
    "            pass\n",
    "\n",
    "        #get content\n",
    "        try:\n",
    "            content = WebDriverWait(driver,10).until(EC.presence_of_all_elements_located((By.CLASS_NAME, \"story-text__paragraph  \")))\n",
    "            text = \"\"\n",
    "            for c in range(0,len(content)):\n",
    "                text += content[c].text\n",
    "\n",
    "            P_article_content.append(text)\n",
    "\n",
    "        except:\n",
    "            P_article_content.append(\"PRO\")\n",
    "            pass\n",
    "\n",
    "    driver.quit()\n",
    "    print(\"--- %s seconds ---\" % (time.time() - scrape_time))\n",
    "\n",
    "    P = [\"Politico\"] * len(P_article_content)\n",
    "    data = {\"Title\" : P_title, \"Text\" : P_article_content, \"Date\" : P_date, \"News Paper\" : P, \"Link\" : P_link}\n",
    "    Politico = pd.DataFrame(data)\n",
    "\n",
    "    start_date = start_date.strftime(\"%d%m%Y\")\n",
    "    end_date = end_date.strftime(\"%d%m%Y\")\n",
    "    path = f\"Politico_{start_date}_{end_date}.csv\"\n",
    "    Politico.to_csv(path)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf21f495",
   "metadata": {},
   "source": [
    "# 3. Running Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6de5ca",
   "metadata": {},
   "source": [
    "getWallStreetJournal(username, password, start_date, end_date)\n",
    "\n",
    "getWashingtonPost(search_term, start_date, end_date, username, password)\n",
    "\n",
    "getNewYorkTimes(start_date, end_date, username, password):\n",
    "\n",
    "getPolitico(start_date, end_date):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8c85c8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01/01/2010\n",
      "--- 394.8229627609253 seconds ---\n"
     ]
    }
   ],
   "source": [
    "getWallStreetJournal(\"reane.delaunoy@telenet.be\", \"REenJUC0MB0\", \"01/01/2010\", \"01/01/2010\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9f56be7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 168.77081990242004 seconds ---\n"
     ]
    }
   ],
   "source": [
    "getWashingtonPost(\" \", \"01/01/2022\", \"01/01/2022\", \"tb.boedttibo@gmail.com\", \"ThesisR&T\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1aad6095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press Enter to continue...\n",
      "--- 33978.41111564636 seconds ---\n"
     ]
    }
   ],
   "source": [
    "getNewYorkTimes(\"01/01/2022\", \"31/01/2022\", \"tb.boedttibo@gmail.com\", \"ThesisR&T0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f12c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "getPolitico(\"01/01/2022\", \"01/01/2022\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "86f9649c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_parquet(\"Data/New_York_Times_01012022_31012022.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e5b386",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
