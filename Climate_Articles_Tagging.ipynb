{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "549a3d8c",
   "metadata": {},
   "source": [
    "# 0. Packages and Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7567ca2",
   "metadata": {},
   "source": [
    "## 0.1. Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bd2f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "from nltk import ngrams\n",
    "from collections import Counter\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, pipeline, AutoModelForSequenceClassification, AutoModelForSeq2SeqLM\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pyarrow\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0aaaa2",
   "metadata": {},
   "source": [
    "## 0.2. Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce24a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre-process text for lexicon based approaches\n",
    "def preprocess_text(text):\n",
    "    # remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # convert to lower case\n",
    "    text = text.lower()\n",
    "    # remove blank spaces\n",
    "    text = ' '.join(text.split())\n",
    "    # remove newline characters\n",
    "    text = text.replace('\\n', '')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b22206e",
   "metadata": {},
   "source": [
    "### Absolute Count with Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "2495792e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create function that take the dataframe, lexicon and n-gram value (how many n-grams should be considered) and determine the \n",
    "#count of words in dataframe text that match the lexicon\n",
    "def count_lexicon_words_1(text_df, lexicon):\n",
    "    lexicon = lexicon[\"Lexicon\"]\n",
    "    count = []\n",
    "\n",
    "    for text in text_df[\"Text\"]:\n",
    "        lexicon_counts = 0\n",
    "\n",
    "        for word in lexicon:\n",
    "            lexicon_counts += text.lower().count(word.lower())\n",
    "            \n",
    "        count.append(lexicon_counts)\n",
    "        \n",
    "    text_df[\"Lexicon Count\"] = count\n",
    "    \n",
    "    return(text_df)\n",
    "\n",
    "#create a function that used the lexicon approach to determine with the target is yes or no\n",
    "def lexicon_target_classifier_1(df, treshold):\n",
    "    target = []\n",
    "    \n",
    "    count = df[\"Lexicon Count\"]\n",
    "    \n",
    "    for c in count:\n",
    "        if c < treshold:\n",
    "            target.append(\"No\")\n",
    "        else:\n",
    "            target.append(\"Yes\")\n",
    "            \n",
    "    df[\"Target Lexicon\"] = target\n",
    "    return(df)\n",
    "\n",
    "#Combine both functions to classify articles based on the lexicon\n",
    "def lexicon_climate_classifier_1(text_df, lexicon, treshold):\n",
    "    df = count_lexicon_words_1(text_df, lexicon)\n",
    "    \n",
    "    return(lexicon_target_classifier_1(df, treshold))\n",
    "\n",
    "def threshold_metrics_1(df_text, lexicon, min_treshhold, max_treshhold):\n",
    "    for i in range(min_treshhold, max_treshhold + 1):\n",
    "        df = lexicon_climate_classifier_1(df_text, lexicon, i)\n",
    "        cross_table = pd.crosstab(df['Target'], df['Target Lexicon'], margins=True)\n",
    "        \n",
    "        # calculate classification metrics using scikit-learn\n",
    "        accuracy = (cross_table.iloc[0, 0] + cross_table.iloc[1, 1]) / cross_table.loc['All', 'All'] if cross_table.shape == (3,3) else cross_table.iloc[1,0] / (cross_table.iloc[1, 0] + cross_table.iloc[0,0]) \n",
    "        precision = cross_table.iloc[1,1] / cross_table.iloc[0,1] if cross_table.iloc[0,1] != 0 else 0\n",
    "        recall = cross_table.iloc[1,1] / cross_table.iloc[1,0] if cross_table.iloc[1,0] != 0 else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "\n",
    "        # print the metrics\n",
    "        print(\"Threshhold:\", i)\n",
    "        print(\"Accuracy:\", accuracy)\n",
    "        print(\"Precision:\", precision)\n",
    "        print(\"Recall:\", recall)\n",
    "        print(\"F1 score:\", f1_score)\n",
    "        print(cross_table)\n",
    "        print(\"\\n\")\n",
    "        \n",
    "def get_metrics_df_1(df_text, lexicon, min_treshhold, max_treshhold, Lexicon_name):    \n",
    "    Accuracy = []\n",
    "    Precision = []\n",
    "    Recall = []\n",
    "    F1_score = []\n",
    "    Name = []\n",
    "    Treshhold = []\n",
    "    Technique = []\n",
    "    \n",
    "    for i in range(min_treshhold, max_treshhold + 1):\n",
    "        df = lexicon_climate_classifier_1(df_text, lexicon, i)\n",
    "        cross_table = pd.crosstab(df['Target'], df['Target Lexicon'], margins=True)\n",
    "\n",
    "        # calculate classification metrics using scikit-learn\n",
    "        accuracy = (cross_table.iloc[0, 0] + cross_table.iloc[1, 1]) / cross_table.loc['All', 'All'] if cross_table.shape == (3,3) else cross_table.iloc[1,0] / (cross_table.iloc[1, 0] + cross_table.iloc[0,0]) \n",
    "        precision = cross_table.iloc[1,1] / cross_table.iloc[0,1] if cross_table.iloc[0,1] != 0 else 0\n",
    "        recall = cross_table.iloc[1,1] / cross_table.iloc[1,0] if cross_table.iloc[1,0] != 0 else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "        \n",
    "        Accuracy.append(accuracy)\n",
    "        Precision.append(precision)\n",
    "        Recall.append(recall)\n",
    "        F1_score.append(f1_score)\n",
    "        Name.append(Lexicon_name)\n",
    "        Treshhold.append(i)\n",
    "        Technique.append(\"Absolute Frequency\")\n",
    "        \n",
    "    return(pd.DataFrame({\"Lexicon\" : Name, \"Technique\": Technique, \"Treshhold\" : Treshhold, \"Accuracy\" : Accuracy, \"Precision\" : Precision, \"Recall\" : Recall, \"F1 Score\" : F1_score}))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cf7b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create function that take the dataframe, lexicon and n-gram value (how many n-grams should be considered) and determine the \n",
    "#count of words in dataframe text that match the lexicon\n",
    "def count_lexicon_words_1(text_df, lexicon):\n",
    "    lexicon = lexicon[\"Lexicon\"]\n",
    "    count = []\n",
    "\n",
    "    for text in text_df[\"Text\"]:\n",
    "        lexicon_counts = 0\n",
    "\n",
    "        for word in lexicon:\n",
    "            lexicon_counts += text.lower().count(word.lower())\n",
    "        \n",
    "        count.append(lexicon_counts)\n",
    "        \n",
    "    text_df[\"Lexicon Count\"] = count\n",
    "    \n",
    "    return(text_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440d1eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_lexicon_words_1(tag_climate_df, BBC_Lexicon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c69c28",
   "metadata": {},
   "source": [
    "### Relative Count with Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3ca9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create function that take the dataframe, lexicon and n-gram value (how many n-grams should be considered) and determine the \n",
    "#count of words in dataframe text that match the lexicon\n",
    "def count_lexicon_words_2(text_df, lexicon):\n",
    "    lexicon = lexicon[\"Lexicon\"]\n",
    "    count = []\n",
    "    \n",
    "    for text in text_df[\"Text\"]:\n",
    "        lexicon_counts = 0\n",
    "\n",
    "        for word in lexicon:\n",
    "            lexicon_counts += text.lower().count(word.lower())\n",
    "        \n",
    "        word_list = text.split() \n",
    "        word_count = len(word_list)\n",
    "        count.append((lexicon_counts/word_count)*100)\n",
    "\n",
    "    text_df[\"Lexicon Count\"] = count\n",
    "    \n",
    "    return(text_df)\n",
    "\n",
    "#create a function that used the lexicon approach to determine with the target is yes or no\n",
    "def lexicon_target_classifier_2(df, treshold):\n",
    "    target = []\n",
    "    \n",
    "    count = df[\"Lexicon Count\"]\n",
    "    \n",
    "    for c in count:\n",
    "        if c < treshold:\n",
    "            target.append(\"No\")\n",
    "        else:\n",
    "            target.append(\"Yes\")\n",
    "            \n",
    "    df[\"Target Lexicon\"] = target\n",
    "    return(df)\n",
    "\n",
    "#Combine both functions to classify articles based on the lexicon\n",
    "def lexicon_climate_classifier_2(text_df, lexicon, treshold):\n",
    "    df = count_lexicon_words_2(text_df, lexicon)\n",
    "    \n",
    "    return(lexicon_target_classifier_2(df, treshold))\n",
    "\n",
    "def threshold_metrics_2(df_text, lexicon, min_treshhold, max_treshhold, jump):\n",
    "    \n",
    "    if(jump == 0):\n",
    "        df = lexicon_climate_classifier_2(df_text, lexicon, min_threshhold)\n",
    "        cross_table = pd.crosstab(df['Target'], df['Target Lexicon'], margins=True)\n",
    "\n",
    "        # calculate classification metrics using scikit-learn\n",
    "        accuracy = (cross_table.iloc[0, 0] + cross_table.iloc[1, 1]) / cross_table.loc['All', 'All'] if cross_table.shape == (3,3) else cross_table.iloc[1,0] / (cross_table.iloc[1, 0] + cross_table.iloc[0,0]) \n",
    "        precision = cross_table.iloc[1,1] / cross_table.iloc[0,1] if cross_table.iloc[0,1] != 0 else 0\n",
    "        recall = cross_table.iloc[1,1] / cross_table.iloc[1,0] if cross_table.iloc[1,0] != 0 else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "\n",
    "        # print the metrics\n",
    "        print(\"Threshhold:\", i)\n",
    "        print(\"Accuracy:\", accuracy)\n",
    "        print(\"Precision:\", precision)\n",
    "        print(\"Recall:\", recall)\n",
    "        print(\"F1 score:\", f1_score)\n",
    "        print(cross_table)\n",
    "        print(\"\\n\")\n",
    "\n",
    "    else:   \n",
    "        for num in range(int((max_treshhold - min_treshhold) / jump) + 1):\n",
    "            i = min_treshhold + num * jump\n",
    "            df = lexicon_climate_classifier_2(df_text, lexicon, i)\n",
    "            cross_table = pd.crosstab(df['Target'], df['Target Lexicon'], margins=True)\n",
    "\n",
    "            # calculate classification metrics using scikit-learn\n",
    "            accuracy = (cross_table.iloc[0, 0] + cross_table.iloc[1, 1]) / cross_table.loc['All', 'All'] if cross_table.shape == (3,3) else cross_table.iloc[1,0] / (cross_table.iloc[1, 0] + cross_table.iloc[0,0]) \n",
    "            precision = cross_table.iloc[1,1] / cross_table.iloc[0,1] if cross_table.iloc[0,1] != 0 else 0\n",
    "            recall = cross_table.iloc[1,1] / cross_table.iloc[1,0] if cross_table.iloc[1,0] != 0 else 0\n",
    "            f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "\n",
    "            # print the metrics\n",
    "            print(\"Threshhold:\", i)\n",
    "            print(\"Accuracy:\", accuracy)\n",
    "            print(\"Precision:\", precision)\n",
    "            print(\"Recall:\", recall)\n",
    "            print(\"F1 score:\", f1_score)\n",
    "            print(cross_table)\n",
    "            print(\"\\n\")\n",
    "\n",
    "def get_metrics_df_2(df_text, lexicon, min_treshhold, max_treshhold, jump, Lexicon_name):    \n",
    "    Accuracy = []\n",
    "    Precision = []\n",
    "    Recall = []\n",
    "    F1_score = []\n",
    "    Name = []\n",
    "    Treshhold = []\n",
    "    Technique = []\n",
    "    \n",
    "    for num in range(int((max_treshhold - min_treshhold) / jump) + 1):\n",
    "        i = min_treshhold + num * jump\n",
    "        df = lexicon_climate_classifier_2(df_text, lexicon, i)\n",
    "        cross_table = pd.crosstab(df['Target'], df['Target Lexicon'], margins=True)\n",
    "\n",
    "        # calculate classification metrics using scikit-learn\n",
    "        accuracy = (cross_table.iloc[0, 0] + cross_table.iloc[1, 1]) / cross_table.loc['All', 'All'] if cross_table.shape == (3,3) else cross_table.iloc[1,0] / (cross_table.iloc[1, 0] + cross_table.iloc[0,0]) \n",
    "        precision = cross_table.iloc[1,1] / cross_table.iloc[0,1] if cross_table.iloc[0,1] != 0 else 0\n",
    "        recall = cross_table.iloc[1,1] / cross_table.iloc[1,0] if cross_table.iloc[1,0] != 0 else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "        \n",
    "        Accuracy.append(accuracy)\n",
    "        Precision.append(precision)\n",
    "        Recall.append(recall)\n",
    "        F1_score.append(f1_score)\n",
    "        Name.append(Lexicon_name)\n",
    "        Treshhold.append(i)\n",
    "        Technique.append(\"Relative Frequency\")\n",
    "        \n",
    "    return(pd.DataFrame({\"Lexicon\" : Name, \"Technique\": Technique, \"Treshhold\" : Treshhold, \"Accuracy\" : Accuracy, \"Precision\" : Precision, \"Recall\" : Recall, \"F1 Score\" : F1_score}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f4d81d",
   "metadata": {},
   "source": [
    "### Absolute Term Presences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6226be1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create function that take the dataframe, lexicon and n-gram value (how many n-grams should be considered) and determine the \n",
    "#count of words in dataframe text that match the lexicon\n",
    "def count_lexicon_words_3(text_df, lexicon):\n",
    "    lexicon = lexicon[\"Lexicon\"]\n",
    "    count = []\n",
    "\n",
    "    for text in text_df[\"Text\"]:\n",
    "        lexicon_counts = 0\n",
    "\n",
    "        for word in lexicon:\n",
    "            if text.lower().count(word.lower()) > 0:\n",
    "                lexicon_counts += 1\n",
    "        \n",
    "        count.append(lexicon_counts)\n",
    "        \n",
    "    text_df[\"Lexicon Count\"] = count\n",
    "    \n",
    "    return text_df\n",
    "\n",
    "#create a function that used the lexicon approach to determine with the target is yes or no\n",
    "def lexicon_target_classifier_3(df, treshold):\n",
    "    target = []\n",
    "    \n",
    "    count = df[\"Lexicon Count\"]\n",
    "    \n",
    "    for c in count:\n",
    "        if c < treshold:\n",
    "            target.append(\"No\")\n",
    "        else:\n",
    "            target.append(\"Yes\")\n",
    "            \n",
    "    df[\"Target Lexicon\"] = target\n",
    "    return(df)\n",
    "\n",
    "#Combine both functions to classify articles based on the lexicon\n",
    "def lexicon_climate_classifier_3(text_df, lexicon, treshold):\n",
    "    df = count_lexicon_words_3(text_df, lexicon)\n",
    "    \n",
    "    return(lexicon_target_classifier_3(df, treshold))\n",
    "\n",
    "def threshold_metrics_3(df_text, lexicon, min_treshhold, max_treshhold):\n",
    "    for i in range(min_treshhold, max_treshhold + 1):\n",
    "        df = lexicon_climate_classifier_3(df_text, lexicon, i)\n",
    "        cross_table = pd.crosstab(df['Target'], df['Target Lexicon'], margins=True)\n",
    "        \n",
    "        # calculate classification metrics using scikit-learn\n",
    "        accuracy = (cross_table.iloc[0, 0] + cross_table.iloc[1, 1]) / cross_table.loc['All', 'All'] if cross_table.shape == (3,3) else cross_table.iloc[1,0] / (cross_table.iloc[1, 0] + cross_table.iloc[0,0]) \n",
    "        precision = cross_table.iloc[1,1] / cross_table.iloc[0,1] if cross_table.iloc[0,1] != 0 else 0\n",
    "        recall = cross_table.iloc[1,1] / cross_table.iloc[1,0] if cross_table.iloc[1,0] != 0 else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "\n",
    "        # print the metrics\n",
    "        print(\"Threshhold:\", i)\n",
    "        print(\"Accuracy:\", accuracy)\n",
    "        print(\"Precision:\", precision)\n",
    "        print(\"Recall:\", recall)\n",
    "        print(\"F1 score:\", f1_score)\n",
    "        print(cross_table)\n",
    "        print(\"\\n\")\n",
    "        \n",
    "def get_metrics_df_3(df_text, lexicon, min_treshhold, max_treshhold, Lexicon_name):    \n",
    "    Accuracy = []\n",
    "    Precision = []\n",
    "    Recall = []\n",
    "    F1_score = []\n",
    "    Name = []\n",
    "    Treshhold = []\n",
    "    Technique = []\n",
    "    \n",
    "    for i in range(min_treshhold, max_treshhold + 1):\n",
    "        df = lexicon_climate_classifier_3(df_text, lexicon, i)\n",
    "        cross_table = pd.crosstab(df['Target'], df['Target Lexicon'], margins=True)\n",
    "\n",
    "        # calculate classification metrics using scikit-learn\n",
    "        accuracy = (cross_table.iloc[0, 0] + cross_table.iloc[1, 1]) / cross_table.loc['All', 'All'] if cross_table.shape == (3,3) else cross_table.iloc[1,0] / (cross_table.iloc[1, 0] + cross_table.iloc[0,0]) \n",
    "        precision = cross_table.iloc[1,1] / cross_table.iloc[0,1] if cross_table.iloc[0,1] != 0 else 0\n",
    "        recall = cross_table.iloc[1,1] / cross_table.iloc[1,0] if cross_table.iloc[1,0] != 0 else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "        \n",
    "        Accuracy.append(accuracy)\n",
    "        Precision.append(precision)\n",
    "        Recall.append(recall)\n",
    "        F1_score.append(f1_score)\n",
    "        Name.append(Lexicon_name)\n",
    "        Treshhold.append(i)\n",
    "        Technique.append(\"Absolute Presences\")\n",
    "        \n",
    "    return(pd.DataFrame({\"Lexicon\" : Name, \"Technique\": Technique, \"Treshhold\" : Treshhold, \"Accuracy\" : Accuracy, \"Precision\" : Precision, \"Recall\" : Recall, \"F1 Score\" : F1_score}))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dd8d36",
   "metadata": {},
   "source": [
    "### Relative Term Presences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f82d5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create function that take the dataframe, lexicon and n-gram value (how many n-grams should be considered) and determine the \n",
    "#count of words in dataframe text that match the lexicon\n",
    "def count_lexicon_words_4(text_df, lexicon):\n",
    "    lexicon = lexicon[\"Lexicon\"]\n",
    "    count = []\n",
    "\n",
    "    for text in text_df[\"Text\"]:\n",
    "        lexicon_counts = 0\n",
    "\n",
    "        for word in lexicon:\n",
    "            if text.lower().count(word.lower()) > 0:\n",
    "                lexicon_counts += 1\n",
    "        \n",
    "        count.append(lexicon_counts)\n",
    "        \n",
    "    text_df[\"Lexicon Count\"] = count\n",
    "    \n",
    "    return text_df\n",
    "\n",
    "#create a function that used the lexicon approach to determine with the target is yes or no\n",
    "def lexicon_target_classifier_4(df, treshold):\n",
    "    target = []\n",
    "    \n",
    "    count = df[\"Lexicon Count\"]\n",
    "    \n",
    "    for c in count:\n",
    "        if c < treshold:\n",
    "            target.append(\"No\")\n",
    "        else:\n",
    "            target.append(\"Yes\")\n",
    "            \n",
    "    df[\"Target Lexicon\"] = target\n",
    "    return(df)\n",
    "\n",
    "#Combine both functions to classify articles based on the lexicon\n",
    "def lexicon_climate_classifier_4(text_df, lexicon, treshold):\n",
    "    df = count_lexicon_words_4(text_df, lexicon)\n",
    "    \n",
    "    return(lexicon_target_classifier_4(df, treshold))\n",
    "\n",
    "def threshold_metrics_4(df_text, lexicon, min_treshhold, max_treshhold, jump):\n",
    "    \n",
    "    if(jump == 0):\n",
    "        df = lexicon_climate_classifier_4(df_text, lexicon, min_treshhold)\n",
    "        cross_table = pd.crosstab(df['Target'], df['Target Lexicon'], margins=True)\n",
    "\n",
    "        # calculate classification metrics using scikit-learn\n",
    "        accuracy = (cross_table.iloc[0, 0] + cross_table.iloc[1, 1]) / cross_table.loc['All', 'All'] if cross_table.shape == (3,3) else cross_table.iloc[1,0] / (cross_table.iloc[1, 0] + cross_table.iloc[0,0]) \n",
    "        precision = cross_table.iloc[1,1] / cross_table.iloc[0,1] if cross_table.iloc[0,1] != 0 else 0\n",
    "        recall = cross_table.iloc[1,1] / cross_table.iloc[1,0] if cross_table.iloc[1,0] != 0 else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "\n",
    "        # print the metrics\n",
    "        print(\"Threshhold:\", i)\n",
    "        print(\"Accuracy:\", accuracy)\n",
    "        print(\"Precision:\", precision)\n",
    "        print(\"Recall:\", recall)\n",
    "        print(\"F1 score:\", f1_score)\n",
    "        print(cross_table)\n",
    "        print(\"\\n\")\n",
    "        \n",
    "    else:\n",
    "        for num in range(int((max_treshhold - min_treshhold) / jump) + 1):\n",
    "            i = min_treshhold + num * jump\n",
    "            df = lexicon_climate_classifier_4(df_text, lexicon, i)\n",
    "            cross_table = pd.crosstab(df['Target'], df['Target Lexicon'], margins=True)\n",
    "\n",
    "            # calculate classification metrics using scikit-learn\n",
    "            accuracy = (cross_table.iloc[0, 0] + cross_table.iloc[1, 1]) / cross_table.loc['All', 'All'] if cross_table.shape == (3,3) else cross_table.iloc[1,0] / (cross_table.iloc[1, 0] + cross_table.iloc[0,0]) \n",
    "            precision = cross_table.iloc[1,1] / cross_table.iloc[0,1] if cross_table.iloc[0,1] != 0 else 0\n",
    "            recall = cross_table.iloc[1,1] / cross_table.iloc[1,0] if cross_table.iloc[1,0] != 0 else 0\n",
    "            f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "\n",
    "            # print the metrics\n",
    "            print(\"Threshhold:\", i)\n",
    "            print(\"Accuracy:\", accuracy)\n",
    "            print(\"Precision:\", precision)\n",
    "            print(\"Recall:\", recall)\n",
    "            print(\"F1 score:\", f1_score)\n",
    "            print(cross_table)\n",
    "            print(\"\\n\")\n",
    "\n",
    "def get_metrics_df_4(df_text, lexicon, min_treshhold, max_treshhold, jump, Lexicon_name):    \n",
    "    Accuracy = []\n",
    "    Precision = []\n",
    "    Recall = []\n",
    "    F1_score = []\n",
    "    Name = []\n",
    "    Treshhold = []\n",
    "    Technique = []\n",
    "    \n",
    "    for num in range(int((max_treshhold - min_treshhold) / jump) + 1):\n",
    "        i = min_treshhold + num * jump\n",
    "        df = lexicon_climate_classifier_4(df_text, lexicon, i)\n",
    "        cross_table = pd.crosstab(df['Target'], df['Target Lexicon'], margins=True)\n",
    "\n",
    "        # calculate classification metrics using scikit-learn\n",
    "        accuracy = (cross_table.iloc[0, 0] + cross_table.iloc[1, 1]) / cross_table.loc['All', 'All'] if cross_table.shape == (3,3) else cross_table.iloc[1,0] / (cross_table.iloc[1, 0] + cross_table.iloc[0,0]) \n",
    "        precision = cross_table.iloc[1,1] / cross_table.iloc[0,1] if cross_table.iloc[0,1] != 0 else 0\n",
    "        recall = cross_table.iloc[1,1] / cross_table.iloc[1,0] if cross_table.iloc[1,0] != 0 else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "        \n",
    "        Accuracy.append(accuracy)\n",
    "        Precision.append(precision)\n",
    "        Recall.append(recall)\n",
    "        F1_score.append(f1_score)\n",
    "        Name.append(Lexicon_name)\n",
    "        Treshhold.append(i)\n",
    "        Technique.append(\"Relative Presences\")\n",
    "        \n",
    "    return(pd.DataFrame({\"Lexicon\" : Name, \"Technique\": Technique, \"Treshhold\" : Treshhold, \"Accuracy\" : Accuracy, \"Precision\" : Precision, \"Recall\" : Recall, \"F1 Score\" : F1_score}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358383a2",
   "metadata": {},
   "source": [
    "Accuracy: This metric measures the overall performance of a model. It is defined as the number of correct predictions divided by the total number of predictions. Accuracy is a good metric to use when the classes are roughly balanced, meaning there are about the same number of positive and negative examples in the dataset.\n",
    "\n",
    "Precision: This metric measures how many of the positive predictions made by a model are actually correct. It is defined as the number of true positives divided by the total number of positive predictions. Precision is a good metric to use when we care more about avoiding false positives than false negatives.\n",
    "\n",
    "Recall: This metric measures how many of the positive examples in the dataset are correctly predicted by the model. It is defined as the number of true positives divided by the total number of actual positive examples. Recall is a good metric to use when we care more about avoiding false negatives than false positives.\n",
    "\n",
    "F1 score: This metric is a weighted average of precision and recall, where the weight is determined by the beta parameter. The most common value for beta is 1, which gives equal weight to precision and recall. The F1 score is a good metric to use when we want to balance precision and recall, and when the classes are imbalanced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc10b67",
   "metadata": {},
   "source": [
    "# 1. Import Label Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b08d6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_climate_df = pd.read_parquet(\"Climate_Labels_Dataset.parquet\")\n",
    "tag_climate_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9d1d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only keep the required columns\n",
    "tag_climate_df = tag_climate_df[[\"Text\", \"Final_Climate_Change_Level_Label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ee616f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean the tabel\n",
    "tag_climate_df['Final_Climate_Change_Level_Label'] = tag_climate_df['Final_Climate_Change_Level_Label'].str.strip()\n",
    "tag_climate_df[tag_climate_df[\"Final_Climate_Change_Level_Label\"] == \"NA\"] = \"Na\"\n",
    "tag_climate_df[tag_climate_df[\"Final_Climate_Change_Level_Label\"] == \"0\"] = \"Na\"\n",
    "tag_climate_df[\"Target\"] = tag_climate_df[\"Final_Climate_Change_Level_Label\"].apply(lambda x: \"Yes\" if x in [\"High\", \"Medium\"] else \"No\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add6c615",
   "metadata": {},
   "outputs": [],
   "source": [
    "overview_labels_hms = tag_climate_df.groupby(\"Final_Climate_Change_Level_Label\")[\"Text\"].count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c689ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "overview_labels_hms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be23a6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "overview_labels = tag_climate_df.groupby(\"Target\")[\"Text\"].count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86308d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "overview_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf33a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#overview_labels_hms.to_csv(\"C:/Users/Boedt/OneDrive/Bureaublad/R Thesis/overview_tag_labels_hms\", index = False)\n",
    "#overview_labels.to_csv(\"C:/Users/Boedt/OneDrive/Bureaublad/R Thesis/overview_tag_labels\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516660b2",
   "metadata": {},
   "source": [
    "# 2. Taggers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2a7077",
   "metadata": {},
   "source": [
    "## 2.1. Lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31249337",
   "metadata": {},
   "source": [
    "### Global Change Lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c15452",
   "metadata": {},
   "source": [
    "Uitleg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "54704fcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lexicon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100-year flood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>emissions scenario</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>adaptation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>adaptation science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>adaptive capacity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>vector</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>vulnerability</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>vulnerability assessment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>water security</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>water stress</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>105 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Lexicon\n",
       "0              100-year flood\n",
       "1          emissions scenario\n",
       "2                  adaptation\n",
       "3          adaptation science\n",
       "4           adaptive capacity\n",
       "..                        ...\n",
       "100                    vector\n",
       "101             vulnerability\n",
       "102  vulnerability assessment\n",
       "103            water security\n",
       "104              water stress\n",
       "\n",
       "[105 rows x 1 columns]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the lexicon\n",
    "Global_Change_Lexicon = pd.read_csv(\"Global_Change_Lexicon\")\n",
    "Global_Change_Lexicon = Global_Change_Lexicon.drop_duplicates().reset_index(drop = True)\n",
    "Global_Change_Lexicon[\"Lexicon\"] = Global_Change_Lexicon[\"Lexicon\"].str.lower()\n",
    "\n",
    "Global_Change_Lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d26ff50",
   "metadata": {},
   "source": [
    "### IPCC Lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800c5ab8",
   "metadata": {},
   "source": [
    "Uitleg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "c41bb7fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lexicon</th>\n",
       "      <th>Afkorting</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>acceptability of policy or system change</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adaptability</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>adaptation</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>adaptation behaviour</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>adaptation limits</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>sd</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>sdgs</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>tcre</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>tod</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>unfccc</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>405 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Lexicon Afkorting\n",
       "0    acceptability of policy or system change        No\n",
       "1                                adaptability        No\n",
       "2                                  adaptation        No\n",
       "3                        adaptation behaviour        No\n",
       "4                           adaptation limits        No\n",
       "..                                        ...       ...\n",
       "400                                       sd        Yes\n",
       "401                                     sdgs        Yes\n",
       "402                                     tcre        Yes\n",
       "403                                      tod        Yes\n",
       "404                                   unfccc        Yes\n",
       "\n",
       "[405 rows x 2 columns]"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the lexicon\n",
    "IPCC_Lexicon = pd.read_csv(\"IPCC_Lexicon\")\n",
    "IPCC_Lexicon = IPCC_Lexicon.drop_duplicates().reset_index(drop = True)\n",
    "IPCC_Lexicon[\"Lexicon\"] = IPCC_Lexicon[\"Lexicon\"].str.lower()\n",
    "\n",
    "IPCC_Lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb16e34",
   "metadata": {},
   "source": [
    "### Wikipedia Lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90610cf6",
   "metadata": {},
   "source": [
    "Uitleg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "335d527b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lexicon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100,000-year problem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adaptation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>additionality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>albedo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>anoxic event</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>volcanism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>water vapor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>weather</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>world climate report</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>younger dryas</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>164 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Lexicon\n",
       "0    100,000-year problem\n",
       "1              adaptation\n",
       "2           additionality\n",
       "3                  albedo\n",
       "4            anoxic event\n",
       "..                    ...\n",
       "159             volcanism\n",
       "160           water vapor\n",
       "161               weather\n",
       "162  world climate report\n",
       "163         younger dryas\n",
       "\n",
       "[164 rows x 1 columns]"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the lexicon\n",
    "Wikipedia_Lexicon = pd.read_csv(\"Wikipedia_Lexicon\")\n",
    "Wikipedia_Lexicon = Wikipedia_Lexicon.drop_duplicates().reset_index(drop = True)\n",
    "Wikipedia_Lexicon[\"Lexicon\"] = Wikipedia_Lexicon[\"Lexicon\"].str.lower()\n",
    "\n",
    "Wikipedia_Lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c8a3be",
   "metadata": {},
   "source": [
    "### EPA Lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638f81be",
   "metadata": {},
   "source": [
    "Uitleg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "e93d019b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abrupt climate change',\n",
       " 'adaptation',\n",
       " 'adaptive capacity',\n",
       " 'aerosols',\n",
       " 'afforestation',\n",
       " 'albedo',\n",
       " 'alternative energy',\n",
       " 'annex i countries/parties',\n",
       " 'anthropogenic',\n",
       " 'atmosphere',\n",
       " 'atmospheric lifetime',\n",
       " 'biofuels',\n",
       " 'biogeochemical cycle',\n",
       " 'biomass',\n",
       " 'biosphere',\n",
       " 'black carbon aerosol',\n",
       " 'borehole',\n",
       " 'carbon cycle',\n",
       " 'carbon dioxide',\n",
       " 'carbon dioxide equivalent',\n",
       " 'carbon dioxide fertilization',\n",
       " 'carbon footprint',\n",
       " 'carbon sequestration',\n",
       " 'carbon capture and sequestration',\n",
       " 'chlorofluorocarbons',\n",
       " 'climate',\n",
       " 'climate change',\n",
       " 'climate feedback',\n",
       " 'climate lag',\n",
       " 'climate model',\n",
       " 'climate sensitivity',\n",
       " 'climate system',\n",
       " 'coal mine methane',\n",
       " 'coalbed methane',\n",
       " 'co-benefit',\n",
       " 'concentration',\n",
       " 'conference of the parties',\n",
       " 'coral bleaching',\n",
       " 'cryosphere',\n",
       " 'deforestation',\n",
       " 'desertification',\n",
       " 'dryland farming',\n",
       " 'eccentricity',\n",
       " 'ecosystem',\n",
       " 'el niño - southern oscillation',\n",
       " 'emissions',\n",
       " 'emissions factor',\n",
       " 'energy efficiency',\n",
       " 'energy star',\n",
       " 'enhanced greenhouse effect',\n",
       " 'enteric fermentation',\n",
       " 'evaporation',\n",
       " 'evapotranspiration',\n",
       " 'feedback mechanisms',\n",
       " 'fluorinated gases',\n",
       " 'fluorocarbons',\n",
       " 'forcing mechanism',\n",
       " 'fossil fuel',\n",
       " 'fuel switching',\n",
       " 'general circulation model',\n",
       " 'geosphere',\n",
       " 'glacier',\n",
       " 'global average temperature',\n",
       " 'global warming',\n",
       " 'global warming potential',\n",
       " 'greenhouse effect',\n",
       " 'greenhouse gas',\n",
       " 'habitat fragmentation',\n",
       " 'halocarbons',\n",
       " 'heat island',\n",
       " 'heat waves',\n",
       " 'hydrocarbons',\n",
       " 'hydrochlorofluorocarbons',\n",
       " 'hydrofluorocarbons',\n",
       " 'hydrologic cycle',\n",
       " 'hydrosphere',\n",
       " 'ice core',\n",
       " 'indirect emissions',\n",
       " 'industrial revolution',\n",
       " 'infrared radiation',\n",
       " 'intergovernmental panel on climate change',\n",
       " 'inundation',\n",
       " 'landfill',\n",
       " 'latitude',\n",
       " 'least developed country',\n",
       " 'longwave radiation',\n",
       " 'megacities',\n",
       " 'methane',\n",
       " 'metric ton',\n",
       " 'mitigation',\n",
       " 'mount pinatubo',\n",
       " 'municipal solid waste',\n",
       " 'natural gas',\n",
       " 'natural variability',\n",
       " 'nitrogen cycle',\n",
       " 'nitrogen oxides',\n",
       " 'nitrous oxide',\n",
       " 'non-methane volatile organic compounds',\n",
       " 'ocean acidification',\n",
       " 'oxidize',\n",
       " 'ozone',\n",
       " 'ozone depleting substance',\n",
       " 'ozone layer',\n",
       " 'ozone precursors',\n",
       " 'particulate matter',\n",
       " 'parts per billion',\n",
       " 'parts per million by volume',\n",
       " 'parts per trillion',\n",
       " 'perfluorocarbons',\n",
       " 'permafrost',\n",
       " 'phenology',\n",
       " 'photosynthesis',\n",
       " 'precession',\n",
       " 'radiation',\n",
       " 'radiative forcing',\n",
       " 'recycling',\n",
       " 'reflectivity',\n",
       " 'reforestation',\n",
       " 'relative sea level rise',\n",
       " 'renewable energy',\n",
       " 'residence time',\n",
       " 'resilience',\n",
       " 'respiration',\n",
       " 'salt water intrusion',\n",
       " 'scenarios',\n",
       " 'sea surface temperature',\n",
       " 'sensitivity',\n",
       " 'short ton',\n",
       " 'sink',\n",
       " 'snowpack',\n",
       " 'soil carbon',\n",
       " 'solar radiation',\n",
       " 'storm surge',\n",
       " 'stratosphere',\n",
       " 'stratospheric ozone',\n",
       " 'streamflow',\n",
       " 'subsiding/subsidence',\n",
       " 'sulfate aerosols',\n",
       " 'sulfur hexafluoride',\n",
       " 'teragram',\n",
       " 'thermal expansion',\n",
       " 'thermohaline circulation',\n",
       " 'trace gas',\n",
       " 'troposphere',\n",
       " 'tropospheric ozone',\n",
       " 'tropospheric ozone precursors',\n",
       " 'tundra',\n",
       " 'ultraviolet radiation',\n",
       " 'united nations framework convention on climate change',\n",
       " 'vulnerability',\n",
       " 'wastewater',\n",
       " 'water vapor',\n",
       " 'weather',\n",
       " '100-year flood levels',\n",
       " 'or earth system',\n",
       " ' enso ',\n",
       " ' gcm ',\n",
       " ' ghg ',\n",
       " ' hcfcs ',\n",
       " ' hfcs ',\n",
       " ' ipcc ',\n",
       " ' ch4 ',\n",
       " ' msw ',\n",
       " ' nox ',\n",
       " ' n2o ',\n",
       " ' nmvocs ',\n",
       " ' ods ',\n",
       " ' pm ',\n",
       " ' ppb ',\n",
       " ' ppmv ',\n",
       " ' ppt ',\n",
       " ' pfcs ',\n",
       " ' sf6 ',\n",
       " ' o3 ',\n",
       " ' uv ',\n",
       " ' unfccc ']"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the lexicon\n",
    "EPA_Lexicon = pd.read_csv(\"EPA_Lexicon\")\n",
    "EPA_Lexicon = EPA_Lexicon.drop_duplicates().reset_index(drop = True)\n",
    "EPA_Lexicon[\"Lexicon\"] = EPA_Lexicon[\"Lexicon\"].str.lower()\n",
    "\n",
    "EPA_Lexicon\n",
    "\n",
    "list(EPA_Lexicon[\"Lexicon\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5e47f8",
   "metadata": {},
   "source": [
    "### BBC Lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b284bd04",
   "metadata": {},
   "source": [
    "Uitleg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "7ea125ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lexicon</th>\n",
       "      <th>Afkorting</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adaptation</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adaptation fund</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>annex i countries</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>annex ii</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>anthropogenic climate change</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>gwp</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>ghgs</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>ji</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>350/450</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>20-20-20</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>79 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Lexicon Afkorting\n",
       "0                     adaptation        No\n",
       "1                adaptation fund        No\n",
       "2              annex i countries        No\n",
       "3                       annex ii        No\n",
       "4   anthropogenic climate change        No\n",
       "..                           ...       ...\n",
       "74                           gwp       Yes\n",
       "75                          ghgs       Yes\n",
       "76                            ji       Yes\n",
       "77                       350/450        No\n",
       "78                      20-20-20        No\n",
       "\n",
       "[79 rows x 2 columns]"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the lexicon\n",
    "BBC_Lexicon = pd.read_csv(\"BBC_Lexicon\")\n",
    "BBC_Lexicon = BBC_Lexicon.drop_duplicates().reset_index(drop = True)\n",
    "BBC_Lexicon[\"Lexicon\"] = BBC_Lexicon[\"Lexicon\"].str.lower()\n",
    "\n",
    "BBC_Lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80aade8b",
   "metadata": {},
   "source": [
    "### UNDP Lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bae000f",
   "metadata": {},
   "source": [
    "Uitleg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "f66d51e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lexicon</th>\n",
       "      <th>Afkorting</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>weather</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>climate</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>greenhouse gases</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>greenhouse gas emmisions</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>global warming</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>climate change</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>climate crisis</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>feedback loop</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>tipping point</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>climate overshoot</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>mitigation</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>adaptation</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>resilience</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>carbon footprint</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>climate justice</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>nature-based solutions</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>indigenous knowledge</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>loss and damage</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>climate security</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>climate finance</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>net zero</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>decarbonization</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>renewable energy</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>carbon sink</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>carbon removal</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>carbon capture</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>carbon markets</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>regenerative agriculture</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>reforestation</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>afforestation</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>rewilding</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>circular economy</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>blue economy</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>green jobs</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>greenwashing</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>just transition</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>unfccc</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>conference of the parties</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>cop</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>paris agreement</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>nationally determined contributions</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>transparent reporting</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>transparency</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>national adaptation plans</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>long-term strategies</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>redd+</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>intergovernmental panel on climate change</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Lexicon Afkorting\n",
       "0                                     weather        No\n",
       "1                                     climate        No\n",
       "2                            greenhouse gases        No\n",
       "3                    greenhouse gas emmisions        No\n",
       "4                              global warming        No\n",
       "5                              climate change        No\n",
       "6                              climate crisis        No\n",
       "7                               feedback loop        No\n",
       "8                               tipping point        No\n",
       "9                           climate overshoot        No\n",
       "10                                 mitigation        No\n",
       "11                                 adaptation        No\n",
       "12                                 resilience        No\n",
       "13                           carbon footprint        No\n",
       "14                            climate justice        No\n",
       "15                     nature-based solutions        No\n",
       "16                       indigenous knowledge        No\n",
       "17                            loss and damage        No\n",
       "18                           climate security        No\n",
       "19                            climate finance        No\n",
       "20                                   net zero        No\n",
       "21                            decarbonization        No\n",
       "22                           renewable energy        No\n",
       "23                                carbon sink        No\n",
       "24                             carbon removal        No\n",
       "25                             carbon capture        No\n",
       "26                             carbon markets        No\n",
       "27                   regenerative agriculture        No\n",
       "28                              reforestation        No\n",
       "29                              afforestation        No\n",
       "30                                  rewilding        No\n",
       "31                           circular economy        No\n",
       "32                               blue economy        No\n",
       "33                                 green jobs        No\n",
       "34                               greenwashing        No\n",
       "35                            just transition        No\n",
       "36                                     unfccc       Yes\n",
       "37                  conference of the parties        No\n",
       "38                                        cop       Yes\n",
       "39                            paris agreement        No\n",
       "40        nationally determined contributions        No\n",
       "41                      transparent reporting        No\n",
       "42                               transparency        No\n",
       "43                  national adaptation plans        No\n",
       "44                       long-term strategies        No\n",
       "45                                      redd+       Yes\n",
       "46  intergovernmental panel on climate change        No"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the lexicon\n",
    "UNDP_Lexicon = pd.read_csv(\"UNDP_Lexicon\")\n",
    "UNDP_Lexicon = UNDP_Lexicon.drop_duplicates().reset_index(drop = True)\n",
    "UNDP_Lexicon[\"Lexicon\"] = UNDP_Lexicon[\"Lexicon\"].str.lower()\n",
    "\n",
    "UNDP_Lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d668aa",
   "metadata": {},
   "source": [
    "### Compare the lexicons to each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "43089e67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lexicon</th>\n",
       "      <th>Global Change</th>\n",
       "      <th>IPCC</th>\n",
       "      <th>Wikipedia</th>\n",
       "      <th>EPA</th>\n",
       "      <th>BBC</th>\n",
       "      <th>UNDP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Global Change</td>\n",
       "      <td>105</td>\n",
       "      <td>38</td>\n",
       "      <td>12</td>\n",
       "      <td>20</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IPCC</td>\n",
       "      <td>38</td>\n",
       "      <td>405</td>\n",
       "      <td>28</td>\n",
       "      <td>46</td>\n",
       "      <td>22</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>12</td>\n",
       "      <td>28</td>\n",
       "      <td>164</td>\n",
       "      <td>33</td>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EPA</td>\n",
       "      <td>20</td>\n",
       "      <td>46</td>\n",
       "      <td>33</td>\n",
       "      <td>176</td>\n",
       "      <td>16</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BBC</td>\n",
       "      <td>11</td>\n",
       "      <td>22</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>79</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>UNDP</td>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>11</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Lexicon  Global Change  IPCC  Wikipedia  EPA  BBC  UNDP\n",
       "0  Global Change            105    38         12   20   11     7\n",
       "1           IPCC             38   405         28   46   22    19\n",
       "2      Wikipedia             12    28        164   33   15    10\n",
       "3            EPA             20    46         33  176   16    14\n",
       "4            BBC             11    22         15   16   79    11\n",
       "5           UNDP              7    19         10   14   11    47"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create an empty dataframe and write a function to fill with the values\n",
    "\n",
    "common_words_df = pd.DataFrame({\"Lexicon\" : [\"Global Change\", \"IPCC\", \"Wikipedia\", \"EPA\", \"BBC\", \"UNDP\"], \n",
    "                               \"Global Change\": [0, 0, 0, 0, 0, 0], \"IPCC\" : [0, 0, 0, 0, 0, 0], \n",
    "                               \"Wikipedia\" : [0, 0, 0, 0, 0, 0], \"EPA\" : [0, 0, 0, 0, 0, 0], \n",
    "                               \"BBC\" : [0, 0, 0, 0, 0, 0], \"UNDP\" : [0, 0, 0, 0, 0, 0]})\n",
    "\n",
    "dfs = [Global_Change_Lexicon, IPCC_Lexicon, Wikipedia_Lexicon, EPA_Lexicon, BBC_Lexicon, UNDP_Lexicon]\n",
    "\n",
    "for r in range(0, len(dfs)):\n",
    "    for c in range(0, len(dfs)):\n",
    "        # Get the common values between the two columns\n",
    "        common_words = set(dfs[r]['Lexicon']).intersection(set(dfs[c]['Lexicon']))\n",
    "        common_words_df.loc[r, common_words_df.columns[c +1]] = len(common_words)\n",
    "\n",
    "common_words_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f304b018",
   "metadata": {},
   "source": [
    "## 2.2. Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158a8097",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary(df_with_text, name, model_name, max_lenght_input=-1):\n",
    "    data_in_list = df_with_text[name].tolist()\n",
    "    tokenizer_sum = AutoTokenizer.from_pretrained(model_name)\n",
    "    model_sum = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "    summarizer = pipeline('summarization', model=model_sum, tokenizer = tokenizer_sum) \n",
    "\n",
    "    if max_lenght_input>=0:\n",
    "        df_with_text['summary'] = summarizer(data_in_list, max_length=max_lenght_input)\n",
    "\n",
    "    else:\n",
    "        df_with_text['summary'] = summarizer(data_in_list)\n",
    "\n",
    "def classification(df_with_text, name, model_name, max_lenght_input=-1):\n",
    "    data_in_list = df_with_text[name].tolist()\n",
    "    tokenizer_clas = AutoTokenizer.from_pretrained(model_name)\n",
    "    model_clas = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "    classification = pipeline('text-classification', model=model_clas, tokenizer = tokenizer_clas) \n",
    "\n",
    "    if max_lenght_input>=0:\n",
    "        df_with_text['classification'] = classification(data_in_list, max_length=max_lenght_input, truncation=True)\n",
    "\n",
    "    else:\n",
    "        df_with_text['classification'] = classification(data_in_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa8d3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#classification(Lexicon_df, 'Text',\"climatebert/environmental-claims\",512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53737e8c",
   "metadata": {},
   "source": [
    "# 3. Testen Taggers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b72304",
   "metadata": {},
   "source": [
    "## 3.1. Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "cca33293",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a separate df with the specific cleaning for the lexicons\n",
    "Lexicon_df = tag_climate_df.copy()\n",
    "Lexicon_df[\"Text\"] = Lexicon_df[\"Text\"].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bc8d45",
   "metadata": {},
   "source": [
    "### 3.1.1. One Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "61e0d1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global Change Lexicon\n",
    "Global_Change_df_1 = get_metrics_df_1(Lexicon_df, Global_Change_Lexicon, 0, 20, \"Global Change\")\n",
    "Global_Change_df_2 = get_metrics_df_2(Lexicon_df, Global_Change_Lexicon, 0, 2, 0.1, \"Global Change\")\n",
    "Global_Change_df_3 = get_metrics_df_3(Lexicon_df, Global_Change_Lexicon, 0, 20, \"Global Change\")\n",
    "Global_Change_df_4 = get_metrics_df_4(Lexicon_df, Global_Change_Lexicon, 0, 2, 0.1, \"Global Change\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "ebbd51ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "Global_Change_df = pd.concat([Global_Change_df_1, Global_Change_df_2, Global_Change_df_3, Global_Change_df_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "b48f7730",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IPCC Lexicon\n",
    "IPCC_df_1 = get_metrics_df_1(Lexicon_df, IPCC_Lexicon, 0, 20, \"IPCC\")\n",
    "IPCC_df_2 = get_metrics_df_2(Lexicon_df, IPCC_Lexicon, 0, 2, 0.1, \"IPCC\")\n",
    "IPCC_df_3 = get_metrics_df_3(Lexicon_df, IPCC_Lexicon, 0, 20, \"IPCC\")\n",
    "IPCC_df_4 = get_metrics_df_4(Lexicon_df, IPCC_Lexicon, 0, 2, 0.1, \"IPCC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "83e242f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "IPCC_df = pd.concat([IPCC_df_1, IPCC_df_2, IPCC_df_3, IPCC_df_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "e5c72d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wikipedia Lexicon\n",
    "Wikipedia_df_1 = get_metrics_df_1(Lexicon_df, Wikipedia_Lexicon, 0, 20, \"Wikipedia\")\n",
    "Wikipedia_df_2 = get_metrics_df_2(Lexicon_df, Wikipedia_Lexicon, 0, 2, 0.1, \"Wikipedia\")\n",
    "Wikipedia_df_3 = get_metrics_df_3(Lexicon_df, Wikipedia_Lexicon, 0, 20, \"Wikipedia\")\n",
    "Wikipedia_df_4 = get_metrics_df_4(Lexicon_df, Wikipedia_Lexicon, 0, 2, 0.1, \"Wikipedia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "b9e1474f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Wikipedia_df = pd.concat([Wikipedia_df_1, Wikipedia_df_2, Wikipedia_df_3, Wikipedia_df_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9caa4baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EPA Lexicon\n",
    "EPA_df_1 = get_metrics_df_1(Lexicon_df, EPA_Lexicon, 0, 20, \"EPA\")\n",
    "EPA_df_2 = get_metrics_df_2(Lexicon_df, EPA_Lexicon, 0, 2, 0.1, \"EPA\")\n",
    "EPA_df_3 = get_metrics_df_3(Lexicon_df, EPA_Lexicon, 0, 20, \"EPA\")\n",
    "EPA_df_4 = get_metrics_df_4(Lexicon_df, EPA_Lexicon, 0, 2, 0.1, \"EPA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920067f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPA_df = pd.concat([EPA_df_1, EPA_df_2, EPA_df_3, EPA_df_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a445db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BBC Lexicon\n",
    "BBC_df_1 = get_metrics_df_1(Lexicon_df, BBC_Lexicon, 0, 20, \"BBC\")\n",
    "BBC_df_2 = get_metrics_df_2(Lexicon_df, BBC_Lexicon, 0, 2, 0.1, \"BBC\")\n",
    "BBC_df_3 = get_metrics_df_3(Lexicon_df, BBC_Lexicon, 0, 20, \"BBC\")\n",
    "BBC_df_4 = get_metrics_df_4(Lexicon_df, BBC_Lexicon, 0, 2, 0.1, \"BBC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec49a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "BBC_df = pd.concat([BBC_df_1, BBC_df_2, BBC_df_3, BBC_df_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd27f05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_metrics_2(Lexicon_df, UNDP_Lexicon, 0.6, 0.8, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6e7409",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_lexicon_words_1(Lexicon_df, UNDP_Lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8f351e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#UNDP Lexicon\n",
    "UNDP_df_1 = get_metrics_df_1(Lexicon_df, UNDP_Lexicon, 0, 20, \"UNDP\")\n",
    "UNDP_df_2 = get_metrics_df_2(Lexicon_df, UNDP_Lexicon, 0, 2, 0.1, \"UNDP\")\n",
    "UNDP_df_3 = get_metrics_df_3(Lexicon_df, UNDP_Lexicon, 0, 20, \"UNDP\")\n",
    "UNDP_df_4 = get_metrics_df_4(Lexicon_df, UNDP_Lexicon, 0, 2, 0.1, \"UNDP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da66cd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNDP_df = pd.concat([UNDP_df_1, UNDP_df_2, UNDP_df_3, UNDP_df_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917f4664",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get all lexicons together\n",
    "Lexicon_df_1 = pd.concat([Global_Change_df, IPCC_df, Wikipedia_df, EPA_df, BBC_df, UNDP_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea244ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lexicon_df_1.sort_values(\"Accuracy\", ascending = False).reset_index(drop = True).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a074a4f",
   "metadata": {},
   "source": [
    "### 3.1.2. Two Lexicons Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ae84f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#UNDP and EPA\n",
    "EPA_UNDP_Lexicon = pd.concat([EPA_Lexicon, UNDP_Lexicon]).drop_duplicates().reset_index(drop = True)\n",
    "EPA_UNDP_df_1 = get_metrics_df_1(Lexicon_df, EPA_UNDP_Lexicon, 0, 20, \"EPA_UDNP\")\n",
    "EPA_UNDP_df_2 = get_metrics_df_2(Lexicon_df, EPA_UNDP_Lexicon, 0, 2, 0.1, \"EPA_UDNP\")\n",
    "EPA_UNDP_df_3 = get_metrics_df_3(Lexicon_df, EPA_UNDP_Lexicon, 0, 20, \"EPA_UDNP\")\n",
    "EPA_UNDP_df_4 = get_metrics_df_4(Lexicon_df, EPA_UNDP_Lexicon, 0, 2, 0.1, \"EPA_UDNP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf1520b",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPA_UNDP_df = pd.concat([EPA_UNDP_df_1, EPA_UNDP_df_2, EPA_UNDP_df_3, EPA_UNDP_df_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418d74b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#UNDP and BBC\n",
    "BBC_UNDP_Lexicon = pd.concat([BBC_Lexicon, UNDP_Lexicon]).drop_duplicates().reset_index(drop = True)\n",
    "BBC_UNDP_df_1 = get_metrics_df_1(Lexicon_df, BBC_UNDP_Lexicon, 0, 20, \"BBC_UNDP\")\n",
    "BBC_UNDP_df_2 = get_metrics_df_2(Lexicon_df, BBC_UNDP_Lexicon, 0, 2, 0.1, \"BBC_UNDP\")\n",
    "BBC_UNDP_df_3 = get_metrics_df_3(Lexicon_df, BBC_UNDP_Lexicon, 0, 20, \"BBC_UNDP\")\n",
    "BBC_UNDP_df_4 = get_metrics_df_4(Lexicon_df, BBC_UNDP_Lexicon, 0, 2, 0.1, \"BBC_UNDP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56595876",
   "metadata": {},
   "outputs": [],
   "source": [
    "BBC_UNDP_df = pd.concat([BBC_UNDP_df_1, BBC_UNDP_df_2, BBC_UNDP_df_3, BBC_UNDP_df_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dc494f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#UNDP and Global Change \n",
    "UNDP_Global_Change_Lexicon = pd.concat([Global_Change_Lexicon, UNDP_Lexicon]).drop_duplicates().reset_index(drop = True)\n",
    "UNDP_Global_Change_df_1 = get_metrics_df_1(Lexicon_df, UNDP_Global_Change_Lexicon, 0, 20, \"UNDP_Global_Change\")\n",
    "UNDP_Global_Change_df_2 = get_metrics_df_2(Lexicon_df, UNDP_Global_Change_Lexicon, 0, 2, 0.1, \"UNDP_Global_Change\")\n",
    "UNDP_Global_Change_df_3 = get_metrics_df_3(Lexicon_df, UNDP_Global_Change_Lexicon, 0, 20, \"UNDP_Global_Change\")\n",
    "UNDP_Global_Change_df_4 = get_metrics_df_4(Lexicon_df, UNDP_Global_Change_Lexicon, 0, 2, 0.1, \"UNDP_Global_Change\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cbdbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "Global_Change_UNDP_df = pd.concat([UNDP_Global_Change_df_1, UNDP_Global_Change_df_2, UNDP_Global_Change_df_3, UNDP_Global_Change_df_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984b718c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Global_Change_UNDP_df.sort_values(\"Accuracy\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7def3b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#UNDP and IPCC\n",
    "UNDP_IPCC_Lexicon = pd.concat([IPCC_Lexicon, UNDP_Lexicon]).drop_duplicates().reset_index(drop = True)\n",
    "UNDP_IPCC_df_1 = get_metrics_df_1(Lexicon_df, UNDP_IPCC_Lexicon, 0, 20, \"UNDP_IPCC\")\n",
    "UNDP_IPCC_df_2 = get_metrics_df_2(Lexicon_df, UNDP_IPCC_Lexicon, 0, 2, 0.1, \"UNDP_IPCC\")\n",
    "UNDP_IPCC_df_3 = get_metrics_df_3(Lexicon_df, UNDP_IPCC_Lexicon, 0, 20, \"UNDP_IPCC\")\n",
    "UNDP_IPCC_df_4 = get_metrics_df_4(Lexicon_df, UNDP_IPCC_Lexicon, 0, 2, 0.1, \"UNDP_IPCC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c4f888",
   "metadata": {},
   "outputs": [],
   "source": [
    "IPCC_UNDP_df = pd.concat([UNDP_IPCC_df_1, UNDP_IPCC_df_2, UNDP_IPCC_df_3, UNDP_IPCC_df_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881f064f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#UNDP and Wikipedia\n",
    "UNDP_Wikipedia_Lexicon = pd.concat([Wikipedia_Lexicon, UNDP_Lexicon]).drop_duplicates().reset_index(drop = True)\n",
    "UNDP_Wikipedia_df_1 = get_metrics_df_1(Lexicon_df, UNDP_Wikipedia_Lexicon, 0, 20, \"UNDP_Wikipedia\")\n",
    "UNDP_Wikipedia_df_2 = get_metrics_df_2(Lexicon_df, UNDP_Wikipedia_Lexicon, 0, 2, 0.1, \"UNDP_Wikipedia\")\n",
    "UNDP_Wikipedia_df_3 = get_metrics_df_3(Lexicon_df, UNDP_Wikipedia_Lexicon, 0, 20, \"UNDP_Wikipedia\")\n",
    "UNDP_Wikipedia_df_4 = get_metrics_df_4(Lexicon_df, UNDP_Wikipedia_Lexicon, 0, 2, 0.1, \"UNDP_Wikipedia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d486bfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Wikipedia_UNDP_df = pd.concat([UNDP_Wikipedia_df_1, UNDP_Wikipedia_df_2, UNDP_Wikipedia_df_3, UNDP_Wikipedia_df_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9373038f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Wikipedia_UNDP_df.sort_values(\"Accuracy\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29be6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lexicon_df_2 = pd.concat([EPA_UNDP_df, BBC_UNDP_df, Global_Change_UNDP_df, IPCC_UNDP_df, Wikipedia_UNDP_df]).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df23872",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lexicon_df_2.sort_values(\"Accuracy\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637fa27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EPA and BBC\n",
    "EPA_BBC_Lexicon = pd.concat([EPA_Lexicon, BBC_Lexicon]).drop_duplicates().reset_index(drop = True)\n",
    "EPA_BBC_df_1 = get_metrics_df_1(Lexicon_df, EPA_BBC_Lexicon, 0, 20, \"BBC_EPA\")\n",
    "EPA_BBC_df_2 = get_metrics_df_2(Lexicon_df, EPA_BBC_Lexicon, 0, 2, 0.1, \"BBC_EPA\")\n",
    "EPA_BBC_df_3 = get_metrics_df_3(Lexicon_df, EPA_BBC_Lexicon, 0, 20, \"BBC_EPA\")\n",
    "EPA_BBC_df_4 = get_metrics_df_4(Lexicon_df, EPA_BBC_Lexicon, 0, 2, 0.1, \"BBC_EPA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5766b98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPA_BBC_df = pd.concat([EPA_BBC_df_1, EPA_BBC_df_2, EPA_BBC_df_3, EPA_BBC_df_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba89991",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPA_BBC_df.sort_values(\"Accuracy\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101535ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EPA and Global Change\n",
    "EPA_Global_Change_Lexicon = pd.concat([EPA_Lexicon, Global_Change_Lexicon]).drop_duplicates().reset_index(drop = True)\n",
    "EPA_Global_Change_df_1 = get_metrics_df_1(Lexicon_df, EPA_Global_Change_Lexicon, 0, 20, \"EPA_GLobal_Change\")\n",
    "EPA_Global_Change_df_2 = get_metrics_df_2(Lexicon_df, EPA_Global_Change_Lexicon, 0, 2, 0.1, \"BBC_Global_Change\")\n",
    "EPA_Global_Change_df_3 = get_metrics_df_3(Lexicon_df, EPA_Global_Change_Lexicon, 0, 20, \"BBC_Global_Change\")\n",
    "EPA_Global_Change_df_4 = get_metrics_df_4(Lexicon_df, EPA_Global_Change_Lexicon, 0, 2, 0.1, \"BBC_Global_Change\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d585045",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPA_Global_Change_df = pd.concat([EPA_Global_Change_df_1, EPA_Global_Change_df_2, EPA_Global_Change_df_3, EPA_Global_Change_df_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4717a31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPA_Global_Change_df.sort_values(\"Accuracy\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab41474e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EPA and IPCC\n",
    "EPA_IPCC_Lexicon = pd.concat([EPA_Lexicon, IPCC_Lexicon]).drop_duplicates().reset_index(drop = True)\n",
    "EPA_IPCC_df_1 = get_metrics_df_1(Lexicon_df, EPA_IPCC_Lexicon, 0, 20, \"EPA_IPCC\")\n",
    "EPA_IPCC_df_2 = get_metrics_df_2(Lexicon_df, EPA_IPCC_Lexicon, 0, 2, 0.1, \"EPA_IPCC\")\n",
    "EPA_IPCC_df_3 = get_metrics_df_3(Lexicon_df, EPA_IPCC_Lexicon, 0, 20, \"EPA_IPCC\")\n",
    "EPA_IPCC_df_4 = get_metrics_df_4(Lexicon_df, EPA_IPCC_Lexicon, 0, 2, 0.1, \"EPA_IPCC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e6d305",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPA_IPCC_df = pd.concat([EPA_IPCC_df_1, EPA_IPCC_df_2, EPA_IPCC_df_3, EPA_IPCC_df_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2431357",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPA_IPCC_df.sort_values(\"Accuracy\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129ced17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EPA and Wikipedia\n",
    "EPA_Wikipedia_Lexicon = pd.concat([EPA_Lexicon, Wikipedia_Lexicon]).drop_duplicates().reset_index(drop = True)\n",
    "EPA_Wikipedia_df_1 = get_metrics_df_1(Lexicon_df, EPA_Wikipedia_Lexicon, 0, 20, \"EPA_Wikipedia\")\n",
    "EPA_Wikipedia_df_2 = get_metrics_df_2(Lexicon_df, EPA_Wikipedia_Lexicon, 0, 2, 0.1, \"EPA_Wikipedia\")\n",
    "EPA_Wikipedia_df_3 = get_metrics_df_3(Lexicon_df, EPA_Wikipedia_Lexicon, 0, 20, \"EPA_Wikipedia\")\n",
    "EPA_Wikipedia_df_4 = get_metrics_df_4(Lexicon_df, EPA_Wikipedia_Lexicon, 0, 2, 0.1, \"EPA_Wikipedia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c1e917",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPA_Wikipedia_df = pd.concat([EPA_Wikipedia_df_1, EPA_Wikipedia_df_2, EPA_Wikipedia_df_3, EPA_Wikipedia_df_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9e53d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPA_Wikipedia_df.sort_values(\"Accuracy\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd66768",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lexicon_df_2 = pd.concat([Lexicon_df_2, EPA_BBC_df, EPA_Global_Change_df, EPA_IPCC_df, EPA_Wikipedia_df]).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af7a07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lexicon_df_2.sort_values(\"Accuracy\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1260c559",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BBC and Global Change\n",
    "#EPA and Wikipedia\n",
    "BBC_Global_Change_Lexicon = pd.concat([BBC_Lexicon, Global_Change_Lexicon]).drop_duplicates().reset_index(drop = True)\n",
    "BBC_Global_Change_df_1 = get_metrics_df_1(Lexicon_df, BBC_Global_Change_Lexicon, 0, 20, \"BBC_Global_Change\")\n",
    "BBC_Global_Change_df_2 = get_metrics_df_2(Lexicon_df, BBC_Global_Change_Lexicon, 0, 2, 0.1, \"BBC_Global_Change\")\n",
    "BBC_Global_Change_df_3 = get_metrics_df_3(Lexicon_df, BBC_Global_Change_Lexicon, 0, 20, \"BBC_Global_Change\")\n",
    "BBC_Global_Change_df_4 = get_metrics_df_4(Lexicon_df, BBC_Global_Change_Lexicon, 0, 2, 0.1, \"BBC_Global_Change\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2024e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "BBC_Global_Change_df = pd.concat([BBC_Global_Change_df_1, BBC_Global_Change_df_2, BBC_Global_Change_df_3, BBC_Global_Change_df_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a152cd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BBC and IPCC\n",
    "BBC_IPCC_Lexicon = pd.concat([BBC_Lexicon, IPCC_Lexicon]).drop_duplicates().reset_index(drop = True)\n",
    "BBC_IPCC_df_1 = get_metrics_df_1(Lexicon_df, BBC_IPCC_Lexicon, 0, 20, \"BBC_IPCC\")\n",
    "BBC_IPCC_df_2 = get_metrics_df_2(Lexicon_df, BBC_IPCC_Lexicon, 0, 2, 0.1, \"BBC_IPCC\")\n",
    "BBC_IPCC_df_3 = get_metrics_df_3(Lexicon_df, BBC_IPCC_Lexicon, 0, 20, \"BBC_IPCC\")\n",
    "BBC_IPCC_df_4 = get_metrics_df_4(Lexicon_df, BBC_IPCC_Lexicon, 0, 2, 0.1, \"BBC_IPCC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f41eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "BBC_IPCC_df = pd.concat([BBC_IPCC_df_1, BBC_IPCC_df_2, BBC_IPCC_df_3, BBC_IPCC_df_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9bd7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BBC and Wikipedia\n",
    "BBC_Wikipedia_Lexicon = pd.concat([BBC_Lexicon, Wikipedia_Lexicon]).drop_duplicates().reset_index(drop = True)\n",
    "BBC_Wikipedia_df_1 = get_metrics_df_1(Lexicon_df, BBC_Wikipedia_Lexicon, 0, 20, \"BBC_Wikipedia\")\n",
    "BBC_Wikipedia_df_2 = get_metrics_df_2(Lexicon_df, BBC_Wikipedia_Lexicon, 0, 2, 0.1, \"BBC_Wikipedia\")\n",
    "BBC_Wikipedia_df_3 = get_metrics_df_3(Lexicon_df, BBC_Wikipedia_Lexicon, 0, 20, \"BBC_Wikipedia\")\n",
    "BBC_Wikipedia_df_4 = get_metrics_df_4(Lexicon_df, BBC_Wikipedia_Lexicon, 0, 2, 0.1, \"BBC_Wikipedia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf1b899",
   "metadata": {},
   "outputs": [],
   "source": [
    "BBC_Wikpedia_df = pd.concat([BBC_Wikipedia_df_1, BBC_Wikipedia_df_2, BBC_Wikipedia_df_3, BBC_Wikipedia_df_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f6aa7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lexicon_df_2 = pd.concat([Lexicon_df_2, BBC_Global_Change_df, BBC_IPCC_df, BBC_Wikpedia_df]).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477a603c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wikipedia and Global Change\n",
    "Wikipedia_Global_Change_Lexicon = pd.concat([Global_Change_Lexicon, Wikipedia_Lexicon]).drop_duplicates().reset_index(drop = True)\n",
    "Wikipedia_Global_Change_df_1 = get_metrics_df_1(Lexicon_df, Wikipedia_Global_Change_Lexicon, 0, 20, \"Wikipedia_Global_Change\")\n",
    "Wikipedia_Global_Change_df_2 = get_metrics_df_2(Lexicon_df, Wikipedia_Global_Change_Lexicon, 0, 2, 0.1, \"Wikipedia_Global_Change\")\n",
    "Wikipedia_Global_Change_df_3 = get_metrics_df_3(Lexicon_df, Wikipedia_Global_Change_Lexicon, 0, 20, \"Wikipedia_Global_Change\")\n",
    "Wikipedia_Global_Change_df_4 = get_metrics_df_4(Lexicon_df, Wikipedia_Global_Change_Lexicon, 0, 2, 0.1, \"Wikipedia_Global_Change\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5ea019",
   "metadata": {},
   "outputs": [],
   "source": [
    "Wikpedia_Global_Change_df = pd.concat([Wikipedia_Global_Change_df_1, Wikipedia_Global_Change_df_2, Wikipedia_Global_Change_df_3, Wikipedia_Global_Change_df_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2287a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wikipedia and IPCC\n",
    "Wikipedia_IPCC_Lexicon = pd.concat([IPCC_Lexicon, Wikipedia_Lexicon]).drop_duplicates().reset_index(drop = True)\n",
    "Wikipedia_IPCC_df_1 = get_metrics_df_1(Lexicon_df, Wikipedia_IPCC_Lexicon, 0, 20, \"Wikipedia_IPCC\")\n",
    "Wikipedia_IPCC_df_2 = get_metrics_df_2(Lexicon_df, Wikipedia_IPCC_Lexicon, 0, 2, 0.1, \"Wikipedia_IPCC\")\n",
    "Wikipedia_IPCC_df_3 = get_metrics_df_3(Lexicon_df, Wikipedia_IPCC_Lexicon, 0, 20, \"Wikipedia_IPCC\")\n",
    "Wikipedia_IPCC_df_4 = get_metrics_df_4(Lexicon_df, Wikipedia_IPCC_Lexicon, 0, 2, 0.1, \"Wikipedia_IPCC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e11a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "Wikpedia_IPCC_df = pd.concat([Wikipedia_IPCC_df_1, Wikipedia_IPCC_df_2, Wikipedia_IPCC_df_3, Wikipedia_IPCC_df_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0198c9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IPCC and Global Change\n",
    "Global_Change_IPCC_Lexicon = pd.concat([IPCC_Lexicon, Global_Change_Lexicon]).drop_duplicates().reset_index(drop = True)\n",
    "Global_Change_IPCC_df_1 = get_metrics_df_1(Lexicon_df, Global_Change_IPCC_Lexicon, 0, 20, \"Global_Change_IPCC\")\n",
    "Global_Change_IPCC_df_2 = get_metrics_df_2(Lexicon_df, Global_Change_IPCC_Lexicon, 0, 2, 0.1, \"Global_Change_IPCC\")\n",
    "Global_Change_IPCC_df_3 = get_metrics_df_3(Lexicon_df, Global_Change_IPCC_Lexicon, 0, 20, \"Global_Change_IPCC\")\n",
    "Global_Change_IPCC_df_4 = get_metrics_df_4(Lexicon_df, Global_Change_IPCC_Lexicon, 0, 2, 0.1, \"Global_Change_IPCC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a0fd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "Global_Change_IPCC_df = pd.concat([Global_Change_IPCC_df_1, Global_Change_IPCC_df_2, Global_Change_IPCC_df_3, Global_Change_IPCC_df_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157d3ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lexicon_df_2 = pd.concat([Lexicon_df_2, Wikpedia_Global_Change_df, Wikpedia_IPCC_df, Global_Change_IPCC_df]).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d5c0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lexicon_df_2.sort_values(\"Precision\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b24048",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lexicon_df = pd.concat([Lexicon_df_1, Lexicon_df_2]).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fe9429",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lexicon_df.sort_values(by=\"Accuracy\", ascending = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
