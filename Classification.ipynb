{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b21dbe2",
   "metadata": {},
   "source": [
    "# 0. Packages & Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0154ec9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "from nltk import ngrams\n",
    "from collections import Counter\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, pipeline, AutoModelForSequenceClassification, AutoModelForSeq2SeqLM\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pyarrow\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5d4be568",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre-process text for lexicon based approaches\n",
    "def preprocess_text(text):\n",
    "    # remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # convert to lower case\n",
    "    text = text.lower()\n",
    "    # remove blank spaces\n",
    "    text = ' '.join(text.split())\n",
    "    # remove newline characters\n",
    "    text = text.replace('\\n', '')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "25fa7ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_lexicon_words(text_df, lexicon):\n",
    "    lexicon = lexicon[\"Lexicon\"]\n",
    "    frequency = []\n",
    "    present = []\n",
    "    rfrequency = []\n",
    "    rpresent = []\n",
    "\n",
    "    for text in text_df[\"Text\"]:\n",
    "        lexicon_counts = 0\n",
    "        present_count = 0\n",
    "        for word in lexicon:\n",
    "            lexicon_counts += text.lower().count(word.lower())\n",
    "            if(text.lower().count(word.lower()) > 0):\n",
    "                present_count += 1\n",
    "        \n",
    "        word_list = text.split() \n",
    "        word_count = len(word_list)\n",
    "\n",
    "        frequency.append(lexicon_counts)\n",
    "        present.append(present_count)\n",
    "        rfrequency.append((lexicon_counts/word_count)*100)\n",
    "        rpresent.append((present_count/word_count)*100)\n",
    "        \n",
    "        \n",
    "    text_df[\"Absolute Frequency\"] = frequency\n",
    "    text_df[\"Absolute Present\"] = present\n",
    "    text_df[\"Relative Frequency\"] = rfrequency\n",
    "    text_df[\"Relative Present\"] = rpresent\n",
    "    \n",
    "    return(text_df)\n",
    "\n",
    "def get_metrics(df, colname, threshold):\n",
    "    target = []\n",
    "    values = df[colname]\n",
    "    \n",
    "    for v in values:\n",
    "        if v >= threshold:\n",
    "            target.append(\"Yes\")\n",
    "        else:\n",
    "            target.append(\"No\")\n",
    "    \n",
    "    df[\"Estimate\"] = target\n",
    "    \n",
    "    cross_table = pd.crosstab(df['Target'], df['Estimate'], margins=True)\n",
    "    accuracy = (cross_table.iloc[0, 0] + cross_table.iloc[1, 1]) / cross_table.loc['All', 'All'] if cross_table.shape == (3,3) else cross_table.iloc[1,0] / (cross_table.iloc[1, 0] + cross_table.iloc[0,0]) \n",
    "    precision = cross_table.iloc[1,1] / (cross_table.iloc[0,1] + cross_table.iloc[1,1]) if cross_table.shape == (3,3) else 0\n",
    "    recall = cross_table.iloc[1,1] / (cross_table.iloc[1,0] + cross_table.iloc[1,1]) if cross_table.shape == (3,3) != 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "    return([accuracy, precision, recall, f1_score])\n",
    "\n",
    "def get_gross_table_data(df, colname, threshold, binary):\n",
    "    target = []\n",
    "    values = df[colname]\n",
    "    \n",
    "    for v in values:\n",
    "        if v >= threshold:\n",
    "            target.append(\"Yes\")\n",
    "        else:\n",
    "            target.append(\"No\")\n",
    "    \n",
    "    df[\"Estimate\"] = target\n",
    "    \n",
    "    if(binary):\n",
    "        cross_table = pd.crosstab(df['Target'], df['Estimate'], margins=True)\n",
    "    else:\n",
    "        cross_table = pd.crosstab(df['Final_Climate_Change_Level_Label'], df['Estimate'], margins=True)\n",
    "    return(cross_table)\n",
    "    \n",
    "def find_optimal_threshold(df, lexicon, lexicon_name):\n",
    "    df = count_lexicon_words(df, lexicon)\n",
    "    \n",
    "    #absolute frequency\n",
    "    af_accuracy = 0\n",
    "    af_th = 0\n",
    "    found = False\n",
    "    while(found == False):\n",
    "        metrics = get_metrics(df, \"Absolute Frequency\", af_th)\n",
    "        if metrics[0] > af_accuracy:\n",
    "            af_accuracy = metrics[0]\n",
    "            af_precision = metrics[1]\n",
    "            af_recall = metrics[2]\n",
    "            af_f1 = metrics[3]\n",
    "            af_th += 1\n",
    "        else:\n",
    "            found = True\n",
    "            \n",
    "    #absolute present\n",
    "    ap_accuracy = 0\n",
    "    ap_th = 0\n",
    "    found = False\n",
    "    while(found == False):\n",
    "        metrics = get_metrics(df, \"Absolute Present\", ap_th)\n",
    "        if metrics[0] > ap_accuracy:\n",
    "            ap_accuracy = metrics[0]\n",
    "            ap_precision = metrics[1]\n",
    "            ap_recall = metrics[2]\n",
    "            ap_f1 = metrics[3]\n",
    "            ap_th += 1\n",
    "        else:\n",
    "            found = True\n",
    "            \n",
    "    #relative frequency\n",
    "    rf_accuracy = 0\n",
    "    rf_th = 0\n",
    "    found = False\n",
    "    while(found == False):\n",
    "        metrics = get_metrics(df, \"Relative Frequency\", rf_th)\n",
    "        if metrics[0] > rf_accuracy:\n",
    "            rf_accuracy = metrics[0]\n",
    "            rf_precision = metrics[1]\n",
    "            rf_recall = metrics[2]\n",
    "            rf_f1 = metrics[3]\n",
    "            rf_th += 0.1\n",
    "        else:\n",
    "            found = True\n",
    "            \n",
    "    #relative present\n",
    "    rp_accuracy = 0\n",
    "    rp_th = 0\n",
    "    found = False\n",
    "    while(found == False):\n",
    "        metrics = get_metrics(df, \"Relative Present\", rp_th)\n",
    "        if metrics[0] > rp_accuracy:\n",
    "            rp_accuracy = metrics[0]\n",
    "            rp_precision = metrics[1]\n",
    "            rp_recall = metrics[2]\n",
    "            rp_f1 = metrics[3]\n",
    "            rp_th += 0.1\n",
    "        else:\n",
    "            found = True\n",
    "    \n",
    "    return(pd.DataFrame({\"Lexicon\" : [lexicon_name] * 4, \n",
    "                         \"Technique\" : [\"Absolute Frequency\", \"Absolute Present\", \"Relative Frequency\", \"Relative Present\"],\n",
    "                         \"Threshold\" : [af_th - 1, ap_th - 1, rf_th - 0.1, rp_th - 0.1], \n",
    "                 \"Accuracy\" : [af_accuracy, ap_accuracy, rf_accuracy, rp_accuracy], \n",
    "                        \"Precision\" : [af_precision, ap_precision, rf_precision, rp_precision], \n",
    "                        \"Recall\" : [af_recall, ap_recall, rf_recall, rp_recall], \n",
    "                        \"F1 Score\" : [af_f1, ap_f1, rf_f1, rp_f1]}))\n",
    "    \n",
    "def test_lexicon(test_df, results_df, lexicon, lexicon_name):\n",
    "    techniques = [\"Absolute Frequency\", \"Absolute Present\", \"Relative Frequency\", \"Relative Present\"]\n",
    "    df = count_lexicon_words(test_df, lexicon)\n",
    "    accuracy = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1 = []\n",
    "    th_df = []\n",
    "    for t in range(len(techniques)):\n",
    "        th = results_df[results_df[\"Technique\"] == techniques[t]][\"Threshold\"].iloc[0]\n",
    "        th_df.append(th)\n",
    "        accuracy.append(get_metrics(df, techniques[t], th)[0])\n",
    "        precision.append(get_metrics(df, techniques[t], th)[1])\n",
    "        recall.append(get_metrics(df, techniques[t], th)[2])\n",
    "        f1.append(get_metrics(df, techniques[t], th)[3])\n",
    "    \n",
    "    return(pd.DataFrame({\"Lexicon\" : [lexicon_name] * 4, \"Technique\" : techniques, \"Threshold\" : th_df ,\n",
    "                         \"Test Accuracy\" : accuracy, \n",
    "                        \"Test Precision\" : precision, \n",
    "                        \"Test Recall\" : recall, \n",
    "                        \"Test F1 Score\" : f1}))\n",
    "\n",
    "\n",
    "\n",
    "def get_cross_table(text_df, lexicon, threshold, colname, binary):\n",
    "    df = count_lexicon_words(text_df, lexicon)\n",
    "    return(get_gross_table_data(df, colname, threshold, binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "25c89c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary(df_with_text, name, model_name, max_lenght_input=-1):\n",
    "    data_in_list = df_with_text[name].tolist()\n",
    "    tokenizer_sum = AutoTokenizer.from_pretrained(model_name)\n",
    "    model_sum = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "    summarizer = pipeline('summarization', model=model_sum, tokenizer = tokenizer_sum) \n",
    "\n",
    "    if max_lenght_input>=0:\n",
    "        df_with_text['summary'] = summarizer(data_in_list, max_length=max_lenght_input)\n",
    "\n",
    "    else:\n",
    "        df_with_text['summary'] = summarizer(data_in_list)\n",
    "\n",
    "def classification(df_with_text, name, model_name, max_lenght_input=-1):\n",
    "    data_in_list = df_with_text[name].tolist()\n",
    "    tokenizer_clas = AutoTokenizer.from_pretrained(model_name)\n",
    "    model_clas = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "    classification = pipeline('text-classification', model=model_clas, tokenizer = tokenizer_clas) \n",
    "\n",
    "    if max_lenght_input>=0:\n",
    "        df_with_text['classification'] = classification(data_in_list, max_length=max_lenght_input, truncation=True)\n",
    "\n",
    "    else:\n",
    "        df_with_text['classification'] = classification(data_in_list)\n",
    "        \n",
    "    return(df_with_text)\n",
    "\n",
    "def get_metrics_hugging_face(text_df, text_column, model, tokens):\n",
    "    \n",
    "    df = classification(text_df, text_column, model, tokens)\n",
    "    label_list = list(df[\"classification\"])\n",
    "    labels = [entry['label'] for entry in label_list]\n",
    "    df[\"Label_Hugging\"] = labels\n",
    "    \n",
    "    cross_table = pd.crosstab(df['Target'], df['Label_Hugging'], margins=True)\n",
    "    \n",
    "    # calculate classification metrics using scikit-learn\n",
    "    accuracy = (cross_table.iloc[0, 0] + cross_table.iloc[1, 1]) / cross_table.loc['All', 'All'] if cross_table.shape == (3,3) else cross_table.iloc[1,0] / (cross_table.iloc[1, 0] + cross_table.iloc[0,0]) \n",
    "    precision = cross_table.iloc[1,1] / (cross_table.iloc[0,1] + cross_table.iloc[1,1]) if cross_table.shape == (3,3) else 0\n",
    "    recall = cross_table.iloc[1,1] / (cross_table.iloc[1,0] + cross_table.iloc[1,1]) if cross_table.shape == (3,3) != 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "\n",
    "\n",
    "    # print the metrics\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1 score:\", f1_score)\n",
    "    print(cross_table)\n",
    "    print(\"\\n\")\n",
    "    print(pd.crosstab(df['Final_Climate_Change_Level_Label'], df['Label_Hugging'], margins=True))\n",
    "    \n",
    "\n",
    "def get_metrics_df_hugging_face(text_df, text_column, model, tokens, model_name):\n",
    "    df = classification(text_df, text_column, model, tokens)\n",
    "    label_list = list(df[\"classification\"])\n",
    "    labels = [entry['label'] for entry in label_list]\n",
    "    df[\"Label_Hugging\"] = labels\n",
    "    \n",
    "    cross_table = pd.crosstab(df['Target'], df['Label_Hugging'], margins=True)\n",
    "    \n",
    "    # calculate classification metrics using scikit-learn\n",
    "    accuracy = (cross_table.iloc[0, 0] + cross_table.iloc[1, 1]) / cross_table.loc['All', 'All'] if cross_table.shape == (3,3) else cross_table.iloc[1,0] / (cross_table.iloc[1, 0] + cross_table.iloc[0,0]) \n",
    "    precision = cross_table.iloc[1,1] / (cross_table.iloc[0,1] + cross_table.iloc[1,1]) if cross_table.shape == (3,3) else 0\n",
    "    recall = cross_table.iloc[1,1] / (cross_table.iloc[1,0] + cross_table.iloc[1,1]) if cross_table.shape == (3,3) != 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "    \n",
    "    return(pd.DataFrame({\"Model\" : [model_name], \"Accuracy\" : [accuracy], \"Precision\" : [precision], \"Recall\" : [recall],\n",
    "                         \"F1 Score\" : [f1_score]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfa34b3",
   "metadata": {},
   "source": [
    "# 1. Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e2bf7db0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Final_Climate_Change_Level_Label</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>More than a dozen state attorneys general gath...</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sen. Jeff Merkley of Oregon endorsed Bernie S...</td>\n",
       "      <td>Small</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>When Carmen Luna moved to a neighborhood on t...</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>As ocean warming continues to trigger widespre...</td>\n",
       "      <td>High</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PG&amp;E Corp. told California regulators that it...</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>U.S. government bond prices swung Wednesday, u...</td>\n",
       "      <td>No Climate</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>Japan’s corporate governance reforms are start...</td>\n",
       "      <td>No Climate</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>While President Trump is out there wheezing hi...</td>\n",
       "      <td>No Climate</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>The South is home to three schools ranked four...</td>\n",
       "      <td>No Climate</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>One of the last jokes I wrote for President Ob...</td>\n",
       "      <td>No Climate</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Text  \\\n",
       "0    More than a dozen state attorneys general gath...   \n",
       "1     Sen. Jeff Merkley of Oregon endorsed Bernie S...   \n",
       "2     When Carmen Luna moved to a neighborhood on t...   \n",
       "3    As ocean warming continues to trigger widespre...   \n",
       "4     PG&E Corp. told California regulators that it...   \n",
       "..                                                 ...   \n",
       "295  U.S. government bond prices swung Wednesday, u...   \n",
       "296  Japan’s corporate governance reforms are start...   \n",
       "297  While President Trump is out there wheezing hi...   \n",
       "298  The South is home to three schools ranked four...   \n",
       "299  One of the last jokes I wrote for President Ob...   \n",
       "\n",
       "    Final_Climate_Change_Level_Label Target  \n",
       "0                             Medium    Yes  \n",
       "1                              Small     No  \n",
       "2                             Medium    Yes  \n",
       "3                               High    Yes  \n",
       "4                             Medium    Yes  \n",
       "..                               ...    ...  \n",
       "295                       No Climate     No  \n",
       "296                       No Climate     No  \n",
       "297                       No Climate     No  \n",
       "298                       No Climate     No  \n",
       "299                       No Climate     No  \n",
       "\n",
       "[300 rows x 3 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_climate_df = pd.read_parquet(\"Climate_Labels_Dataset.parquet\")\n",
    "tag_climate_df.head(5)\n",
    "#Only keep the required columns\n",
    "tag_climate_df = tag_climate_df[[\"Text\", \"Final_Climate_Change_Level_Label\"]]\n",
    "#Clean the tabel\n",
    "tag_climate_df['Final_Climate_Change_Level_Label'] = tag_climate_df['Final_Climate_Change_Level_Label'].str.strip()\n",
    "tag_climate_df.loc[tag_climate_df[\"Final_Climate_Change_Level_Label\"] == \"NA\", \"Final_Climate_Change_Level_Label\"] = \"Na\"\n",
    "tag_climate_df.loc[tag_climate_df[\"Final_Climate_Change_Level_Label\"] == \"0\", \"Final_Climate_Change_Level_Label\"] = \"Na\"\n",
    "tag_climate_df.loc[tag_climate_df[\"Final_Climate_Change_Level_Label\"] == \"Na\", \"Final_Climate_Change_Level_Label\"] = \"No Climate\"\n",
    "tag_climate_df[\"Target\"] = tag_climate_df[\"Final_Climate_Change_Level_Label\"].apply(lambda x: \"Yes\" if x in [\"High\", \"Medium\"] else \"No\")\n",
    "tag_climate_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "5418e92f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Final_Climate_Change_Level_Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>High</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Medium</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No Climate</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Small</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Final_Climate_Change_Level_Label  Text\n",
       "0                             High    65\n",
       "1                           Medium    35\n",
       "2                       No Climate   109\n",
       "3                            Small    91"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overview_labels_hms = tag_climate_df.groupby(\"Final_Climate_Change_Level_Label\")[\"Text\"].count().reset_index()\n",
    "overview_labels_hms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8b25c3c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yes</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Target  Text\n",
       "0     No   200\n",
       "1    Yes   100"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overview_labels = tag_climate_df.groupby(\"Target\")[\"Text\"].count().reset_index()\n",
    "overview_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d5bef2",
   "metadata": {},
   "source": [
    "## Splits in Train en Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e5931a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataframe into two sets\n",
    "df_train, df_test = train_test_split(tag_climate_df, test_size = 1/3, random_state = 23)\n",
    "df_train = df_train.reset_index(drop = True)\n",
    "df_test = df_test.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0765752f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Final_Climate_Change_Level_Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>High</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Medium</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No Climate</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Small</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Final_Climate_Change_Level_Label  Text\n",
       "0                             High    46\n",
       "1                           Medium    18\n",
       "2                       No Climate    75\n",
       "3                            Small    61"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.groupby(\"Final_Climate_Change_Level_Label\")[\"Text\"].count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d6a621a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yes</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Target  Text\n",
       "0     No   136\n",
       "1    Yes    64"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.groupby(\"Target\")[\"Text\"].count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "cd3722a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Final_Climate_Change_Level_Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>High</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Medium</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No Climate</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Small</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Final_Climate_Change_Level_Label  Text\n",
       "0                             High    19\n",
       "1                           Medium    17\n",
       "2                       No Climate    34\n",
       "3                            Small    30"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.groupby(\"Final_Climate_Change_Level_Label\")[\"Text\"].count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e3320d1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yes</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Target  Text\n",
       "0     No    64\n",
       "1    Yes    36"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.groupby(\"Target\")[\"Text\"].count().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ad56cb",
   "metadata": {},
   "source": [
    "# 2. Lexicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "54fc2163",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the lexicon\n",
    "UNDP_Lexicon = pd.read_csv(\"Lexicons/UNDP_Lexicon\")\n",
    "UNDP_Lexicon = UNDP_Lexicon.drop_duplicates().reset_index(drop = True)\n",
    "UNDP_Lexicon[\"Lexicon\"] = UNDP_Lexicon[\"Lexicon\"].str.lower()\n",
    "UNDP_Lexicon = UNDP_Lexicon[UNDP_Lexicon[\"Keep\"] == \"Yes\"]\n",
    "UNDP_Lexicon = pd.DataFrame(UNDP_Lexicon[\"Lexicon\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "dabba077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lexicon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>acceptability of policy or system change</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adaptability</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>adaptation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>adaptation behaviour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>adaptation limits</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>for climate change mitigation and adaptation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>sea level rise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>sea level fall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>social</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>unfccc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>343 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Lexicon\n",
       "0        acceptability of policy or system change\n",
       "1                                    adaptability\n",
       "2                                      adaptation\n",
       "3                            adaptation behaviour\n",
       "4                               adaptation limits\n",
       "..                                            ...\n",
       "383  for climate change mitigation and adaptation\n",
       "388                                sea level rise\n",
       "389                                sea level fall\n",
       "397                                        social\n",
       "404                                       unfccc \n",
       "\n",
       "[343 rows x 1 columns]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the lexicon\n",
    "IPCC_Lexicon = pd.read_csv(\"Lexicons/IPCC_Lexicon\")\n",
    "IPCC_Lexicon = IPCC_Lexicon.drop_duplicates().reset_index(drop = True)\n",
    "IPCC_Lexicon[\"Lexicon\"] = IPCC_Lexicon[\"Lexicon\"].str.lower()\n",
    "IPCC_Lexicon = IPCC_Lexicon[IPCC_Lexicon[\"Keep\"] == \"Yes\"]\n",
    "IPCC_Lexicon = pd.DataFrame(IPCC_Lexicon[\"Lexicon\"])\n",
    "IPCC_Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "4a69ff34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lexicon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abrupt climate change</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adaptation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>adaptive capacity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aerosols</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>afforestation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>wastewater</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>water vapor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>weather</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>100-year flood levels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>or earth system</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>155 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Lexicon\n",
       "0    abrupt climate change\n",
       "1               adaptation\n",
       "2        adaptive capacity\n",
       "3                 aerosols\n",
       "4            afforestation\n",
       "..                     ...\n",
       "150             wastewater\n",
       "151            water vapor\n",
       "152                weather\n",
       "153  100-year flood levels\n",
       "154        or earth system\n",
       "\n",
       "[155 rows x 1 columns]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the lexicon\n",
    "EPA_Lexicon = pd.read_csv(\"Lexicons/EPA_Lexicon\")\n",
    "EPA_Lexicon = EPA_Lexicon.drop_duplicates().reset_index(drop = True)\n",
    "EPA_Lexicon[\"Lexicon\"] = EPA_Lexicon[\"Lexicon\"].str.lower()\n",
    "EPA_Lexicon = EPA_Lexicon[EPA_Lexicon[\"Afkorting\"] == \"No\"]\n",
    "\n",
    "EPA_Lexicon = pd.DataFrame(EPA_Lexicon[\"Lexicon\"])\n",
    "EPA_Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "4e55e2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the lexicon\n",
    "Wikipedia_Lexicon = pd.read_csv(\"Lexicons/Wikipedia_Lexicon\")\n",
    "Wikipedia_Lexicon = Wikipedia_Lexicon.drop_duplicates().reset_index(drop = True)\n",
    "Wikipedia_Lexicon[\"Lexicon\"] = Wikipedia_Lexicon[\"Lexicon\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66c26da",
   "metadata": {},
   "outputs": [],
   "source": [
    "Wikipedi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "2627bec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the lexicon\n",
    "Global_Change_Lexicon = pd.read_csv(\"Lexicons/Global_Change_Lexicon\")\n",
    "Global_Change_Lexicon = Global_Change_Lexicon.drop_duplicates().reset_index(drop = True)\n",
    "Global_Change_Lexicon[\"Lexicon\"] = Global_Change_Lexicon[\"Lexicon\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "61bc5049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lexicon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adaptation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adaptation fund</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>annex i countries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>annex ii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>anthropogenic climate change</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>technology transfer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>tipping point</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>twenty-twenty-twenty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>350/450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>20-20-20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Lexicon\n",
       "0                     adaptation\n",
       "1                adaptation fund\n",
       "2              annex i countries\n",
       "3                       annex ii\n",
       "4   anthropogenic climate change\n",
       "..                           ...\n",
       "67           technology transfer\n",
       "68                 tipping point\n",
       "69          twenty-twenty-twenty\n",
       "77                       350/450\n",
       "78                      20-20-20\n",
       "\n",
       "[62 rows x 1 columns]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the lexicon\n",
    "BBC_Lexicon = pd.read_csv(\"Lexicons/BBC_Lexicon\")\n",
    "BBC_Lexicon = BBC_Lexicon.drop_duplicates().reset_index(drop = True)\n",
    "BBC_Lexicon[\"Lexicon\"] = BBC_Lexicon[\"Lexicon\"].str.lower()\n",
    "BBC_Lexicon = BBC_Lexicon[BBC_Lexicon[\"Afkorting\"] == \"No\"]\n",
    "\n",
    "BBC_Lexicon = pd.DataFrame(BBC_Lexicon[\"Lexicon\"])\n",
    "BBC_Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "62316160",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lexicon</th>\n",
       "      <th>Global Change</th>\n",
       "      <th>IPCC</th>\n",
       "      <th>Wikipedia</th>\n",
       "      <th>EPA</th>\n",
       "      <th>BBC</th>\n",
       "      <th>UNDP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Global Change</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.110787</td>\n",
       "      <td>0.073171</td>\n",
       "      <td>0.129032</td>\n",
       "      <td>0.177419</td>\n",
       "      <td>0.148936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IPCC</td>\n",
       "      <td>0.361905</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.170732</td>\n",
       "      <td>0.258065</td>\n",
       "      <td>0.306452</td>\n",
       "      <td>0.382979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.114286</td>\n",
       "      <td>0.081633</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.212903</td>\n",
       "      <td>0.241935</td>\n",
       "      <td>0.212766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EPA</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.116618</td>\n",
       "      <td>0.201220</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.225806</td>\n",
       "      <td>0.276596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BBC</td>\n",
       "      <td>0.104762</td>\n",
       "      <td>0.055394</td>\n",
       "      <td>0.091463</td>\n",
       "      <td>0.090323</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.212766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>UNDP</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.052478</td>\n",
       "      <td>0.060976</td>\n",
       "      <td>0.083871</td>\n",
       "      <td>0.161290</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Lexicon  Global Change      IPCC  Wikipedia       EPA       BBC  \\\n",
       "0  Global Change       1.000000  0.110787   0.073171  0.129032  0.177419   \n",
       "1           IPCC       0.361905  1.000000   0.170732  0.258065  0.306452   \n",
       "2      Wikipedia       0.114286  0.081633   1.000000  0.212903  0.241935   \n",
       "3            EPA       0.190476  0.116618   0.201220  1.000000  0.225806   \n",
       "4            BBC       0.104762  0.055394   0.091463  0.090323  1.000000   \n",
       "5           UNDP       0.066667  0.052478   0.060976  0.083871  0.161290   \n",
       "\n",
       "       UNDP  \n",
       "0  0.148936  \n",
       "1  0.382979  \n",
       "2  0.212766  \n",
       "3  0.276596  \n",
       "4  0.212766  \n",
       "5  1.000000  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create an empty dataframe and write a function to fill with the values\n",
    "\n",
    "common_words_df = pd.DataFrame({\"Lexicon\" : [\"Global Change\", \"IPCC\", \"Wikipedia\", \"EPA\", \"BBC\", \"UNDP\"], \n",
    "                               \"Global Change\": [0, 0, 0, 0, 0, 0], \"IPCC\" : [0, 0, 0, 0, 0, 0], \n",
    "                               \"Wikipedia\" : [0, 0, 0, 0, 0, 0], \"EPA\" : [0, 0, 0, 0, 0, 0], \n",
    "                               \"BBC\" : [0, 0, 0, 0, 0, 0], \"UNDP\" : [0, 0, 0, 0, 0, 0]})\n",
    "\n",
    "dfs = [Global_Change_Lexicon, IPCC_Lexicon, Wikipedia_Lexicon, EPA_Lexicon, BBC_Lexicon, UNDP_Lexicon]\n",
    "\n",
    "for r in range(0, len(dfs)):\n",
    "    for c in range(0, len(dfs)):\n",
    "        # Get the common values between the two columns\n",
    "        common_words = set(dfs[r]['Lexicon']).intersection(set(dfs[c]['Lexicon']))\n",
    "        common_words_df.loc[r, common_words_df.columns[c +1]] = len(common_words)/len(dfs[c]['Lexicon'])\n",
    "\n",
    "common_words_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "1b01b5d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lexicon</th>\n",
       "      <th>Global Change</th>\n",
       "      <th>IPCC</th>\n",
       "      <th>Wikipedia</th>\n",
       "      <th>EPA</th>\n",
       "      <th>BBC</th>\n",
       "      <th>UNDP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Global Change</td>\n",
       "      <td>105</td>\n",
       "      <td>38</td>\n",
       "      <td>12</td>\n",
       "      <td>20</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IPCC</td>\n",
       "      <td>38</td>\n",
       "      <td>343</td>\n",
       "      <td>28</td>\n",
       "      <td>40</td>\n",
       "      <td>19</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>12</td>\n",
       "      <td>28</td>\n",
       "      <td>164</td>\n",
       "      <td>33</td>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EPA</td>\n",
       "      <td>20</td>\n",
       "      <td>40</td>\n",
       "      <td>33</td>\n",
       "      <td>155</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BBC</td>\n",
       "      <td>11</td>\n",
       "      <td>19</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>62</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>UNDP</td>\n",
       "      <td>7</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Lexicon  Global Change  IPCC  Wikipedia  EPA  BBC  UNDP\n",
       "0  Global Change            105    38         12   20   11     7\n",
       "1           IPCC             38   343         28   40   19    18\n",
       "2      Wikipedia             12    28        164   33   15    10\n",
       "3            EPA             20    40         33  155   14    13\n",
       "4            BBC             11    19         15   14   62    10\n",
       "5           UNDP              7    18         10   13   10    47"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create an empty dataframe and write a function to fill with the values\n",
    "\n",
    "common_words_df = pd.DataFrame({\"Lexicon\" : [\"Global Change\", \"IPCC\", \"Wikipedia\", \"EPA\", \"BBC\", \"UNDP\"], \n",
    "                               \"Global Change\": [0, 0, 0, 0, 0, 0], \"IPCC\" : [0, 0, 0, 0, 0, 0], \n",
    "                               \"Wikipedia\" : [0, 0, 0, 0, 0, 0], \"EPA\" : [0, 0, 0, 0, 0, 0], \n",
    "                               \"BBC\" : [0, 0, 0, 0, 0, 0], \"UNDP\" : [0, 0, 0, 0, 0, 0]})\n",
    "\n",
    "dfs = [Global_Change_Lexicon, IPCC_Lexicon, Wikipedia_Lexicon, EPA_Lexicon, BBC_Lexicon, UNDP_Lexicon]\n",
    "\n",
    "for r in range(0, len(dfs)):\n",
    "    for c in range(0, len(dfs)):\n",
    "        # Get the common values between the two columns\n",
    "        common_words = set(dfs[r]['Lexicon']).intersection(set(dfs[c]['Lexicon']))\n",
    "        common_words_df.loc[r, common_words_df.columns[c +1]] = len(common_words)\n",
    "\n",
    "common_words_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "1106d880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lexicon</th>\n",
       "      <th>Non unique words</th>\n",
       "      <th>unique words</th>\n",
       "      <th>total_words</th>\n",
       "      <th>Richness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Global Change</td>\n",
       "      <td>45</td>\n",
       "      <td>60</td>\n",
       "      <td>105</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IPCC</td>\n",
       "      <td>80</td>\n",
       "      <td>263</td>\n",
       "      <td>343</td>\n",
       "      <td>0.766764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>47</td>\n",
       "      <td>117</td>\n",
       "      <td>164</td>\n",
       "      <td>0.713415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EPA</td>\n",
       "      <td>59</td>\n",
       "      <td>96</td>\n",
       "      <td>155</td>\n",
       "      <td>0.619355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BBC</td>\n",
       "      <td>30</td>\n",
       "      <td>32</td>\n",
       "      <td>62</td>\n",
       "      <td>0.516129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>UNDP</td>\n",
       "      <td>25</td>\n",
       "      <td>22</td>\n",
       "      <td>47</td>\n",
       "      <td>0.468085</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Lexicon  Non unique words  unique words  total_words  Richness\n",
       "0  Global Change                45            60          105  0.571429\n",
       "1           IPCC                80           263          343  0.766764\n",
       "2      Wikipedia                47           117          164  0.713415\n",
       "3            EPA                59            96          155  0.619355\n",
       "4            BBC                30            32           62  0.516129\n",
       "5           UNDP                25            22           47  0.468085"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs = [Global_Change_Lexicon, IPCC_Lexicon, Wikipedia_Lexicon, EPA_Lexicon, BBC_Lexicon, UNDP_Lexicon]\n",
    "non_unique_words = []\n",
    "unique_words = []\n",
    "total_words = []\n",
    "for r in range(len(dfs)):\n",
    "    common_words = []\n",
    "    for c in range(len(dfs)):\n",
    "        if c != r:\n",
    "            # Get the common values between the two columns\n",
    "            common_words.extend(list(set(dfs[r]['Lexicon']).intersection(set(dfs[c]['Lexicon']))))\n",
    "    common_words = list(set(common_words))  # Remove duplicates by converting to a set and back to a list\n",
    "    total_words.append(len(dfs[r][\"Lexicon\"]))\n",
    "    unique_words.append(len(dfs[r][\"Lexicon\"]) - len(common_words))\n",
    "    non_unique_words.append(len(common_words))\n",
    "\n",
    "non_unique_words\n",
    "\n",
    "unique_words_df = pd.DataFrame({\"Lexicon\" : [\"Global Change\", \"IPCC\", \"Wikipedia\", \"EPA\", \"BBC\", \"UNDP\"], \n",
    "                                \"Non unique words\" : non_unique_words, \"unique words\" : unique_words, \n",
    "                               \"total_words\" : total_words})\n",
    "\n",
    "unique_words_df[\"Richness\"] = unique_words_df[\"unique words\"] / unique_words_df[\"total_words\"]\n",
    "\n",
    "unique_words_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "8d2cf85c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Lexicon_names = [\"Global Change\", \"IPCC\", \"Wikipedia\", \"EPA\", \"BBC\", \"UNDP\"]\n",
    "Lexicons = [Global_Change_Lexicon, IPCC_Lexicon, Wikipedia_Lexicon, EPA_Lexicon, BBC_Lexicon, UNDP_Lexicon]\n",
    "Lexicon_name = []\n",
    "Topic = []\n",
    "for i in range(len(Lexicon_names)):\n",
    "\n",
    "    #Get the topic of each Lexicon\n",
    "    # Concatenate all the terms in the Lexicon column\n",
    "    text = ' '.join(Lexicons[i]['Lexicon'].values)\n",
    "    Lexicon_name.extend([Lexicon_names[i]] * 5)\n",
    "    # Create a new dataframe with a single row containing the concatenated text\n",
    "    df = pd.DataFrame({'Text': [text]})\n",
    "\n",
    "    # Create the document-term matrix\n",
    "    vectorizer = CountVectorizer()\n",
    "    dtm = vectorizer.fit_transform(df['Text'])\n",
    "\n",
    "    # Apply Latent Dirichlet Allocation (LDA)\n",
    "    lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "    lda.fit(dtm)\n",
    "\n",
    "    # Extract the topics and associated words\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    num_top_words = 10  # Number of words per topic to display\n",
    "\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        #print(f\"Topic #{topic_idx + 1}:\")\n",
    "        Topic.append(\" \".join([feature_names[i] for i in topic.argsort()[:-num_top_words - 1:-1]]))\n",
    "\n",
    "Lexicon_Topics_df = pd.DataFrame({\"Lexicon\" : Lexicon_name, \"Topic\" : Topic})\n",
    "Lexicon_Topics_df = Lexicon_Topics_df.drop_duplicates().reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "41689d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "Lexicon_Topics_df\n",
    "pd.reset_option('display.max_colwidth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fb7b13",
   "metadata": {},
   "source": [
    "# 3. Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6340f21",
   "metadata": {},
   "source": [
    "## 3.1. Lexicons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518f9459",
   "metadata": {},
   "source": [
    "### 3.1.1. Train Lexicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e42af9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Text_df = df_train.copy()\n",
    "Text_df[\"Text\"] = Text_df[\"Text\"].apply(preprocess_text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3ebadcc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.4037930965423584 seconds ---\n",
      "--- 1368.5836322307587 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "names = [\"IPCC\", \"Global_Change\", \"UNDP\", \"EPA\", \"Wikipedia\", \"BBC\"]\n",
    "All_names = []\n",
    "# Generate all combinations\n",
    "all_combinations = []\n",
    "for r in range(1, len(names) + 1):\n",
    "    combinations_r = combinations(names, r)\n",
    "    all_combinations.extend(combinations_r)\n",
    "\n",
    "# Print all combinations\n",
    "for combination in all_combinations:\n",
    "    combined_string = \"_\".join(combination)\n",
    "    All_names.append(combined_string)\n",
    "    \n",
    "Lexicons = [IPCC_Lexicon, Global_Change_Lexicon, UNDP_Lexicon, EPA_Lexicon, Wikipedia_Lexicon, BBC_Lexicon]\n",
    "All_Lexicons = []\n",
    "# Generate all combinations\n",
    "all_combinations = []\n",
    "for r in range(1, len(Lexicons) + 1):\n",
    "    combinations_r = combinations(Lexicons, r)\n",
    "    all_combinations.extend(combinations_r)\n",
    "\n",
    "# Concatenate and print all combinations\n",
    "for combination in all_combinations:\n",
    "    combined_df = pd.concat(combination, axis=0).drop_duplicates().reset_index()\n",
    "    All_Lexicons.append(combined_df)\n",
    "    \n",
    "results_df = pd.DataFrame()\n",
    "\n",
    "start_results = time.time()\n",
    "\n",
    "print(\"--- %s seconds ---\" % (start_results - start))\n",
    "\n",
    "for i in range(len(All_Lexicons)):\n",
    "    r_df = find_optimal_threshold(Text_df, All_Lexicons[i], All_names[i])\n",
    "    results_df = pd.concat([results_df, r_df])\n",
    "\n",
    "results_df = results_df.reset_index(drop = True)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "82c26fee",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lexicon</th>\n",
       "      <th>Technique</th>\n",
       "      <th>Threshold</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>Wikipedia_BBC</td>\n",
       "      <td>Relative Frequency</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.910</td>\n",
       "      <td>0.838235</td>\n",
       "      <td>0.890625</td>\n",
       "      <td>0.863636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>Global_Change_UNDP_Wikipedia</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.905</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>0.840336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>Global_Change_UNDP_Wikipedia_BBC</td>\n",
       "      <td>Relative Frequency</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.905</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.863309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>Global_Change_UNDP_Wikipedia_BBC</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.905</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>0.840336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>Global_Change_UNDP_EPA</td>\n",
       "      <td>Relative Frequency</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.782051</td>\n",
       "      <td>0.953125</td>\n",
       "      <td>0.859155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>UNDP_Wikipedia_BBC</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.474074</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.643216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>IPCC_Global_Change_UNDP_EPA_Wikipedia_BBC</td>\n",
       "      <td>Absolute Present</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.610</td>\n",
       "      <td>0.449275</td>\n",
       "      <td>0.968750</td>\n",
       "      <td>0.613861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>IPCC_Global_Change_UNDP_EPA</td>\n",
       "      <td>Absolute Present</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.610</td>\n",
       "      <td>0.449275</td>\n",
       "      <td>0.968750</td>\n",
       "      <td>0.613861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>IPCC_Global_Change_UNDP_EPA_Wikipedia</td>\n",
       "      <td>Absolute Present</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.610</td>\n",
       "      <td>0.449275</td>\n",
       "      <td>0.968750</td>\n",
       "      <td>0.613861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>IPCC_Global_Change_UNDP_EPA_BBC</td>\n",
       "      <td>Absolute Present</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.610</td>\n",
       "      <td>0.449275</td>\n",
       "      <td>0.968750</td>\n",
       "      <td>0.613861</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>252 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Lexicon           Technique  Threshold  \\\n",
       "82                               Wikipedia_BBC  Relative Frequency        0.6   \n",
       "128               Global_Change_UNDP_Wikipedia  Absolute Frequency       11.0   \n",
       "214           Global_Change_UNDP_Wikipedia_BBC  Relative Frequency        1.1   \n",
       "212           Global_Change_UNDP_Wikipedia_BBC  Absolute Frequency       11.0   \n",
       "126                     Global_Change_UNDP_EPA  Relative Frequency        1.1   \n",
       "..                                         ...                 ...        ...   \n",
       "156                         UNDP_Wikipedia_BBC  Absolute Frequency        3.0   \n",
       "249  IPCC_Global_Change_UNDP_EPA_Wikipedia_BBC    Absolute Present        5.0   \n",
       "165                IPCC_Global_Change_UNDP_EPA    Absolute Present        5.0   \n",
       "225      IPCC_Global_Change_UNDP_EPA_Wikipedia    Absolute Present        5.0   \n",
       "229            IPCC_Global_Change_UNDP_EPA_BBC    Absolute Present        5.0   \n",
       "\n",
       "     Accuracy  Precision    Recall  F1 Score  \n",
       "82      0.910   0.838235  0.890625  0.863636  \n",
       "128     0.905   0.909091  0.781250  0.840336  \n",
       "214     0.905   0.800000  0.937500  0.863309  \n",
       "212     0.905   0.909091  0.781250  0.840336  \n",
       "126     0.900   0.782051  0.953125  0.859155  \n",
       "..        ...        ...       ...       ...  \n",
       "156     0.645   0.474074  1.000000  0.643216  \n",
       "249     0.610   0.449275  0.968750  0.613861  \n",
       "165     0.610   0.449275  0.968750  0.613861  \n",
       "225     0.610   0.449275  0.968750  0.613861  \n",
       "229     0.610   0.449275  0.968750  0.613861  \n",
       "\n",
       "[252 rows x 7 columns]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.sort_values(by = \"Accuracy\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "4e93d6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_parquet(\"Lexicon_Tagging_Train_Results.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68434da",
   "metadata": {},
   "source": [
    "### 3.1.2. Test Lexicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "0f74ff59",
   "metadata": {},
   "outputs": [],
   "source": [
    "Text_df_test = df_test.copy()\n",
    "Text_df_test[\"Text\"] = Text_df_test[\"Text\"].apply(preprocess_text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "49cd1c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.9570615291595459 seconds ---\n",
      "--- 599.5314538478851 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "names = [\"IPCC\", \"Global_Change\", \"UNDP\", \"EPA\", \"Wikipedia\", \"BBC\"]\n",
    "All_names = []\n",
    "# Generate all combinations\n",
    "all_combinations = []\n",
    "for r in range(1, len(names) + 1):\n",
    "    combinations_r = combinations(names, r)\n",
    "    all_combinations.extend(combinations_r)\n",
    "\n",
    "# Print all combinations\n",
    "for combination in all_combinations:\n",
    "    combined_string = \"_\".join(combination)\n",
    "    All_names.append(combined_string)\n",
    "    \n",
    "Lexicons = [IPCC_Lexicon, Global_Change_Lexicon, UNDP_Lexicon, EPA_Lexicon, Wikipedia_Lexicon, BBC_Lexicon]\n",
    "All_Lexicons = []\n",
    "# Generate all combinations\n",
    "all_combinations = []\n",
    "for r in range(1, len(Lexicons) + 1):\n",
    "    combinations_r = combinations(Lexicons, r)\n",
    "    all_combinations.extend(combinations_r)\n",
    "\n",
    "# Concatenate and print all combinations\n",
    "for combination in all_combinations:\n",
    "    combined_df = pd.concat(combination, axis=0).drop_duplicates().reset_index()\n",
    "    All_Lexicons.append(combined_df)\n",
    "    \n",
    "test_result_df = pd.DataFrame()\n",
    "\n",
    "start_results = time.time()\n",
    "\n",
    "print(\"--- %s seconds ---\" % (start_results - start))\n",
    "\n",
    "for n in range(0, len(All_names)):\n",
    "    test_lexicon_result = test_lexicon(Text_df_test, results_df[results_df[\"Lexicon\"] == All_names[n]], All_Lexicons[All_names.index(All_names[n])], All_names[n])\n",
    "    test_result_df = pd.concat([test_result_df, test_lexicon_result])\n",
    "    \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "38fdeef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lexicon</th>\n",
       "      <th>Technique</th>\n",
       "      <th>Threshold</th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Test Precision</th>\n",
       "      <th>Test Recall</th>\n",
       "      <th>Test F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UNDP_EPA_Wikipedia</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.903226</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.835821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EPA</td>\n",
       "      <td>Relative Frequency</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.845070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UNDP_EPA</td>\n",
       "      <td>Relative Frequency</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.804878</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UNDP_EPA_BBC</td>\n",
       "      <td>Relative Frequency</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.804878</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UNDP_EPA_Wikipedia</td>\n",
       "      <td>Relative Frequency</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.804878</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UNDP_Wikipedia_BBC</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.514286</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.679245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IPCC_Global_Change_UNDP_EPA_BBC</td>\n",
       "      <td>Absolute Present</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.660377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IPCC_Global_Change_UNDP_EPA_Wikipedia</td>\n",
       "      <td>Absolute Present</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.660377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IPCC_Global_Change_UNDP_EPA_Wikipedia_BBC</td>\n",
       "      <td>Absolute Present</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.660377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IPCC_Global_Change_UNDP_EPA</td>\n",
       "      <td>Absolute Present</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.660377</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>252 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Lexicon           Technique  Threshold  \\\n",
       "0                          UNDP_EPA_Wikipedia  Absolute Frequency        9.0   \n",
       "2                                         EPA  Relative Frequency        0.6   \n",
       "2                                    UNDP_EPA  Relative Frequency        0.9   \n",
       "2                                UNDP_EPA_BBC  Relative Frequency        0.9   \n",
       "2                          UNDP_EPA_Wikipedia  Relative Frequency        0.9   \n",
       "..                                        ...                 ...        ...   \n",
       "0                          UNDP_Wikipedia_BBC  Absolute Frequency        3.0   \n",
       "1             IPCC_Global_Change_UNDP_EPA_BBC    Absolute Present        5.0   \n",
       "1       IPCC_Global_Change_UNDP_EPA_Wikipedia    Absolute Present        5.0   \n",
       "1   IPCC_Global_Change_UNDP_EPA_Wikipedia_BBC    Absolute Present        5.0   \n",
       "1                 IPCC_Global_Change_UNDP_EPA    Absolute Present        5.0   \n",
       "\n",
       "    Test Accuracy  Test Precision  Test Recall  Test F1 Score  \n",
       "0            0.89        0.903226     0.777778       0.835821  \n",
       "2            0.89        0.857143     0.833333       0.845070  \n",
       "2            0.89        0.804878     0.916667       0.857143  \n",
       "2            0.89        0.804878     0.916667       0.857143  \n",
       "2            0.89        0.804878     0.916667       0.857143  \n",
       "..            ...             ...          ...            ...  \n",
       "0            0.66        0.514286     1.000000       0.679245  \n",
       "1            0.64        0.500000     0.972222       0.660377  \n",
       "1            0.64        0.500000     0.972222       0.660377  \n",
       "1            0.64        0.500000     0.972222       0.660377  \n",
       "1            0.64        0.500000     0.972222       0.660377  \n",
       "\n",
       "[252 rows x 7 columns]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_result_df.sort_values(by = \"Test Accuracy\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c86b1827",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result_df.to_parquet(\"Lexicon_Tagging_Test_Results.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9ec1e5",
   "metadata": {},
   "source": [
    "## 3.2. Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "84af67e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_test_df = df_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "de7964bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert1 = get_metrics_df_hugging_face(bert_test_df, 'Text',\"climatebert/environmental-claims\",512, \"climatebert/environmental-claims\")\n",
    "bert2 = get_metrics_df_hugging_face(bert_test_df, \"Text\", \"climatebert/distilroberta-base-climate-detector\", 512, \"climatebert/distilroberta-base-climate-detector\")\n",
    "bert_df = pd.concat([bert1, bert2]).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "64a8f4dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>climatebert/environmental-claims</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>climatebert/distilroberta-base-climate-detector</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.758621</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Model  Accuracy  Precision  \\\n",
       "0                 climatebert/environmental-claims      0.36   0.000000   \n",
       "1  climatebert/distilroberta-base-climate-detector      0.79   0.647059   \n",
       "\n",
       "     Recall  F1 Score  \n",
       "0  0.000000  0.000000  \n",
       "1  0.916667  0.758621  "
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "93bf9786",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_df.to_parquet(\"Bert_Tagging_Test_Results.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cedaec8",
   "metadata": {},
   "source": [
    "# 4. Comparing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "61d2e135",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lexicon_train_result_df = pd.read_parquet(\"Lexicon_Tagging_Train_Results.parquet\")\n",
    "Lexicon_test_result_df = pd.read_parquet(\"Lexicon_Tagging_Test_Results.parquet\")\n",
    "Bert_test_result_df = pd.read_parquet(\"Bert_Tagging_Test_Results.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "e17a2237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lexicon</th>\n",
       "      <th>Technique</th>\n",
       "      <th>Threshold</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wikipedia_BBC</td>\n",
       "      <td>Relative Frequency</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.910</td>\n",
       "      <td>0.838235</td>\n",
       "      <td>0.890625</td>\n",
       "      <td>0.863636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Global_Change_UNDP_Wikipedia</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.905</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>0.840336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Global_Change_UNDP_Wikipedia_BBC</td>\n",
       "      <td>Relative Frequency</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.905</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.863309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Global_Change_UNDP_Wikipedia_BBC</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.905</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>0.840336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Global_Change_UNDP_EPA</td>\n",
       "      <td>Relative Frequency</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.782051</td>\n",
       "      <td>0.953125</td>\n",
       "      <td>0.859155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Global_Change_UNDP_EPA_BBC</td>\n",
       "      <td>Relative Frequency</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.782051</td>\n",
       "      <td>0.953125</td>\n",
       "      <td>0.859155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Global_Change_UNDP_EPA_BBC</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.879310</td>\n",
       "      <td>0.796875</td>\n",
       "      <td>0.836066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Global_Change_UNDP_EPA_Wikipedia</td>\n",
       "      <td>Relative Frequency</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.782051</td>\n",
       "      <td>0.953125</td>\n",
       "      <td>0.859155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Global_Change_UNDP_Wikipedia</td>\n",
       "      <td>Relative Frequency</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.814286</td>\n",
       "      <td>0.890625</td>\n",
       "      <td>0.850746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>UNDP_Wikipedia_BBC</td>\n",
       "      <td>Relative Frequency</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.782051</td>\n",
       "      <td>0.953125</td>\n",
       "      <td>0.859155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Lexicon           Technique  Threshold  Accuracy  \\\n",
       "0                     Wikipedia_BBC  Relative Frequency        0.6     0.910   \n",
       "1      Global_Change_UNDP_Wikipedia  Absolute Frequency       11.0     0.905   \n",
       "2  Global_Change_UNDP_Wikipedia_BBC  Relative Frequency        1.1     0.905   \n",
       "3  Global_Change_UNDP_Wikipedia_BBC  Absolute Frequency       11.0     0.905   \n",
       "4            Global_Change_UNDP_EPA  Relative Frequency        1.1     0.900   \n",
       "5        Global_Change_UNDP_EPA_BBC  Relative Frequency        1.1     0.900   \n",
       "6        Global_Change_UNDP_EPA_BBC  Absolute Frequency       11.0     0.900   \n",
       "7  Global_Change_UNDP_EPA_Wikipedia  Relative Frequency        1.1     0.900   \n",
       "8      Global_Change_UNDP_Wikipedia  Relative Frequency        1.2     0.900   \n",
       "9                UNDP_Wikipedia_BBC  Relative Frequency        0.9     0.900   \n",
       "\n",
       "   Precision    Recall  F1 Score  \n",
       "0   0.838235  0.890625  0.863636  \n",
       "1   0.909091  0.781250  0.840336  \n",
       "2   0.800000  0.937500  0.863309  \n",
       "3   0.909091  0.781250  0.840336  \n",
       "4   0.782051  0.953125  0.859155  \n",
       "5   0.782051  0.953125  0.859155  \n",
       "6   0.879310  0.796875  0.836066  \n",
       "7   0.782051  0.953125  0.859155  \n",
       "8   0.814286  0.890625  0.850746  \n",
       "9   0.782051  0.953125  0.859155  "
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Lexicon_train_result_df.sort_values(by = \"Accuracy\", ascending = False).head(10).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "31f13431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lexicon</th>\n",
       "      <th>Technique</th>\n",
       "      <th>Threshold</th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Test F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UNDP_EPA</td>\n",
       "      <td>Relative Frequency</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UNDP_EPA_Wikipedia</td>\n",
       "      <td>Relative Frequency</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UNDP_EPA_BBC</td>\n",
       "      <td>Relative Frequency</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UNDP_EPA_Wikipedia_BBC</td>\n",
       "      <td>Relative Frequency</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Global_Change_EPA</td>\n",
       "      <td>Relative Frequency</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.853333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>EPA</td>\n",
       "      <td>Relative Frequency</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.845070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>EPA_BBC</td>\n",
       "      <td>Relative Frequency</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.845070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>UNDP_EPA_Wikipedia</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.835821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>IPCC_UNDP_Wikipedia_BBC</td>\n",
       "      <td>Relative Frequency</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.846154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Global_Change_EPA_Wikipedia</td>\n",
       "      <td>Relative Frequency</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.842105</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Lexicon           Technique  Threshold  Test Accuracy  \\\n",
       "0                     UNDP_EPA  Relative Frequency        0.9           0.89   \n",
       "1           UNDP_EPA_Wikipedia  Relative Frequency        0.9           0.89   \n",
       "2                 UNDP_EPA_BBC  Relative Frequency        0.9           0.89   \n",
       "3       UNDP_EPA_Wikipedia_BBC  Relative Frequency        0.9           0.89   \n",
       "4            Global_Change_EPA  Relative Frequency        0.8           0.89   \n",
       "5                          EPA  Relative Frequency        0.6           0.89   \n",
       "6                      EPA_BBC  Relative Frequency        0.6           0.89   \n",
       "7           UNDP_EPA_Wikipedia  Absolute Frequency        9.0           0.89   \n",
       "8      IPCC_UNDP_Wikipedia_BBC  Relative Frequency        1.6           0.88   \n",
       "9  Global_Change_EPA_Wikipedia  Relative Frequency        0.8           0.88   \n",
       "\n",
       "   Test F1 Score  \n",
       "0       0.857143  \n",
       "1       0.857143  \n",
       "2       0.857143  \n",
       "3       0.857143  \n",
       "4       0.853333  \n",
       "5       0.845070  \n",
       "6       0.845070  \n",
       "7       0.835821  \n",
       "8       0.846154  \n",
       "9       0.842105  "
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Lexicon_test_result_df.sort_values(by = [\"Test Accuracy\", \"Test F1 Score\"], ascending = False).head(10).reset_index(drop = True)[[\"Lexicon\", \"Technique\", \"Threshold\", \"Test Accuracy\", \"Test F1 Score\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "cf50a66f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lexicon</th>\n",
       "      <th>Technique</th>\n",
       "      <th>Threshold</th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Test Precision</th>\n",
       "      <th>Test Recall</th>\n",
       "      <th>Test F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IPCC</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.736842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IPCC</td>\n",
       "      <td>Absolute Present</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.682927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IPCC</td>\n",
       "      <td>Relative Frequency</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.695652</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.780488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IPCC</td>\n",
       "      <td>Relative Present</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.628571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Global_Change</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.509091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Global_Change_UNDP_EPA_Wikipedia_BBC</td>\n",
       "      <td>Relative Present</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.660377</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.786517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IPCC_Global_Change_UNDP_EPA_Wikipedia_BBC</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.638298</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.722892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IPCC_Global_Change_UNDP_EPA_Wikipedia_BBC</td>\n",
       "      <td>Absolute Present</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.660377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IPCC_Global_Change_UNDP_EPA_Wikipedia_BBC</td>\n",
       "      <td>Relative Frequency</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.693878</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IPCC_Global_Change_UNDP_EPA_Wikipedia_BBC</td>\n",
       "      <td>Relative Present</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.685714</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.676056</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>252 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Lexicon           Technique  Threshold  \\\n",
       "0                                        IPCC  Absolute Frequency       10.0   \n",
       "1                                        IPCC    Absolute Present        6.0   \n",
       "2                                        IPCC  Relative Frequency        1.1   \n",
       "3                                        IPCC    Relative Present        0.8   \n",
       "0                               Global_Change  Absolute Frequency        5.0   \n",
       "..                                        ...                 ...        ...   \n",
       "3        Global_Change_UNDP_EPA_Wikipedia_BBC    Relative Present        0.6   \n",
       "0   IPCC_Global_Change_UNDP_EPA_Wikipedia_BBC  Absolute Frequency       12.0   \n",
       "1   IPCC_Global_Change_UNDP_EPA_Wikipedia_BBC    Absolute Present        5.0   \n",
       "2   IPCC_Global_Change_UNDP_EPA_Wikipedia_BBC  Relative Frequency        1.5   \n",
       "3   IPCC_Global_Change_UNDP_EPA_Wikipedia_BBC    Relative Present        1.3   \n",
       "\n",
       "    Test Accuracy  Test Precision  Test Recall  Test F1 Score  \n",
       "0            0.80        0.700000     0.777778       0.736842  \n",
       "1            0.74        0.608696     0.777778       0.682927  \n",
       "2            0.82        0.695652     0.888889       0.780488  \n",
       "3            0.74        0.647059     0.611111       0.628571  \n",
       "0            0.73        0.736842     0.388889       0.509091  \n",
       "..            ...             ...          ...            ...  \n",
       "3            0.81        0.660377     0.972222       0.786517  \n",
       "0            0.77        0.638298     0.833333       0.722892  \n",
       "1            0.64        0.500000     0.972222       0.660377  \n",
       "2            0.83        0.693878     0.944444       0.800000  \n",
       "3            0.77        0.685714     0.666667       0.676056  \n",
       "\n",
       "[252 rows x 7 columns]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Lexicon_test_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "f9e27fa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Lexicon_test_result_df[\"Lexicon\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "bfb5ff5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>climatebert/environmental-claims</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>climatebert/distilroberta-base-climate-detector</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.758621</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Model  Accuracy  Precision  \\\n",
       "0                 climatebert/environmental-claims      0.36   0.000000   \n",
       "1  climatebert/distilroberta-base-climate-detector      0.79   0.647059   \n",
       "\n",
       "     Recall  F1 Score  \n",
       "0  0.000000  0.000000  \n",
       "1  0.916667  0.758621  "
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Bert_test_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "4e521996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Estimate</th>\n",
       "      <th>No</th>\n",
       "      <th>Yes</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Final_Climate_Change_Level_Label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>High</th>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Medium</th>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No Climate</th>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Small</th>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>65</td>\n",
       "      <td>35</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Estimate                          No  Yes  All\n",
       "Final_Climate_Change_Level_Label              \n",
       "High                               0   19   19\n",
       "Medium                             6   11   17\n",
       "No Climate                        34    0   34\n",
       "Small                             25    5   30\n",
       "All                               65   35  100"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Lexicon_cross = pd.concat([EPA_Lexicon]).drop_duplicates()\n",
    "get_cross_table(Text_df_test, Lexicon_cross, 0.6, \"Relative Frequency\", False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602d8cad",
   "metadata": {},
   "source": [
    "# 5. Classify Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "461db6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "WP = pd.read_parquet(\"C:/Users/Boedt/OneDrive/Bureaublad/Scraped_Articles/Final/WP_Final_Articles.parquet\")\n",
    "WP_clean = WP.copy()\n",
    "WP_clean[\"Text\"] = WP_clean[\"Text\"].apply(preprocess_text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a2d5dd51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "      <th>Date</th>\n",
       "      <th>News Paper</th>\n",
       "      <th>Link</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Full text: Clinton testifies before House comm...</td>\n",
       "      <td>2015 clintons benghazi hearing in three minute...</td>\n",
       "      <td>2016-09-23</td>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>https://www.washingtonpost.com/news/post-polit...</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Full transcript: FBI Director James Comey test...</td>\n",
       "      <td>comey rogers testify on alleged russian interf...</td>\n",
       "      <td>2017-03-21</td>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>https://www.washingtonpost.com/news/post-polit...</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Transcript of Zuckerberg’s appearance before H...</td>\n",
       "      <td>top takeaways from mark zuckerberg’s hearings ...</td>\n",
       "      <td>2018-04-11</td>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>https://www.washingtonpost.com/news/the-switch...</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Transcript of Mark Zuckerberg’s Senate hearing</td>\n",
       "      <td>facebook ceo mark zuckerberg sat down before l...</td>\n",
       "      <td>2018-04-10</td>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>https://www.washingtonpost.com/news/the-switch...</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The October Democratic debate transcript</td>\n",
       "      <td>sen kamala harris dcalif sen bernie sanders iv...</td>\n",
       "      <td>2019-10-16</td>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>https://www.washingtonpost.com/politics/2019/1...</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143891</th>\n",
       "      <td>By Martin Weil\\nJuly 25, 2016\\nA man was shot ...</td>\n",
       "      <td>a man was shot and wounded late monday on nort...</td>\n",
       "      <td>2016-07-26</td>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>https://www.washingtonpost.com/local/public-sa...</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143892</th>\n",
       "      <td>Short Circuit: A roundup of recent federal cou...</td>\n",
       "      <td>here is the latest edition of the institute fo...</td>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>https://www.washingtonpost.com/news/volokh-con...</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143893</th>\n",
       "      <td>Juno spacecraft slips into safe mode, putting ...</td>\n",
       "      <td>wednesday was meant to be a momentous day for ...</td>\n",
       "      <td>2016-10-20</td>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>https://www.washingtonpost.com/news/speaking-o...</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143894</th>\n",
       "      <td>The down-ballot GOP candidate’s guide to survi...</td>\n",
       "      <td>when you run for office you tend to get a lot ...</td>\n",
       "      <td>2016-10-28</td>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>https://www.washingtonpost.com/blogs/post-part...</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143895</th>\n",
       "      <td>Part of MD 202 in Prince George’s County close...</td>\n",
       "      <td>part of maryland route 202 in prince george’s ...</td>\n",
       "      <td>2016-11-21</td>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>https://www.washingtonpost.com/news/dr-gridloc...</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>143896 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Title  \\\n",
       "0       Full text: Clinton testifies before House comm...   \n",
       "1       Full transcript: FBI Director James Comey test...   \n",
       "2       Transcript of Zuckerberg’s appearance before H...   \n",
       "3          Transcript of Mark Zuckerberg’s Senate hearing   \n",
       "4                The October Democratic debate transcript   \n",
       "...                                                   ...   \n",
       "143891  By Martin Weil\\nJuly 25, 2016\\nA man was shot ...   \n",
       "143892  Short Circuit: A roundup of recent federal cou...   \n",
       "143893  Juno spacecraft slips into safe mode, putting ...   \n",
       "143894  The down-ballot GOP candidate’s guide to survi...   \n",
       "143895  Part of MD 202 in Prince George’s County close...   \n",
       "\n",
       "                                                     Text       Date  \\\n",
       "0       2015 clintons benghazi hearing in three minute... 2016-09-23   \n",
       "1       comey rogers testify on alleged russian interf... 2017-03-21   \n",
       "2       top takeaways from mark zuckerberg’s hearings ... 2018-04-11   \n",
       "3       facebook ceo mark zuckerberg sat down before l... 2018-04-10   \n",
       "4       sen kamala harris dcalif sen bernie sanders iv... 2019-10-16   \n",
       "...                                                   ...        ...   \n",
       "143891  a man was shot and wounded late monday on nort... 2016-07-26   \n",
       "143892  here is the latest edition of the institute fo... 2017-01-04   \n",
       "143893  wednesday was meant to be a momentous day for ... 2016-10-20   \n",
       "143894  when you run for office you tend to get a lot ... 2016-10-28   \n",
       "143895  part of maryland route 202 in prince george’s ... 2016-11-21   \n",
       "\n",
       "             News Paper                                               Link  \\\n",
       "0       Washington_Post  https://www.washingtonpost.com/news/post-polit...   \n",
       "1       Washington_Post  https://www.washingtonpost.com/news/post-polit...   \n",
       "2       Washington_Post  https://www.washingtonpost.com/news/the-switch...   \n",
       "3       Washington_Post  https://www.washingtonpost.com/news/the-switch...   \n",
       "4       Washington_Post  https://www.washingtonpost.com/politics/2019/1...   \n",
       "...                 ...                                                ...   \n",
       "143891  Washington_Post  https://www.washingtonpost.com/local/public-sa...   \n",
       "143892  Washington_Post  https://www.washingtonpost.com/news/volokh-con...   \n",
       "143893  Washington_Post  https://www.washingtonpost.com/news/speaking-o...   \n",
       "143894  Washington_Post  https://www.washingtonpost.com/blogs/post-part...   \n",
       "143895  Washington_Post  https://www.washingtonpost.com/news/dr-gridloc...   \n",
       "\n",
       "        Year  \n",
       "0       2016  \n",
       "1       2017  \n",
       "2       2018  \n",
       "3       2018  \n",
       "4       2019  \n",
       "...      ...  \n",
       "143891  2016  \n",
       "143892  2017  \n",
       "143893  2016  \n",
       "143894  2016  \n",
       "143895  2016  \n",
       "\n",
       "[143896 rows x 6 columns]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WP_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "48bf495e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = count_lexicon_words(WP_clean, EPA_Lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "b38c4627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "      <th>Date</th>\n",
       "      <th>News Paper</th>\n",
       "      <th>Link</th>\n",
       "      <th>Year</th>\n",
       "      <th>Absolute Frequency</th>\n",
       "      <th>Absolute Present</th>\n",
       "      <th>Relative Frequency</th>\n",
       "      <th>Relative Present</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>The issues 2020 Democrats are running\\non, acc...</td>\n",
       "      <td>politics analysis the issues 2020 democrats ar...</td>\n",
       "      <td>2019-04-08</td>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>https://www.washingtonpost.com/graphics/politi...</td>\n",
       "      <td>2019</td>\n",
       "      <td>67</td>\n",
       "      <td>6</td>\n",
       "      <td>0.605458</td>\n",
       "      <td>0.054220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>Treacherous freezing rain tonight with dangero...</td>\n",
       "      <td>a winter storm warning for montgomery fairfax ...</td>\n",
       "      <td>2016-02-16</td>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>https://www.washingtonpost.com/news/capital-we...</td>\n",
       "      <td>2016</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>0.986288</td>\n",
       "      <td>0.024056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>Hurricane Michael forecast updates: Historic s...</td>\n",
       "      <td>in the wake of hurricane michael panama city r...</td>\n",
       "      <td>2018-10-10</td>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>https://www.washingtonpost.com/weather/2018/10...</td>\n",
       "      <td>2018</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>0.687380</td>\n",
       "      <td>0.082486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>Opinion Change is afoot in Trump country</td>\n",
       "      <td>add to your saved stories save gift article sh...</td>\n",
       "      <td>2018-01-11</td>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>https://www.washingtonpost.com/news/theworldpo...</td>\n",
       "      <td>2018</td>\n",
       "      <td>97</td>\n",
       "      <td>16</td>\n",
       "      <td>2.788158</td>\n",
       "      <td>0.459902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>Radical warming in Siberia leaves millions on ...</td>\n",
       "      <td>2°c beyond the limit extreme climate change ha...</td>\n",
       "      <td>2019-10-04</td>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>https://www.washingtonpost.com/graphics/2019/n...</td>\n",
       "      <td>2019</td>\n",
       "      <td>74</td>\n",
       "      <td>11</td>\n",
       "      <td>2.167545</td>\n",
       "      <td>0.322203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143826</th>\n",
       "      <td>PM Update: Dense fog and drizzle tonight, then...</td>\n",
       "      <td>1045 pm fog has formed across the area this ev...</td>\n",
       "      <td>2018-02-23</td>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>https://www.washingtonpost.com/news/capital-we...</td>\n",
       "      <td>2018</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.986755</td>\n",
       "      <td>0.662252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143836</th>\n",
       "      <td>Don’t mind those rumbling jets overhead. It’s ...</td>\n",
       "      <td>the north american aerospace defense command —...</td>\n",
       "      <td>2017-11-14</td>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>https://www.washingtonpost.com/news/dr-gridloc...</td>\n",
       "      <td>2017</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.324503</td>\n",
       "      <td>0.662252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143837</th>\n",
       "      <td>Ryan’s tax hype falls flat</td>\n",
       "      <td>house speaker paul d ryan rwis perhaps to calm...</td>\n",
       "      <td>2017-06-21</td>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>https://www.washingtonpost.com/blogs/right-tur...</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.662252</td>\n",
       "      <td>0.662252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143845</th>\n",
       "      <td>By Martin Weil\\nJanuary 25, 2016\\nMotorists an...</td>\n",
       "      <td>motorists and pedestrians across the washingto...</td>\n",
       "      <td>2016-01-26</td>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>https://www.washingtonpost.com/local/streets-b...</td>\n",
       "      <td>2016</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.986755</td>\n",
       "      <td>0.662252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143886</th>\n",
       "      <td>Nationals vs. Cardinals weather forecast: Part...</td>\n",
       "      <td>comment 0 add to your saved stories save gift ...</td>\n",
       "      <td>2019-05-01</td>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>https://www.washingtonpost.com/weather/2019/05...</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.662252</td>\n",
       "      <td>0.662252</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3923 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Title  \\\n",
       "48      The issues 2020 Democrats are running\\non, acc...   \n",
       "346     Treacherous freezing rain tonight with dangero...   \n",
       "481     Hurricane Michael forecast updates: Historic s...   \n",
       "557              Opinion Change is afoot in Trump country   \n",
       "583     Radical warming in Siberia leaves millions on ...   \n",
       "...                                                   ...   \n",
       "143826  PM Update: Dense fog and drizzle tonight, then...   \n",
       "143836  Don’t mind those rumbling jets overhead. It’s ...   \n",
       "143837                         Ryan’s tax hype falls flat   \n",
       "143845  By Martin Weil\\nJanuary 25, 2016\\nMotorists an...   \n",
       "143886  Nationals vs. Cardinals weather forecast: Part...   \n",
       "\n",
       "                                                     Text       Date  \\\n",
       "48      politics analysis the issues 2020 democrats ar... 2019-04-08   \n",
       "346     a winter storm warning for montgomery fairfax ... 2016-02-16   \n",
       "481     in the wake of hurricane michael panama city r... 2018-10-10   \n",
       "557     add to your saved stories save gift article sh... 2018-01-11   \n",
       "583     2°c beyond the limit extreme climate change ha... 2019-10-04   \n",
       "...                                                   ...        ...   \n",
       "143826  1045 pm fog has formed across the area this ev... 2018-02-23   \n",
       "143836  the north american aerospace defense command —... 2017-11-14   \n",
       "143837  house speaker paul d ryan rwis perhaps to calm... 2017-06-21   \n",
       "143845  motorists and pedestrians across the washingto... 2016-01-26   \n",
       "143886  comment 0 add to your saved stories save gift ... 2019-05-01   \n",
       "\n",
       "             News Paper                                               Link  \\\n",
       "48      Washington_Post  https://www.washingtonpost.com/graphics/politi...   \n",
       "346     Washington_Post  https://www.washingtonpost.com/news/capital-we...   \n",
       "481     Washington_Post  https://www.washingtonpost.com/weather/2018/10...   \n",
       "557     Washington_Post  https://www.washingtonpost.com/news/theworldpo...   \n",
       "583     Washington_Post  https://www.washingtonpost.com/graphics/2019/n...   \n",
       "...                 ...                                                ...   \n",
       "143826  Washington_Post  https://www.washingtonpost.com/news/capital-we...   \n",
       "143836  Washington_Post  https://www.washingtonpost.com/news/dr-gridloc...   \n",
       "143837  Washington_Post  https://www.washingtonpost.com/blogs/right-tur...   \n",
       "143845  Washington_Post  https://www.washingtonpost.com/local/streets-b...   \n",
       "143886  Washington_Post  https://www.washingtonpost.com/weather/2019/05...   \n",
       "\n",
       "        Year  Absolute Frequency  Absolute Present  Relative Frequency  \\\n",
       "48      2019                  67                 6            0.605458   \n",
       "346     2016                  41                 1            0.986288   \n",
       "481     2018                  25                 3            0.687380   \n",
       "557     2018                  97                16            2.788158   \n",
       "583     2019                  74                11            2.167545   \n",
       "...      ...                 ...               ...                 ...   \n",
       "143826  2018                   3                 1            1.986755   \n",
       "143836  2017                   2                 1            1.324503   \n",
       "143837  2017                   1                 1            0.662252   \n",
       "143845  2016                   3                 1            1.986755   \n",
       "143886  2019                   1                 1            0.662252   \n",
       "\n",
       "        Relative Present  \n",
       "48              0.054220  \n",
       "346             0.024056  \n",
       "481             0.082486  \n",
       "557             0.459902  \n",
       "583             0.322203  \n",
       "...                  ...  \n",
       "143826          0.662252  \n",
       "143836          0.662252  \n",
       "143837          0.662252  \n",
       "143845          0.662252  \n",
       "143886          0.662252  \n",
       "\n",
       "[3923 rows x 10 columns]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[test[\"Relative Frequency\"] >= 0.6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "c7463261",
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"Climate\"] = \"No\"\n",
    "test.loc[test[\"Relative Frequency\"] >= 0.6, \"Climate\"] = \"Yes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "d5c3ab67",
   "metadata": {},
   "outputs": [],
   "source": [
    "WP[\"Climate\"] = test[\"Climate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "1e824c2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "      <th>Date</th>\n",
       "      <th>News Paper</th>\n",
       "      <th>Link</th>\n",
       "      <th>Year</th>\n",
       "      <th>Climate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>The issues 2020 Democrats are running\\non, acc...</td>\n",
       "      <td>Politics Analysis\\nThe issues 2020 Democrats a...</td>\n",
       "      <td>2019-04-08</td>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>https://www.washingtonpost.com/graphics/politi...</td>\n",
       "      <td>2019</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>Treacherous freezing rain tonight with dangero...</td>\n",
       "      <td>*A winter storm warning for Montgomery, Fairfa...</td>\n",
       "      <td>2016-02-16</td>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>https://www.washingtonpost.com/news/capital-we...</td>\n",
       "      <td>2016</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>Hurricane Michael forecast updates: Historic s...</td>\n",
       "      <td>In the wake of Hurricane Michael, Panama City ...</td>\n",
       "      <td>2018-10-10</td>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>https://www.washingtonpost.com/weather/2018/10...</td>\n",
       "      <td>2018</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>Opinion Change is afoot in Trump country</td>\n",
       "      <td>Add to your saved stories\\nSave\\nGift Article\\...</td>\n",
       "      <td>2018-01-11</td>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>https://www.washingtonpost.com/news/theworldpo...</td>\n",
       "      <td>2018</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>Radical warming in Siberia leaves millions on ...</td>\n",
       "      <td>2°C: Beyond the limit\\nExtreme climate change ...</td>\n",
       "      <td>2019-10-04</td>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>https://www.washingtonpost.com/graphics/2019/n...</td>\n",
       "      <td>2019</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143826</th>\n",
       "      <td>PM Update: Dense fog and drizzle tonight, then...</td>\n",
       "      <td>10:45 p.m.: Fog has formed across the area thi...</td>\n",
       "      <td>2018-02-23</td>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>https://www.washingtonpost.com/news/capital-we...</td>\n",
       "      <td>2018</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143836</th>\n",
       "      <td>Don’t mind those rumbling jets overhead. It’s ...</td>\n",
       "      <td>The North American Aerospace Defense Command —...</td>\n",
       "      <td>2017-11-14</td>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>https://www.washingtonpost.com/news/dr-gridloc...</td>\n",
       "      <td>2017</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143837</th>\n",
       "      <td>Ryan’s tax hype falls flat</td>\n",
       "      <td>House Speaker Paul D. Ryan (R-Wis.), perhaps t...</td>\n",
       "      <td>2017-06-21</td>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>https://www.washingtonpost.com/blogs/right-tur...</td>\n",
       "      <td>2017</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143845</th>\n",
       "      <td>By Martin Weil\\nJanuary 25, 2016\\nMotorists an...</td>\n",
       "      <td>Motorists and pedestrians across the Washingto...</td>\n",
       "      <td>2016-01-26</td>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>https://www.washingtonpost.com/local/streets-b...</td>\n",
       "      <td>2016</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143886</th>\n",
       "      <td>Nationals vs. Cardinals weather forecast: Part...</td>\n",
       "      <td>Comment\\n0\\nAdd to your saved stories\\nSave\\nG...</td>\n",
       "      <td>2019-05-01</td>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>https://www.washingtonpost.com/weather/2019/05...</td>\n",
       "      <td>2019</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3923 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Title  \\\n",
       "48      The issues 2020 Democrats are running\\non, acc...   \n",
       "346     Treacherous freezing rain tonight with dangero...   \n",
       "481     Hurricane Michael forecast updates: Historic s...   \n",
       "557              Opinion Change is afoot in Trump country   \n",
       "583     Radical warming in Siberia leaves millions on ...   \n",
       "...                                                   ...   \n",
       "143826  PM Update: Dense fog and drizzle tonight, then...   \n",
       "143836  Don’t mind those rumbling jets overhead. It’s ...   \n",
       "143837                         Ryan’s tax hype falls flat   \n",
       "143845  By Martin Weil\\nJanuary 25, 2016\\nMotorists an...   \n",
       "143886  Nationals vs. Cardinals weather forecast: Part...   \n",
       "\n",
       "                                                     Text       Date  \\\n",
       "48      Politics Analysis\\nThe issues 2020 Democrats a... 2019-04-08   \n",
       "346     *A winter storm warning for Montgomery, Fairfa... 2016-02-16   \n",
       "481     In the wake of Hurricane Michael, Panama City ... 2018-10-10   \n",
       "557     Add to your saved stories\\nSave\\nGift Article\\... 2018-01-11   \n",
       "583     2°C: Beyond the limit\\nExtreme climate change ... 2019-10-04   \n",
       "...                                                   ...        ...   \n",
       "143826  10:45 p.m.: Fog has formed across the area thi... 2018-02-23   \n",
       "143836  The North American Aerospace Defense Command —... 2017-11-14   \n",
       "143837  House Speaker Paul D. Ryan (R-Wis.), perhaps t... 2017-06-21   \n",
       "143845  Motorists and pedestrians across the Washingto... 2016-01-26   \n",
       "143886  Comment\\n0\\nAdd to your saved stories\\nSave\\nG... 2019-05-01   \n",
       "\n",
       "             News Paper                                               Link  \\\n",
       "48      Washington_Post  https://www.washingtonpost.com/graphics/politi...   \n",
       "346     Washington_Post  https://www.washingtonpost.com/news/capital-we...   \n",
       "481     Washington_Post  https://www.washingtonpost.com/weather/2018/10...   \n",
       "557     Washington_Post  https://www.washingtonpost.com/news/theworldpo...   \n",
       "583     Washington_Post  https://www.washingtonpost.com/graphics/2019/n...   \n",
       "...                 ...                                                ...   \n",
       "143826  Washington_Post  https://www.washingtonpost.com/news/capital-we...   \n",
       "143836  Washington_Post  https://www.washingtonpost.com/news/dr-gridloc...   \n",
       "143837  Washington_Post  https://www.washingtonpost.com/blogs/right-tur...   \n",
       "143845  Washington_Post  https://www.washingtonpost.com/local/streets-b...   \n",
       "143886  Washington_Post  https://www.washingtonpost.com/weather/2019/05...   \n",
       "\n",
       "        Year Climate  \n",
       "48      2019     Yes  \n",
       "346     2016     Yes  \n",
       "481     2018     Yes  \n",
       "557     2018     Yes  \n",
       "583     2019     Yes  \n",
       "...      ...     ...  \n",
       "143826  2018     Yes  \n",
       "143836  2017     Yes  \n",
       "143837  2017     Yes  \n",
       "143845  2016     Yes  \n",
       "143886  2019     Yes  \n",
       "\n",
       "[3923 rows x 7 columns]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WP[WP[\"Climate\"] == \"Yes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e499ed3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
