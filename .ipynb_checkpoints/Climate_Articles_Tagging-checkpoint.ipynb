{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "549a3d8c",
   "metadata": {},
   "source": [
    "# 0. Packages and Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7567ca2",
   "metadata": {},
   "source": [
    "## 0.1. Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bd2f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "from nltk import ngrams\n",
    "from collections import Counter\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, pipeline, AutoModelForSequenceClassification, AutoModelForSeq2SeqLM\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pyarrow\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0aaaa2",
   "metadata": {},
   "source": [
    "## 0.2. Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce24a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre-process text for lexicon based approaches\n",
    "def preprocess_text(text):\n",
    "    # remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # convert to lower case\n",
    "    text = text.lower()\n",
    "    # remove blank spaces\n",
    "    text = ' '.join(text.split())\n",
    "    # remove newline characters\n",
    "    text = text.replace('\\n', '')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b22206e",
   "metadata": {},
   "source": [
    "### Absolute Count with Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2495792e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create function that take the dataframe, lexicon and n-gram value (how many n-grams should be considered) and determine the \n",
    "#count of words in dataframe text that match the lexicon\n",
    "def count_lexicon_words_1(text_df, lexicon):\n",
    "    lexicon = lexicon[\"Lexicon\"]\n",
    "    count = []\n",
    "\n",
    "    for text in text_df[\"Text\"]:\n",
    "        lexicon_counts = 0\n",
    "\n",
    "        for word in lexicon:\n",
    "            lexicon_counts += text.lower().count(word.lower())\n",
    "            \n",
    "        count.append(lexicon_counts)\n",
    "        \n",
    "    text_df[\"Lexicon Count\"] = count\n",
    "    \n",
    "    return(text_df)\n",
    "\n",
    "#create a function that used the lexicon approach to determine with the target is yes or no\n",
    "def lexicon_target_classifier_1(df, treshold):\n",
    "    target = []\n",
    "    \n",
    "    count = df[\"Lexicon Count\"]\n",
    "    \n",
    "    for c in count:\n",
    "        if c < treshold:\n",
    "            target.append(\"No\")\n",
    "        else:\n",
    "            target.append(\"Yes\")\n",
    "            \n",
    "    df[\"Target Lexicon\"] = target\n",
    "    return(df)\n",
    "\n",
    "#Combine both functions to classify articles based on the lexicon\n",
    "def lexicon_climate_classifier_1(text_df, lexicon, treshold):\n",
    "    df = count_lexicon_words_1(text_df, lexicon)\n",
    "    \n",
    "    return(lexicon_target_classifier_1(df, treshold))\n",
    "\n",
    "def threshold_metrics_1(df_text, lexicon, min_treshhold, max_treshhold):\n",
    "    for i in range(min_treshhold, max_treshhold + 1):\n",
    "        df = lexicon_climate_classifier_1(df_text, lexicon, i)\n",
    "        cross_table = pd.crosstab(df['Target'], df['Target Lexicon'], margins=True)\n",
    "        \n",
    "        # calculate classification metrics using scikit-learn\n",
    "        accuracy = (cross_table.iloc[0, 0] + cross_table.iloc[1, 1]) / cross_table.loc['All', 'All'] if cross_table.shape == (3,3) else cross_table.iloc[1,0] / (cross_table.iloc[1, 0] + cross_table.iloc[0,0]) \n",
    "        precision = cross_table.iloc[1,1] / cross_table.iloc[0,1] if cross_table.iloc[0,1] != 0 else 0\n",
    "        recall = cross_table.iloc[1,1] / cross_table.iloc[1,0] if cross_table.iloc[1,0] != 0 else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "\n",
    "        # print the metrics\n",
    "        print(\"Threshhold:\", i)\n",
    "        print(\"Accuracy:\", accuracy)\n",
    "        print(\"Precision:\", precision)\n",
    "        print(\"Recall:\", recall)\n",
    "        print(\"F1 score:\", f1_score)\n",
    "        print(cross_table)\n",
    "        print(\"\\n\")\n",
    "        \n",
    "def get_metrics_df_1(df_text, lexicon, min_treshhold, max_treshhold, Lexicon_name):    \n",
    "    Accuracy = []\n",
    "    Precision = []\n",
    "    Recall = []\n",
    "    F1_score = []\n",
    "    Name = []\n",
    "    Treshhold = []\n",
    "    Technique = []\n",
    "    \n",
    "    for i in range(min_treshhold, max_treshhold + 1):\n",
    "        df = lexicon_climate_classifier_1(df_text, lexicon, i)\n",
    "        cross_table = pd.crosstab(df['Target'], df['Target Lexicon'], margins=True)\n",
    "\n",
    "        # calculate classification metrics using scikit-learn\n",
    "        accuracy = (cross_table.iloc[0, 0] + cross_table.iloc[1, 1]) / cross_table.loc['All', 'All'] if cross_table.shape == (3,3) else cross_table.iloc[1,0] / (cross_table.iloc[1, 0] + cross_table.iloc[0,0]) \n",
    "        precision = cross_table.iloc[1,1] / cross_table.iloc[0,1] if cross_table.iloc[0,1] != 0 else 0\n",
    "        recall = cross_table.iloc[1,1] / cross_table.iloc[1,0] if cross_table.iloc[1,0] != 0 else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "        \n",
    "        Accuracy.append(accuracy)\n",
    "        Precision.append(precision)\n",
    "        Recall.append(recall)\n",
    "        F1_score.append(f1_score)\n",
    "        Name.append(Lexicon_name)\n",
    "        Treshhold.append(i)\n",
    "        Technique.append(\"Absolute Frequency\")\n",
    "        \n",
    "    return(pd.DataFrame({\"Lexicon\" : Name, \"Technique\": Technique, \"Treshhold\" : Treshhold, \"Accuracy\" : Accuracy, \"Precision\" : Precision, \"Recall\" : Recall, \"F1 Score\" : F1_score}))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf746a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create function that take the dataframe, lexicon and n-gram value (how many n-grams should be considered) and determine the \n",
    "#count of words in dataframe text that match the lexicon\n",
    "def count_lexicon_words_1(text_df, lexicon):\n",
    "    lexicon = lexicon[\"Lexicon\"]\n",
    "    count = []\n",
    "\n",
    "    for text in text_df[\"Text\"]:\n",
    "        lexicon_counts = 0\n",
    "\n",
    "        for word in lexicon:\n",
    "            lexicon_counts += text.lower().count(word.lower())\n",
    "        \n",
    "        count.append(lexicon_counts)\n",
    "        \n",
    "    text_df[\"Lexicon Count\"] = count\n",
    "    \n",
    "    return(text_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628c64e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_lexicon_words_1(tag_climate_df, BBC_Lexicon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c69c28",
   "metadata": {},
   "source": [
    "### Relative Count with Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3ca9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create function that take the dataframe, lexicon and n-gram value (how many n-grams should be considered) and determine the \n",
    "#count of words in dataframe text that match the lexicon\n",
    "def count_lexicon_words_2(text_df, lexicon):\n",
    "    lexicon = lexicon[\"Lexicon\"]\n",
    "    count = []\n",
    "    \n",
    "    for text in text_df[\"Text\"]:\n",
    "        lexicon_counts = 0\n",
    "\n",
    "        for word in lexicon:\n",
    "            lexicon_counts += text.lower().count(word.lower())\n",
    "        \n",
    "        word_list = text.split() \n",
    "        word_count = len(word_list)\n",
    "        count.append((lexicon_counts/word_count)*100)\n",
    "\n",
    "    text_df[\"Lexicon Count\"] = count\n",
    "    \n",
    "    return(text_df)\n",
    "\n",
    "#create a function that used the lexicon approach to determine with the target is yes or no\n",
    "def lexicon_target_classifier_2(df, treshold):\n",
    "    target = []\n",
    "    \n",
    "    count = df[\"Lexicon Count\"]\n",
    "    \n",
    "    for c in count:\n",
    "        if c < treshold:\n",
    "            target.append(\"No\")\n",
    "        else:\n",
    "            target.append(\"Yes\")\n",
    "            \n",
    "    df[\"Target Lexicon\"] = target\n",
    "    return(df)\n",
    "\n",
    "#Combine both functions to classify articles based on the lexicon\n",
    "def lexicon_climate_classifier_2(text_df, lexicon, treshold):\n",
    "    df = count_lexicon_words_2(text_df, lexicon)\n",
    "    \n",
    "    return(lexicon_target_classifier_2(df, treshold))\n",
    "\n",
    "def threshold_metrics_2(df_text, lexicon, min_treshhold, max_treshhold, jump):\n",
    "    \n",
    "    if(jump == 0):\n",
    "        df = lexicon_climate_classifier_2(df_text, lexicon, min_treshhold)\n",
    "        cross_table = pd.crosstab(df['Target'], df['Target Lexicon'], margins=True)\n",
    "\n",
    "        # calculate classification metrics using scikit-learn\n",
    "        accuracy = (cross_table.iloc[0, 0] + cross_table.iloc[1, 1]) / cross_table.loc['All', 'All'] if cross_table.shape == (3,3) else cross_table.iloc[1,0] / (cross_table.iloc[1, 0] + cross_table.iloc[0,0]) \n",
    "        precision = cross_table.iloc[1,1] / cross_table.iloc[0,1] if cross_table.iloc[0,1] != 0 else 0\n",
    "        recall = cross_table.iloc[1,1] / cross_table.iloc[1,0] if cross_table.iloc[1,0] != 0 else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "\n",
    "        # print the metrics\n",
    "        print(\"Threshhold:\", min_treshhold)\n",
    "        print(\"Accuracy:\", accuracy)\n",
    "        print(\"Precision:\", precision)\n",
    "        print(\"Recall:\", recall)\n",
    "        print(\"F1 score:\", f1_score)\n",
    "        print(cross_table)\n",
    "        print(\"\\n\")\n",
    "\n",
    "    else:   \n",
    "        for num in range(int((max_treshhold - min_treshhold) / jump) + 1):\n",
    "            i = min_treshhold + num * jump\n",
    "            df = lexicon_climate_classifier_2(df_text, lexicon, i)\n",
    "            cross_table = pd.crosstab(df['Target'], df['Target Lexicon'], margins=True)\n",
    "\n",
    "            # calculate classification metrics using scikit-learn\n",
    "            accuracy = (cross_table.iloc[0, 0] + cross_table.iloc[1, 1]) / cross_table.loc['All', 'All'] if cross_table.shape == (3,3) else cross_table.iloc[1,0] / (cross_table.iloc[1, 0] + cross_table.iloc[0,0]) \n",
    "            precision = cross_table.iloc[1,1] / cross_table.iloc[0,1] if cross_table.iloc[0,1] != 0 else 0\n",
    "            recall = cross_table.iloc[1,1] / cross_table.iloc[1,0] if cross_table.iloc[1,0] != 0 else 0\n",
    "            f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "\n",
    "            # print the metrics\n",
    "            print(\"Threshhold:\", i)\n",
    "            print(\"Accuracy:\", accuracy)\n",
    "            print(\"Precision:\", precision)\n",
    "            print(\"Recall:\", recall)\n",
    "            print(\"F1 score:\", f1_score)\n",
    "            print(cross_table)\n",
    "            print(\"\\n\")\n",
    "\n",
    "def get_metrics_df_2(df_text, lexicon, min_treshhold, max_treshhold, jump, Lexicon_name):    \n",
    "    Accuracy = []\n",
    "    Precision = []\n",
    "    Recall = []\n",
    "    F1_score = []\n",
    "    Name = []\n",
    "    Treshhold = []\n",
    "    Technique = []\n",
    "    \n",
    "    for num in range(int((max_treshhold - min_treshhold) / jump) + 1):\n",
    "        i = min_treshhold + num * jump\n",
    "        df = lexicon_climate_classifier_2(df_text, lexicon, i)\n",
    "        cross_table = pd.crosstab(df['Target'], df['Target Lexicon'], margins=True)\n",
    "\n",
    "        # calculate classification metrics using scikit-learn\n",
    "        accuracy = (cross_table.iloc[0, 0] + cross_table.iloc[1, 1]) / cross_table.loc['All', 'All'] if cross_table.shape == (3,3) else cross_table.iloc[1,0] / (cross_table.iloc[1, 0] + cross_table.iloc[0,0]) \n",
    "        precision = cross_table.iloc[1,1] / cross_table.iloc[0,1] if cross_table.iloc[0,1] != 0 else 0\n",
    "        recall = cross_table.iloc[1,1] / cross_table.iloc[1,0] if cross_table.iloc[1,0] != 0 else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "        \n",
    "        Accuracy.append(accuracy)\n",
    "        Precision.append(precision)\n",
    "        Recall.append(recall)\n",
    "        F1_score.append(f1_score)\n",
    "        Name.append(Lexicon_name)\n",
    "        Treshhold.append(i)\n",
    "        Technique.append(\"Relative Frequency\")\n",
    "        \n",
    "    return(pd.DataFrame({\"Lexicon\" : Name, \"Technique\": Technique, \"Treshhold\" : Treshhold, \"Accuracy\" : Accuracy, \"Precision\" : Precision, \"Recall\" : Recall, \"F1 Score\" : F1_score}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f4d81d",
   "metadata": {},
   "source": [
    "### Absolute Term Presences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6226be1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create function that take the dataframe, lexicon and n-gram value (how many n-grams should be considered) and determine the \n",
    "#count of words in dataframe text that match the lexicon\n",
    "def count_lexicon_words_3(text_df, lexicon):\n",
    "    lexicon = lexicon[\"Lexicon\"]\n",
    "    count = []\n",
    "\n",
    "    for text in text_df[\"Text\"]:\n",
    "        lexicon_counts = 0\n",
    "\n",
    "        for word in lexicon:\n",
    "            if text.lower().count(word.lower()) > 0:\n",
    "                lexicon_counts += 1\n",
    "        \n",
    "        count.append(lexicon_counts)\n",
    "        \n",
    "    text_df[\"Lexicon Count\"] = count\n",
    "    \n",
    "    return text_df\n",
    "\n",
    "#create a function that used the lexicon approach to determine with the target is yes or no\n",
    "def lexicon_target_classifier_3(df, treshold):\n",
    "    target = []\n",
    "    \n",
    "    count = df[\"Lexicon Count\"]\n",
    "    \n",
    "    for c in count:\n",
    "        if c < treshold:\n",
    "            target.append(\"No\")\n",
    "        else:\n",
    "            target.append(\"Yes\")\n",
    "            \n",
    "    df[\"Target Lexicon\"] = target\n",
    "    return(df)\n",
    "\n",
    "#Combine both functions to classify articles based on the lexicon\n",
    "def lexicon_climate_classifier_3(text_df, lexicon, treshold):\n",
    "    df = count_lexicon_words_3(text_df, lexicon)\n",
    "    \n",
    "    return(lexicon_target_classifier_3(df, treshold))\n",
    "\n",
    "def threshold_metrics_3(df_text, lexicon, min_treshhold, max_treshhold):\n",
    "    for i in range(min_treshhold, max_treshhold + 1):\n",
    "        df = lexicon_climate_classifier_3(df_text, lexicon, i)\n",
    "        cross_table = pd.crosstab(df['Target'], df['Target Lexicon'], margins=True)\n",
    "        \n",
    "        # calculate classification metrics using scikit-learn\n",
    "        accuracy = (cross_table.iloc[0, 0] + cross_table.iloc[1, 1]) / cross_table.loc['All', 'All'] if cross_table.shape == (3,3) else cross_table.iloc[1,0] / (cross_table.iloc[1, 0] + cross_table.iloc[0,0]) \n",
    "        precision = cross_table.iloc[1,1] / cross_table.iloc[0,1] if cross_table.iloc[0,1] != 0 else 0\n",
    "        recall = cross_table.iloc[1,1] / cross_table.iloc[1,0] if cross_table.iloc[1,0] != 0 else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "\n",
    "        # print the metrics\n",
    "        print(\"Threshhold:\", i)\n",
    "        print(\"Accuracy:\", accuracy)\n",
    "        print(\"Precision:\", precision)\n",
    "        print(\"Recall:\", recall)\n",
    "        print(\"F1 score:\", f1_score)\n",
    "        print(cross_table)\n",
    "        print(\"\\n\")\n",
    "        \n",
    "def get_metrics_df_3(df_text, lexicon, min_treshhold, max_treshhold, Lexicon_name):    \n",
    "    Accuracy = []\n",
    "    Precision = []\n",
    "    Recall = []\n",
    "    F1_score = []\n",
    "    Name = []\n",
    "    Treshhold = []\n",
    "    Technique = []\n",
    "    \n",
    "    for i in range(min_treshhold, max_treshhold + 1):\n",
    "        df = lexicon_climate_classifier_3(df_text, lexicon, i)\n",
    "        cross_table = pd.crosstab(df['Target'], df['Target Lexicon'], margins=True)\n",
    "\n",
    "        # calculate classification metrics using scikit-learn\n",
    "        accuracy = (cross_table.iloc[0, 0] + cross_table.iloc[1, 1]) / cross_table.loc['All', 'All'] if cross_table.shape == (3,3) else cross_table.iloc[1,0] / (cross_table.iloc[1, 0] + cross_table.iloc[0,0]) \n",
    "        precision = cross_table.iloc[1,1] / cross_table.iloc[0,1] if cross_table.iloc[0,1] != 0 else 0\n",
    "        recall = cross_table.iloc[1,1] / cross_table.iloc[1,0] if cross_table.iloc[1,0] != 0 else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "        \n",
    "        Accuracy.append(accuracy)\n",
    "        Precision.append(precision)\n",
    "        Recall.append(recall)\n",
    "        F1_score.append(f1_score)\n",
    "        Name.append(Lexicon_name)\n",
    "        Treshhold.append(i)\n",
    "        Technique.append(\"Absolute Presences\")\n",
    "        \n",
    "    return(pd.DataFrame({\"Lexicon\" : Name, \"Technique\": Technique, \"Treshhold\" : Treshhold, \"Accuracy\" : Accuracy, \"Precision\" : Precision, \"Recall\" : Recall, \"F1 Score\" : F1_score}))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dd8d36",
   "metadata": {},
   "source": [
    "### Relative Term Presences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f82d5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create function that take the dataframe, lexicon and n-gram value (how many n-grams should be considered) and determine the \n",
    "#count of words in dataframe text that match the lexicon\n",
    "def count_lexicon_words_4(text_df, lexicon):\n",
    "    lexicon = lexicon[\"Lexicon\"]\n",
    "    count = []\n",
    "\n",
    "    for text in text_df[\"Text\"]:\n",
    "        lexicon_counts = 0\n",
    "\n",
    "        for word in lexicon:\n",
    "            if text.lower().count(word.lower()) > 0:\n",
    "                lexicon_counts += 1\n",
    "        \n",
    "        count.append(lexicon_counts)\n",
    "        \n",
    "    text_df[\"Lexicon Count\"] = count\n",
    "    \n",
    "    return text_df\n",
    "\n",
    "#create a function that used the lexicon approach to determine with the target is yes or no\n",
    "def lexicon_target_classifier_4(df, treshold):\n",
    "    target = []\n",
    "    \n",
    "    count = df[\"Lexicon Count\"]\n",
    "    \n",
    "    for c in count:\n",
    "        if c < treshold:\n",
    "            target.append(\"No\")\n",
    "        else:\n",
    "            target.append(\"Yes\")\n",
    "            \n",
    "    df[\"Target Lexicon\"] = target\n",
    "    return(df)\n",
    "\n",
    "#Combine both functions to classify articles based on the lexicon\n",
    "def lexicon_climate_classifier_4(text_df, lexicon, treshold):\n",
    "    df = count_lexicon_words_4(text_df, lexicon)\n",
    "    \n",
    "    return(lexicon_target_classifier_4(df, treshold))\n",
    "\n",
    "def threshold_metrics_4(df_text, lexicon, min_treshhold, max_treshhold, jump):\n",
    "    \n",
    "    if(jump == 0):\n",
    "        df = lexicon_climate_classifier_4(df_text, lexicon, min_treshhold)\n",
    "        cross_table = pd.crosstab(df['Target'], df['Target Lexicon'], margins=True)\n",
    "\n",
    "        # calculate classification metrics using scikit-learn\n",
    "        accuracy = (cross_table.iloc[0, 0] + cross_table.iloc[1, 1]) / cross_table.loc['All', 'All'] if cross_table.shape == (3,3) else cross_table.iloc[1,0] / (cross_table.iloc[1, 0] + cross_table.iloc[0,0]) \n",
    "        precision = cross_table.iloc[1,1] / cross_table.iloc[0,1] if cross_table.iloc[0,1] != 0 else 0\n",
    "        recall = cross_table.iloc[1,1] / cross_table.iloc[1,0] if cross_table.iloc[1,0] != 0 else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "\n",
    "        # print the metrics\n",
    "        print(\"Threshhold:\", i)\n",
    "        print(\"Accuracy:\", accuracy)\n",
    "        print(\"Precision:\", precision)\n",
    "        print(\"Recall:\", recall)\n",
    "        print(\"F1 score:\", f1_score)\n",
    "        print(cross_table)\n",
    "        print(\"\\n\")\n",
    "        \n",
    "    else:\n",
    "        for num in range(int((max_treshhold - min_treshhold) / jump) + 1):\n",
    "            i = min_treshhold + num * jump\n",
    "            df = lexicon_climate_classifier_4(df_text, lexicon, i)\n",
    "            cross_table = pd.crosstab(df['Target'], df['Target Lexicon'], margins=True)\n",
    "\n",
    "            # calculate classification metrics using scikit-learn\n",
    "            accuracy = (cross_table.iloc[0, 0] + cross_table.iloc[1, 1]) / cross_table.loc['All', 'All'] if cross_table.shape == (3,3) else cross_table.iloc[1,0] / (cross_table.iloc[1, 0] + cross_table.iloc[0,0]) \n",
    "            precision = cross_table.iloc[1,1] / cross_table.iloc[0,1] if cross_table.iloc[0,1] != 0 else 0\n",
    "            recall = cross_table.iloc[1,1] / cross_table.iloc[1,0] if cross_table.iloc[1,0] != 0 else 0\n",
    "            f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "\n",
    "            # print the metrics\n",
    "            print(\"Threshhold:\", i)\n",
    "            print(\"Accuracy:\", accuracy)\n",
    "            print(\"Precision:\", precision)\n",
    "            print(\"Recall:\", recall)\n",
    "            print(\"F1 score:\", f1_score)\n",
    "            print(cross_table)\n",
    "            print(\"\\n\")\n",
    "\n",
    "def get_metrics_df_4(df_text, lexicon, min_treshhold, max_treshhold, jump, Lexicon_name):    \n",
    "    Accuracy = []\n",
    "    Precision = []\n",
    "    Recall = []\n",
    "    F1_score = []\n",
    "    Name = []\n",
    "    Treshhold = []\n",
    "    Technique = []\n",
    "    \n",
    "    for num in range(int((max_treshhold - min_treshhold) / jump) + 1):\n",
    "        i = min_treshhold + num * jump\n",
    "        df = lexicon_climate_classifier_4(df_text, lexicon, i)\n",
    "        cross_table = pd.crosstab(df['Target'], df['Target Lexicon'], margins=True)\n",
    "\n",
    "        # calculate classification metrics using scikit-learn\n",
    "        accuracy = (cross_table.iloc[0, 0] + cross_table.iloc[1, 1]) / cross_table.loc['All', 'All'] if cross_table.shape == (3,3) else cross_table.iloc[1,0] / (cross_table.iloc[1, 0] + cross_table.iloc[0,0]) \n",
    "        precision = cross_table.iloc[1,1] / cross_table.iloc[0,1] if cross_table.iloc[0,1] != 0 else 0\n",
    "        recall = cross_table.iloc[1,1] / cross_table.iloc[1,0] if cross_table.iloc[1,0] != 0 else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "        \n",
    "        Accuracy.append(accuracy)\n",
    "        Precision.append(precision)\n",
    "        Recall.append(recall)\n",
    "        F1_score.append(f1_score)\n",
    "        Name.append(Lexicon_name)\n",
    "        Treshhold.append(i)\n",
    "        Technique.append(\"Relative Presences\")\n",
    "        \n",
    "    return(pd.DataFrame({\"Lexicon\" : Name, \"Technique\": Technique, \"Treshhold\" : Treshhold, \"Accuracy\" : Accuracy, \"Precision\" : Precision, \"Recall\" : Recall, \"F1 Score\" : F1_score}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358383a2",
   "metadata": {},
   "source": [
    "Accuracy: This metric measures the overall performance of a model. It is defined as the number of correct predictions divided by the total number of predictions. Accuracy is a good metric to use when the classes are roughly balanced, meaning there are about the same number of positive and negative examples in the dataset.\n",
    "\n",
    "Precision: This metric measures how many of the positive predictions made by a model are actually correct. It is defined as the number of true positives divided by the total number of positive predictions. Precision is a good metric to use when we care more about avoiding false positives than false negatives.\n",
    "\n",
    "Recall: This metric measures how many of the positive examples in the dataset are correctly predicted by the model. It is defined as the number of true positives divided by the total number of actual positive examples. Recall is a good metric to use when we care more about avoiding false negatives than false positives.\n",
    "\n",
    "F1 score: This metric is a weighted average of precision and recall, where the weight is determined by the beta parameter. The most common value for beta is 1, which gives equal weight to precision and recall. The F1 score is a good metric to use when we want to balance precision and recall, and when the classes are imbalanced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc10b67",
   "metadata": {},
   "source": [
    "# 1. Import Label Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b08d6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_climate_df = pd.read_parquet(\"Climate_Labels_Dataset.parquet\")\n",
    "tag_climate_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9d1d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only keep the required columns\n",
    "tag_climate_df = tag_climate_df[[\"Text\", \"Final_Climate_Change_Level_Label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ee616f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean the tabel\n",
    "tag_climate_df['Final_Climate_Change_Level_Label'] = tag_climate_df['Final_Climate_Change_Level_Label'].str.strip()\n",
    "tag_climate_df[tag_climate_df[\"Final_Climate_Change_Level_Label\"] == \"NA\"] = \"Na\"\n",
    "tag_climate_df[tag_climate_df[\"Final_Climate_Change_Level_Label\"] == \"0\"] = \"Na\"\n",
    "tag_climate_df[\"Target\"] = tag_climate_df[\"Final_Climate_Change_Level_Label\"].apply(lambda x: \"Yes\" if x in [\"High\", \"Medium\"] else \"No\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add6c615",
   "metadata": {},
   "outputs": [],
   "source": [
    "overview_labels_hms = tag_climate_df.groupby(\"Final_Climate_Change_Level_Label\")[\"Text\"].count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c689ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "overview_labels_hms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be23a6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "overview_labels = tag_climate_df.groupby(\"Target\")[\"Text\"].count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86308d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "overview_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf33a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#overview_labels_hms.to_csv(\"C:/Users/Boedt/OneDrive/Bureaublad/R Thesis/overview_tag_labels_hms\", index = False)\n",
    "#overview_labels.to_csv(\"C:/Users/Boedt/OneDrive/Bureaublad/R Thesis/overview_tag_labels\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516660b2",
   "metadata": {},
   "source": [
    "# 2. Taggers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2a7077",
   "metadata": {},
   "source": [
    "## 2.1. Lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31249337",
   "metadata": {},
   "source": [
    "### Global Change Lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c15452",
   "metadata": {},
   "source": [
    "Uitleg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54704fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the lexicon\n",
    "Global_Change_Lexicon = pd.read_csv(\"Global_Change_Lexicon\")\n",
    "Global_Change_Lexicon = Global_Change_Lexicon.drop_duplicates().reset_index(drop = True)\n",
    "Global_Change_Lexicon[\"Lexicon\"] = Global_Change_Lexicon[\"Lexicon\"].str.lower()\n",
    "\n",
    "Global_Change_Lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d26ff50",
   "metadata": {},
   "source": [
    "### IPCC Lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800c5ab8",
   "metadata": {},
   "source": [
    "Uitleg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41bb7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the lexicon\n",
    "IPCC_Lexicon = pd.read_csv(\"IPCC_Lexicon\")\n",
    "IPCC_Lexicon = IPCC_Lexicon.drop_duplicates().reset_index(drop = True)\n",
    "IPCC_Lexicon[\"Lexicon\"] = IPCC_Lexicon[\"Lexicon\"].str.lower()\n",
    "\n",
    "IPCC_Lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb16e34",
   "metadata": {},
   "source": [
    "### Wikipedia Lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90610cf6",
   "metadata": {},
   "source": [
    "Uitleg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335d527b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the lexicon\n",
    "Wikipedia_Lexicon = pd.read_csv(\"Wikipedia_Lexicon\")\n",
    "Wikipedia_Lexicon = Wikipedia_Lexicon.drop_duplicates().reset_index(drop = True)\n",
    "Wikipedia_Lexicon[\"Lexicon\"] = Wikipedia_Lexicon[\"Lexicon\"].str.lower()\n",
    "\n",
    "Wikipedia_Lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c8a3be",
   "metadata": {},
   "source": [
    "### EPA Lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638f81be",
   "metadata": {},
   "source": [
    "Uitleg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93d019b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the lexicon\n",
    "EPA_Lexicon = pd.read_csv(\"EPA_Lexicon\")\n",
    "EPA_Lexicon = EPA_Lexicon.drop_duplicates().reset_index(drop = True)\n",
    "EPA_Lexicon[\"Lexicon\"] = EPA_Lexicon[\"Lexicon\"].str.lower()\n",
    "\n",
    "EPA_Lexicon\n",
    "\n",
    "list(EPA_Lexicon[\"Lexicon\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5e47f8",
   "metadata": {},
   "source": [
    "### BBC Lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b284bd04",
   "metadata": {},
   "source": [
    "Uitleg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea125ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the lexicon\n",
    "BBC_Lexicon = pd.read_csv(\"BBC_Lexicon\")\n",
    "BBC_Lexicon = BBC_Lexicon.drop_duplicates().reset_index(drop = True)\n",
    "BBC_Lexicon[\"Lexicon\"] = BBC_Lexicon[\"Lexicon\"].str.lower()\n",
    "\n",
    "BBC_Lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80aade8b",
   "metadata": {},
   "source": [
    "### UNDP Lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bae000f",
   "metadata": {},
   "source": [
    "Uitleg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66d51e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the lexicon\n",
    "UNDP_Lexicon = pd.read_csv(\"UNDP_Lexicon\")\n",
    "UNDP_Lexicon = UNDP_Lexicon.drop_duplicates().reset_index(drop = True)\n",
    "UNDP_Lexicon[\"Lexicon\"] = UNDP_Lexicon[\"Lexicon\"].str.lower()\n",
    "\n",
    "UNDP_Lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de5cce8",
   "metadata": {},
   "source": [
    "### Compare the lexicons to each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9894900d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an empty dataframe and write a function to fill with the values\n",
    "\n",
    "common_words_df = pd.DataFrame({\"Lexicon\" : [\"Global Change\", \"IPCC\", \"Wikipedia\", \"EPA\", \"BBC\", \"UNDP\"], \n",
    "                               \"Global Change\": [0, 0, 0, 0, 0, 0], \"IPCC\" : [0, 0, 0, 0, 0, 0], \n",
    "                               \"Wikipedia\" : [0, 0, 0, 0, 0, 0], \"EPA\" : [0, 0, 0, 0, 0, 0], \n",
    "                               \"BBC\" : [0, 0, 0, 0, 0, 0], \"UNDP\" : [0, 0, 0, 0, 0, 0]})\n",
    "\n",
    "dfs = [Global_Change_Lexicon, IPCC_Lexicon, Wikipedia_Lexicon, EPA_Lexicon, BBC_Lexicon, UNDP_Lexicon]\n",
    "\n",
    "for r in range(0, len(dfs)):\n",
    "    for c in range(0, len(dfs)):\n",
    "        # Get the common values between the two columns\n",
    "        common_words = set(dfs[r]['Lexicon']).intersection(set(dfs[c]['Lexicon']))\n",
    "        common_words_df.loc[r, common_words_df.columns[c +1]] = len(common_words)\n",
    "\n",
    "common_words_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f304b018",
   "metadata": {},
   "source": [
    "## 2.2. Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158a8097",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary(df_with_text, name, model_name, max_lenght_input=-1):\n",
    "    data_in_list = df_with_text[name].tolist()\n",
    "    tokenizer_sum = AutoTokenizer.from_pretrained(model_name)\n",
    "    model_sum = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "    summarizer = pipeline('summarization', model=model_sum, tokenizer = tokenizer_sum) \n",
    "\n",
    "    if max_lenght_input>=0:\n",
    "        df_with_text['summary'] = summarizer(data_in_list, max_length=max_lenght_input)\n",
    "\n",
    "    else:\n",
    "        df_with_text['summary'] = summarizer(data_in_list)\n",
    "\n",
    "def classification(df_with_text, name, model_name, max_lenght_input=-1):\n",
    "    data_in_list = df_with_text[name].tolist()\n",
    "    tokenizer_clas = AutoTokenizer.from_pretrained(model_name)\n",
    "    model_clas = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "    classification = pipeline('text-classification', model=model_clas, tokenizer = tokenizer_clas) \n",
    "\n",
    "    if max_lenght_input>=0:\n",
    "        df_with_text['classification'] = classification(data_in_list, max_length=max_lenght_input, truncation=True)\n",
    "\n",
    "    else:\n",
    "        df_with_text['classification'] = classification(data_in_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa8d3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#classification(Lexicon_df, 'Text',\"climatebert/environmental-claims\",512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53737e8c",
   "metadata": {},
   "source": [
    "# 3. Testen Taggers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b72304",
   "metadata": {},
   "source": [
    "## 3.1. Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "cca33293",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a separate df with the specific cleaning for the lexicons\n",
    "Lexicon_df = tag_climate_df.copy()\n",
    "Lexicon_df[\"Text\"] = Lexicon_df[\"Text\"].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bc8d45",
   "metadata": {},
   "source": [
    "### 3.1.1. One Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e0d1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global Change Lexicon\n",
    "Global_Change_df_1 = get_metrics_df_1(Lexicon_df, Global_Change_Lexicon, 0, 20, \"Global Change\")\n",
    "Global_Change_df_2 = get_metrics_df_2(Lexicon_df, Global_Change_Lexicon, 0, 2, 0.1, \"Global Change\")\n",
    "Global_Change_df_3 = get_metrics_df_3(Lexicon_df, Global_Change_Lexicon, 0, 20, \"Global Change\")\n",
    "Global_Change_df_4 = get_metrics_df_4(Lexicon_df, Global_Change_Lexicon, 0, 2, 0.1, \"Global Change\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbd51ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "Global_Change_df = pd.concat([Global_Change_df_1, Global_Change_df_2, Global_Change_df_3, Global_Change_df_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48f7730",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IPCC Lexicon\n",
    "IPCC_df_1 = get_metrics_df_1(Lexicon_df, IPCC_Lexicon, 0, 20, \"IPCC\")\n",
    "IPCC_df_2 = get_metrics_df_2(Lexicon_df, IPCC_Lexicon, 0, 2, 0.1, \"IPCC\")\n",
    "IPCC_df_3 = get_metrics_df_3(Lexicon_df, IPCC_Lexicon, 0, 20, \"IPCC\")\n",
    "IPCC_df_4 = get_metrics_df_4(Lexicon_df, IPCC_Lexicon, 0, 2, 0.1, \"IPCC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e242f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "IPCC_df = pd.concat([IPCC_df_1, IPCC_df_2, IPCC_df_3, IPCC_df_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c72d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wikipedia Lexicon\n",
    "Wikipedia_df_1 = get_metrics_df_1(Lexicon_df, Wikipedia_Lexicon, 0, 20, \"Wikipedia\")\n",
    "Wikipedia_df_2 = get_metrics_df_2(Lexicon_df, Wikipedia_Lexicon, 0, 2, 0.1, \"Wikipedia\")\n",
    "Wikipedia_df_3 = get_metrics_df_3(Lexicon_df, Wikipedia_Lexicon, 0, 20, \"Wikipedia\")\n",
    "Wikipedia_df_4 = get_metrics_df_4(Lexicon_df, Wikipedia_Lexicon, 0, 2, 0.1, \"Wikipedia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e1474f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Wikipedia_df = pd.concat([Wikipedia_df_1, Wikipedia_df_2, Wikipedia_df_3, Wikipedia_df_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9caa4baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EPA Lexicon\n",
    "EPA_df_1 = get_metrics_df_1(Lexicon_df, EPA_Lexicon, 0, 20, \"EPA\")\n",
    "EPA_df_2 = get_metrics_df_2(Lexicon_df, EPA_Lexicon, 0, 2, 0.1, \"EPA\")\n",
    "EPA_df_3 = get_metrics_df_3(Lexicon_df, EPA_Lexicon, 0, 20, \"EPA\")\n",
    "EPA_df_4 = get_metrics_df_4(Lexicon_df, EPA_Lexicon, 0, 2, 0.1, \"EPA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920067f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPA_df = pd.concat([EPA_df_1, EPA_df_2, EPA_df_3, EPA_df_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a445db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BBC Lexicon\n",
    "BBC_df_1 = get_metrics_df_1(Lexicon_df, BBC_Lexicon, 0, 20, \"BBC\")\n",
    "BBC_df_2 = get_metrics_df_2(Lexicon_df, BBC_Lexicon, 0, 2, 0.1, \"BBC\")\n",
    "BBC_df_3 = get_metrics_df_3(Lexicon_df, BBC_Lexicon, 0, 20, \"BBC\")\n",
    "BBC_df_4 = get_metrics_df_4(Lexicon_df, BBC_Lexicon, 0, 2, 0.1, \"BBC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec49a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "BBC_df = pd.concat([BBC_df_1, BBC_df_2, BBC_df_3, BBC_df_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8f351e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#UNDP Lexicon\n",
    "UNDP_df_1 = get_metrics_df_1(Lexicon_df, UNDP_Lexicon, 0, 20, \"UNDP\")\n",
    "UNDP_df_2 = get_metrics_df_2(Lexicon_df, UNDP_Lexicon, 0, 2, 0.1, \"UNDP\")\n",
    "UNDP_df_3 = get_metrics_df_3(Lexicon_df, UNDP_Lexicon, 0, 20, \"UNDP\")\n",
    "UNDP_df_4 = get_metrics_df_4(Lexicon_df, UNDP_Lexicon, 0, 2, 0.1, \"UNDP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da66cd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNDP_df = pd.concat([UNDP_df_1, UNDP_df_2, UNDP_df_3, UNDP_df_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917f4664",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get all lexicons together\n",
    "Lexicon_df_1 = pd.concat([Global_Change_df, IPCC_df, Wikipedia_df, EPA_df, BBC_df, UNDP_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea244ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lexicon_df_1.sort_values(\"F1 Score\", ascending = False).reset_index(drop = True).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a975060f",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_metrics_2(Lexicon_df, EPA_Lexicon, 0.6, 0.6, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a074a4f",
   "metadata": {},
   "source": [
    "### 3.1.2. Two Lexicons Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ae84f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#UNDP and EPA\n",
    "EPA_UNDP_Lexicon = pd.concat([EPA_Lexicon, UNDP_Lexicon]).drop_duplicates().reset_index(drop = True)\n",
    "EPA_UNDP_df_1 = get_metrics_df_1(Lexicon_df, EPA_UNDP_Lexicon, 0, 20, \"EPA_UDNP\")\n",
    "EPA_UNDP_df_2 = get_metrics_df_2(Lexicon_df, EPA_UNDP_Lexicon, 0, 2, 0.1, \"EPA_UDNP\")\n",
    "EPA_UNDP_df_3 = get_metrics_df_3(Lexicon_df, EPA_UNDP_Lexicon, 0, 20, \"EPA_UDNP\")\n",
    "EPA_UNDP_df_4 = get_metrics_df_4(Lexicon_df, EPA_UNDP_Lexicon, 0, 2, 0.1, \"EPA_UDNP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf1520b",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPA_UNDP_df = pd.concat([EPA_UNDP_df_1, EPA_UNDP_df_2, EPA_UNDP_df_3, EPA_UNDP_df_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418d74b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#UNDP and BBC\n",
    "BBC_UNDP_Lexicon = pd.concat([BBC_Lexicon, UNDP_Lexicon]).drop_duplicates().reset_index(drop = True)\n",
    "BBC_UNDP_df_1 = get_metrics_df_1(Lexicon_df, BBC_UNDP_Lexicon, 0, 20, \"BBC_UNDP\")\n",
    "BBC_UNDP_df_2 = get_metrics_df_2(Lexicon_df, BBC_UNDP_Lexicon, 0, 2, 0.1, \"BBC_UNDP\")\n",
    "BBC_UNDP_df_3 = get_metrics_df_3(Lexicon_df, BBC_UNDP_Lexicon, 0, 20, \"BBC_UNDP\")\n",
    "BBC_UNDP_df_4 = get_metrics_df_4(Lexicon_df, BBC_UNDP_Lexicon, 0, 2, 0.1, \"BBC_UNDP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56595876",
   "metadata": {},
   "outputs": [],
   "source": [
    "BBC_UNDP_df = pd.concat([BBC_UNDP_df_1, BBC_UNDP_df_2, BBC_UNDP_df_3, BBC_UNDP_df_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dc494f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#UNDP and Global Change \n",
    "UNDP_Global_Change_Lexicon = pd.concat([Global_Change_Lexicon, UNDP_Lexicon]).drop_duplicates().reset_index(drop = True)\n",
    "UNDP_Global_Change_df_1 = get_metrics_df_1(Lexicon_df, UNDP_Global_Change_Lexicon, 0, 20, \"UNDP_Global_Change\")\n",
    "UNDP_Global_Change_df_2 = get_metrics_df_2(Lexicon_df, UNDP_Global_Change_Lexicon, 0, 2, 0.1, \"UNDP_Global_Change\")\n",
    "UNDP_Global_Change_df_3 = get_metrics_df_3(Lexicon_df, UNDP_Global_Change_Lexicon, 0, 20, \"UNDP_Global_Change\")\n",
    "UNDP_Global_Change_df_4 = get_metrics_df_4(Lexicon_df, UNDP_Global_Change_Lexicon, 0, 2, 0.1, \"UNDP_Global_Change\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cbdbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "Global_Change_UNDP_df = pd.concat([UNDP_Global_Change_df_1, UNDP_Global_Change_df_2, UNDP_Global_Change_df_3, UNDP_Global_Change_df_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984b718c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Global_Change_UNDP_df.sort_values(\"Accuracy\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7def3b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#UNDP and IPCC\n",
    "UNDP_IPCC_Lexicon = pd.concat([IPCC_Lexicon, UNDP_Lexicon]).drop_duplicates().reset_index(drop = True)\n",
    "UNDP_IPCC_df_1 = get_metrics_df_1(Lexicon_df, UNDP_IPCC_Lexicon, 0, 20, \"UNDP_IPCC\")\n",
    "UNDP_IPCC_df_2 = get_metrics_df_2(Lexicon_df, UNDP_IPCC_Lexicon, 0, 2, 0.1, \"UNDP_IPCC\")\n",
    "UNDP_IPCC_df_3 = get_metrics_df_3(Lexicon_df, UNDP_IPCC_Lexicon, 0, 20, \"UNDP_IPCC\")\n",
    "UNDP_IPCC_df_4 = get_metrics_df_4(Lexicon_df, UNDP_IPCC_Lexicon, 0, 2, 0.1, \"UNDP_IPCC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c4f888",
   "metadata": {},
   "outputs": [],
   "source": [
    "IPCC_UNDP_df = pd.concat([UNDP_IPCC_df_1, UNDP_IPCC_df_2, UNDP_IPCC_df_3, UNDP_IPCC_df_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881f064f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#UNDP and Wikipedia\n",
    "UNDP_Wikipedia_Lexicon = pd.concat([Wikipedia_Lexicon, UNDP_Lexicon]).drop_duplicates().reset_index(drop = True)\n",
    "UNDP_Wikipedia_df_1 = get_metrics_df_1(Lexicon_df, UNDP_Wikipedia_Lexicon, 0, 20, \"UNDP_Wikipedia\")\n",
    "UNDP_Wikipedia_df_2 = get_metrics_df_2(Lexicon_df, UNDP_Wikipedia_Lexicon, 0, 2, 0.1, \"UNDP_Wikipedia\")\n",
    "UNDP_Wikipedia_df_3 = get_metrics_df_3(Lexicon_df, UNDP_Wikipedia_Lexicon, 0, 20, \"UNDP_Wikipedia\")\n",
    "UNDP_Wikipedia_df_4 = get_metrics_df_4(Lexicon_df, UNDP_Wikipedia_Lexicon, 0, 2, 0.1, \"UNDP_Wikipedia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d486bfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Wikipedia_UNDP_df = pd.concat([UNDP_Wikipedia_df_1, UNDP_Wikipedia_df_2, UNDP_Wikipedia_df_3, UNDP_Wikipedia_df_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9373038f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Wikipedia_UNDP_df.sort_values(\"Accuracy\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29be6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lexicon_df_2 = pd.concat([EPA_UNDP_df, BBC_UNDP_df, Global_Change_UNDP_df, IPCC_UNDP_df, Wikipedia_UNDP_df]).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df23872",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lexicon_df_2.sort_values(\"Accuracy\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637fa27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EPA and BBC\n",
    "EPA_BBC_Lexicon = pd.concat([EPA_Lexicon, BBC_Lexicon]).drop_duplicates().reset_index(drop = True)\n",
    "EPA_BBC_df_1 = get_metrics_df_1(Lexicon_df, EPA_BBC_Lexicon, 0, 20, \"BBC_EPA\")\n",
    "EPA_BBC_df_2 = get_metrics_df_2(Lexicon_df, EPA_BBC_Lexicon, 0, 2, 0.1, \"BBC_EPA\")\n",
    "EPA_BBC_df_3 = get_metrics_df_3(Lexicon_df, EPA_BBC_Lexicon, 0, 20, \"BBC_EPA\")\n",
    "EPA_BBC_df_4 = get_metrics_df_4(Lexicon_df, EPA_BBC_Lexicon, 0, 2, 0.1, \"BBC_EPA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5766b98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPA_BBC_df = pd.concat([EPA_BBC_df_1, EPA_BBC_df_2, EPA_BBC_df_3, EPA_BBC_df_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba89991",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPA_BBC_df.sort_values(\"Accuracy\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101535ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EPA and Global Change\n",
    "EPA_Global_Change_Lexicon = pd.concat([EPA_Lexicon, Global_Change_Lexicon]).drop_duplicates().reset_index(drop = True)\n",
    "EPA_Global_Change_df_1 = get_metrics_df_1(Lexicon_df, EPA_Global_Change_Lexicon, 0, 20, \"EPA_GLobal_Change\")\n",
    "EPA_Global_Change_df_2 = get_metrics_df_2(Lexicon_df, EPA_Global_Change_Lexicon, 0, 2, 0.1, \"BBC_Global_Change\")\n",
    "EPA_Global_Change_df_3 = get_metrics_df_3(Lexicon_df, EPA_Global_Change_Lexicon, 0, 20, \"BBC_Global_Change\")\n",
    "EPA_Global_Change_df_4 = get_metrics_df_4(Lexicon_df, EPA_Global_Change_Lexicon, 0, 2, 0.1, \"BBC_Global_Change\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d585045",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPA_Global_Change_df = pd.concat([EPA_Global_Change_df_1, EPA_Global_Change_df_2, EPA_Global_Change_df_3, EPA_Global_Change_df_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4717a31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPA_Global_Change_df.sort_values(\"Accuracy\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab41474e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EPA and IPCC\n",
    "EPA_IPCC_Lexicon = pd.concat([EPA_Lexicon, IPCC_Lexicon]).drop_duplicates().reset_index(drop = True)\n",
    "EPA_IPCC_df_1 = get_metrics_df_1(Lexicon_df, EPA_IPCC_Lexicon, 0, 20, \"EPA_IPCC\")\n",
    "EPA_IPCC_df_2 = get_metrics_df_2(Lexicon_df, EPA_IPCC_Lexicon, 0, 2, 0.1, \"EPA_IPCC\")\n",
    "EPA_IPCC_df_3 = get_metrics_df_3(Lexicon_df, EPA_IPCC_Lexicon, 0, 20, \"EPA_IPCC\")\n",
    "EPA_IPCC_df_4 = get_metrics_df_4(Lexicon_df, EPA_IPCC_Lexicon, 0, 2, 0.1, \"EPA_IPCC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e6d305",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPA_IPCC_df = pd.concat([EPA_IPCC_df_1, EPA_IPCC_df_2, EPA_IPCC_df_3, EPA_IPCC_df_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2431357",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPA_IPCC_df.sort_values(\"Accuracy\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129ced17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EPA and Wikipedia\n",
    "EPA_Wikipedia_Lexicon = pd.concat([EPA_Lexicon, Wikipedia_Lexicon]).drop_duplicates().reset_index(drop = True)\n",
    "EPA_Wikipedia_df_1 = get_metrics_df_1(Lexicon_df, EPA_Wikipedia_Lexicon, 0, 20, \"EPA_Wikipedia\")\n",
    "EPA_Wikipedia_df_2 = get_metrics_df_2(Lexicon_df, EPA_Wikipedia_Lexicon, 0, 2, 0.1, \"EPA_Wikipedia\")\n",
    "EPA_Wikipedia_df_3 = get_metrics_df_3(Lexicon_df, EPA_Wikipedia_Lexicon, 0, 20, \"EPA_Wikipedia\")\n",
    "EPA_Wikipedia_df_4 = get_metrics_df_4(Lexicon_df, EPA_Wikipedia_Lexicon, 0, 2, 0.1, \"EPA_Wikipedia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c1e917",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPA_Wikipedia_df = pd.concat([EPA_Wikipedia_df_1, EPA_Wikipedia_df_2, EPA_Wikipedia_df_3, EPA_Wikipedia_df_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9e53d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPA_Wikipedia_df.sort_values(\"Accuracy\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd66768",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lexicon_df_2 = pd.concat([Lexicon_df_2, EPA_BBC_df, EPA_Global_Change_df, EPA_IPCC_df, EPA_Wikipedia_df]).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af7a07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lexicon_df_2.sort_values(\"Accuracy\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1260c559",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BBC and Global Change\n",
    "BBC_Global_Change_Lexicon = pd.concat([BBC_Lexicon, Global_Change_Lexicon]).drop_duplicates().reset_index(drop = True)\n",
    "BBC_Global_Change_df_1 = get_metrics_df_1(Lexicon_df, BBC_Global_Change_Lexicon, 0, 20, \"BBC_Global_Change\")\n",
    "BBC_Global_Change_df_2 = get_metrics_df_2(Lexicon_df, BBC_Global_Change_Lexicon, 0, 2, 0.1, \"BBC_Global_Change\")\n",
    "BBC_Global_Change_df_3 = get_metrics_df_3(Lexicon_df, BBC_Global_Change_Lexicon, 0, 20, \"BBC_Global_Change\")\n",
    "BBC_Global_Change_df_4 = get_metrics_df_4(Lexicon_df, BBC_Global_Change_Lexicon, 0, 2, 0.1, \"BBC_Global_Change\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2024e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "BBC_Global_Change_df = pd.concat([BBC_Global_Change_df_1, BBC_Global_Change_df_2, BBC_Global_Change_df_3, BBC_Global_Change_df_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a152cd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BBC and IPCC\n",
    "BBC_IPCC_Lexicon = pd.concat([BBC_Lexicon, IPCC_Lexicon]).drop_duplicates().reset_index(drop = True)\n",
    "BBC_IPCC_df_1 = get_metrics_df_1(Lexicon_df, BBC_IPCC_Lexicon, 0, 20, \"BBC_IPCC\")\n",
    "BBC_IPCC_df_2 = get_metrics_df_2(Lexicon_df, BBC_IPCC_Lexicon, 0, 2, 0.1, \"BBC_IPCC\")\n",
    "BBC_IPCC_df_3 = get_metrics_df_3(Lexicon_df, BBC_IPCC_Lexicon, 0, 20, \"BBC_IPCC\")\n",
    "BBC_IPCC_df_4 = get_metrics_df_4(Lexicon_df, BBC_IPCC_Lexicon, 0, 2, 0.1, \"BBC_IPCC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f41eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "BBC_IPCC_df = pd.concat([BBC_IPCC_df_1, BBC_IPCC_df_2, BBC_IPCC_df_3, BBC_IPCC_df_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9bd7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BBC and Wikipedia\n",
    "BBC_Wikipedia_Lexicon = pd.concat([BBC_Lexicon, Wikipedia_Lexicon]).drop_duplicates().reset_index(drop = True)\n",
    "BBC_Wikipedia_df_1 = get_metrics_df_1(Lexicon_df, BBC_Wikipedia_Lexicon, 0, 20, \"BBC_Wikipedia\")\n",
    "BBC_Wikipedia_df_2 = get_metrics_df_2(Lexicon_df, BBC_Wikipedia_Lexicon, 0, 2, 0.1, \"BBC_Wikipedia\")\n",
    "BBC_Wikipedia_df_3 = get_metrics_df_3(Lexicon_df, BBC_Wikipedia_Lexicon, 0, 20, \"BBC_Wikipedia\")\n",
    "BBC_Wikipedia_df_4 = get_metrics_df_4(Lexicon_df, BBC_Wikipedia_Lexicon, 0, 2, 0.1, \"BBC_Wikipedia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf1b899",
   "metadata": {},
   "outputs": [],
   "source": [
    "BBC_Wikpedia_df = pd.concat([BBC_Wikipedia_df_1, BBC_Wikipedia_df_2, BBC_Wikipedia_df_3, BBC_Wikipedia_df_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f6aa7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lexicon_df_2 = pd.concat([Lexicon_df_2, BBC_Global_Change_df, BBC_IPCC_df, BBC_Wikpedia_df]).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477a603c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wikipedia and Global Change\n",
    "Wikipedia_Global_Change_Lexicon = pd.concat([Global_Change_Lexicon, Wikipedia_Lexicon]).drop_duplicates().reset_index(drop = True)\n",
    "Wikipedia_Global_Change_df_1 = get_metrics_df_1(Lexicon_df, Wikipedia_Global_Change_Lexicon, 0, 20, \"Wikipedia_Global_Change\")\n",
    "Wikipedia_Global_Change_df_2 = get_metrics_df_2(Lexicon_df, Wikipedia_Global_Change_Lexicon, 0, 2, 0.1, \"Wikipedia_Global_Change\")\n",
    "Wikipedia_Global_Change_df_3 = get_metrics_df_3(Lexicon_df, Wikipedia_Global_Change_Lexicon, 0, 20, \"Wikipedia_Global_Change\")\n",
    "Wikipedia_Global_Change_df_4 = get_metrics_df_4(Lexicon_df, Wikipedia_Global_Change_Lexicon, 0, 2, 0.1, \"Wikipedia_Global_Change\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5ea019",
   "metadata": {},
   "outputs": [],
   "source": [
    "Wikpedia_Global_Change_df = pd.concat([Wikipedia_Global_Change_df_1, Wikipedia_Global_Change_df_2, Wikipedia_Global_Change_df_3, Wikipedia_Global_Change_df_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2287a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wikipedia and IPCC\n",
    "Wikipedia_IPCC_Lexicon = pd.concat([IPCC_Lexicon, Wikipedia_Lexicon]).drop_duplicates().reset_index(drop = True)\n",
    "Wikipedia_IPCC_df_1 = get_metrics_df_1(Lexicon_df, Wikipedia_IPCC_Lexicon, 0, 20, \"Wikipedia_IPCC\")\n",
    "Wikipedia_IPCC_df_2 = get_metrics_df_2(Lexicon_df, Wikipedia_IPCC_Lexicon, 0, 2, 0.1, \"Wikipedia_IPCC\")\n",
    "Wikipedia_IPCC_df_3 = get_metrics_df_3(Lexicon_df, Wikipedia_IPCC_Lexicon, 0, 20, \"Wikipedia_IPCC\")\n",
    "Wikipedia_IPCC_df_4 = get_metrics_df_4(Lexicon_df, Wikipedia_IPCC_Lexicon, 0, 2, 0.1, \"Wikipedia_IPCC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e11a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "Wikpedia_IPCC_df = pd.concat([Wikipedia_IPCC_df_1, Wikipedia_IPCC_df_2, Wikipedia_IPCC_df_3, Wikipedia_IPCC_df_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0198c9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IPCC and Global Change\n",
    "Global_Change_IPCC_Lexicon = pd.concat([IPCC_Lexicon, Global_Change_Lexicon]).drop_duplicates().reset_index(drop = True)\n",
    "Global_Change_IPCC_df_1 = get_metrics_df_1(Lexicon_df, Global_Change_IPCC_Lexicon, 0, 20, \"Global_Change_IPCC\")\n",
    "Global_Change_IPCC_df_2 = get_metrics_df_2(Lexicon_df, Global_Change_IPCC_Lexicon, 0, 2, 0.1, \"Global_Change_IPCC\")\n",
    "Global_Change_IPCC_df_3 = get_metrics_df_3(Lexicon_df, Global_Change_IPCC_Lexicon, 0, 20, \"Global_Change_IPCC\")\n",
    "Global_Change_IPCC_df_4 = get_metrics_df_4(Lexicon_df, Global_Change_IPCC_Lexicon, 0, 2, 0.1, \"Global_Change_IPCC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a0fd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "Global_Change_IPCC_df = pd.concat([Global_Change_IPCC_df_1, Global_Change_IPCC_df_2, Global_Change_IPCC_df_3, Global_Change_IPCC_df_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157d3ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lexicon_df_2 = pd.concat([Lexicon_df_2, Wikpedia_Global_Change_df, Wikpedia_IPCC_df, Global_Change_IPCC_df]).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d5c0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lexicon_df_2.sort_values(\"Accuracy\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "00b24048",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lexicon_Metrics = pd.concat([Lexicon_df_1, Lexicon_df_2]).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "38fe9429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lexicon</th>\n",
       "      <th>Technique</th>\n",
       "      <th>Treshhold</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>BBC_UNDP</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.823333</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>1.811321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>936</th>\n",
       "      <td>BBC_EPA</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.823333</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>1.811321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>EPA_UDNP</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.820000</td>\n",
       "      <td>47.0</td>\n",
       "      <td>0.886792</td>\n",
       "      <td>1.740741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>EPA</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.816667</td>\n",
       "      <td>46.0</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>1.672727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>EPA_UDNP</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.813333</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>1.607143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>UNDP</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>UNDP</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.726667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.219512</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>UNDP</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>UNDP</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.743333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.298701</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>EPA_UDNP</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.763333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.408451</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1764 rows  7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Lexicon           Technique  Treshhold  Accuracy  Precision    Recall  \\\n",
       "597  BBC_UNDP  Absolute Frequency        9.0  0.823333       48.0  0.923077   \n",
       "936   BBC_EPA  Absolute Frequency       12.0  0.823333       48.0  0.923077   \n",
       "516  EPA_UDNP  Absolute Frequency       12.0  0.820000       47.0  0.886792   \n",
       "264       EPA  Absolute Frequency       12.0  0.816667       46.0  0.851852   \n",
       "518  EPA_UDNP  Absolute Frequency       14.0  0.813333       45.0  0.818182   \n",
       "..        ...                 ...        ...       ...        ...       ...   \n",
       "438      UNDP  Absolute Frequency       18.0  0.720000        0.0  0.190476   \n",
       "437      UNDP  Absolute Frequency       17.0  0.726667        0.0  0.219512   \n",
       "436      UNDP  Absolute Frequency       16.0  0.733333        0.0  0.250000   \n",
       "435      UNDP  Absolute Frequency       15.0  0.743333        0.0  0.298701   \n",
       "524  EPA_UDNP  Absolute Frequency       20.0  0.763333        0.0  0.408451   \n",
       "\n",
       "     F1 Score  \n",
       "597  1.811321  \n",
       "936  1.811321  \n",
       "516  1.740741  \n",
       "264  1.672727  \n",
       "518  1.607143  \n",
       "..        ...  \n",
       "438  0.000000  \n",
       "437  0.000000  \n",
       "436  0.000000  \n",
       "435  0.000000  \n",
       "524  0.000000  \n",
       "\n",
       "[1764 rows x 7 columns]"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Lexicon_Metrics.sort_values(by=\"Precision\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "c80cb04f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshhold: 12\n",
      "Accuracy: 0.7866666666666666\n",
      "Precision: 0\n",
      "Recall: 0.5625\n",
      "F1 score: 0.0\n",
      "Target Lexicon   No  Yes  All\n",
      "Target                       \n",
      "No              200    0  200\n",
      "Yes              64   36  100\n",
      "All             264   36  300\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "threshold_metrics_1(Lexicon_df, BBC_UNDP_Lexicon, 12, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92383277",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
