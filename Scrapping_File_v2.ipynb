{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "285c4d9c",
   "metadata": {},
   "source": [
    "# 0. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d39ea7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "from datetime import date, timedelta\n",
    "import random\n",
    "import threading\n",
    "import numpy\n",
    "import multiprocessing as np\n",
    "from multiprocessing import Pool\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91c53ad",
   "metadata": {},
   "source": [
    "# 1. Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e0c218",
   "metadata": {},
   "source": [
    "## 1.1. Loop Through Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1770ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def daterange(start_date, end_date):\n",
    "    for n in range(int((end_date - start_date).days) + 1):\n",
    "        yield start_date + timedelta(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcdb837a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver = webdriver.Chrome()\n",
    "# driver.get(\"https://www.politico.com/search/20?adv=true&start=02/01/2016&end=02/01/2016\")\n",
    "# #print(driver.find_element(By.CSS_SELECTOR, \"a.button\").text)\n",
    "# print(driver.find_element(By.CSS_SELECTOR, \"a.button:nth-child(2)\").text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cd9278",
   "metadata": {},
   "source": [
    "# 2. Scraping News Media"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb1b56a",
   "metadata": {},
   "source": [
    "## 2.1. Wall Street Journal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746f9a4c",
   "metadata": {},
   "source": [
    "### 2.1.1. Function - Get URLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b27213ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWallStreetJournalURLS(start_date, end_date, username, password):\n",
    "    #put dates in date format\n",
    "    start_date = datetime.strptime(start_date, \"%d/%m/%Y\")\n",
    "    end_date = datetime.strptime(end_date, \"%d/%m/%Y\")\n",
    "\n",
    "    #Define list of variables to store \n",
    "    WSJ_date = []\n",
    "    WSJ_link = []\n",
    "\n",
    "    #Go to the website\n",
    "    fireFoxOptions = webdriver.FirefoxOptions()\n",
    "    fireFoxOptions.add_argument('--ignore-certificate-errors')\n",
    "    fireFoxOptions.add_argument('--allow-running-insecure-content')\n",
    "    fireFoxOptions.add_argument(\"--headless\")\n",
    "    fireFoxOptions.add_argument(\"--disable-gpu\")\n",
    "    fireFoxOptions.add_argument(\"--no-sandbox\")\n",
    "    user_agent = 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.50 Safari/537.36'\n",
    "    fireFoxOptions.add_argument(f'user-agent={user_agent}')\n",
    "    driver_WSJ = webdriver.Firefox(options=fireFoxOptions)\n",
    "    #driver_WSJ = webdriver.Firefox()\n",
    "    driver_WSJ.get(\"https://sso.accounts.dowjones.com/login-page?op=localop&scope=openid%20idp_id%20roles%20email%20given_name%20family_name%20djid%20djUsername%20djStatus%20trackid%20tags%20prts%20suuid%20createTimestamp&client_id=5hssEAdMy0mJTICnJNvC9TXEw3Va7jfO&response_type=code&redirect_uri=https%3A%2F%2Faccounts.wsj.com%2Fauth%2Fsso%2Flogin&nonce=ab6a473a-cfa6-4714-8fad-b6dff98f5f18&ui_locales=en-us-x-wsj-223-2&mars=-1&ns=prod%2Faccounts-wsj&state=8rChOTDzC_Y_AK-i.TJAixN_XjsWxwUEEPoHg2OPCaX6qRBu4nGSk5fqLliY4H0B5F7gj_57-XH-YBWGS&protocol=oauth2&client=5hssEAdMy0mJTICnJNvC9TXEw3Va7jfO#!/signin\")\n",
    "\n",
    "    #put in user_name:\n",
    "    time.sleep(3) #makes sure field is fully loaded\n",
    "    username_field = driver_WSJ.find_element(By.XPATH, \"/html/body/div[1]/div[2]/div/div/div/div[1]/div[1]/form/div[2]/div[1]/div[2]/input\")\n",
    "    username_field.send_keys(username)\n",
    "\n",
    "    #continue to password:\n",
    "    driver_WSJ.find_element(By.XPATH, \"/html/body/div[1]/div[2]/div/div/div/div[1]/div[1]/form/div[2]/div[6]/div[1]/button[2]\").click()\n",
    "\n",
    "    #input password:\n",
    "    time.sleep(3) #makes sure field is fully loaded\n",
    "    password_field = driver_WSJ.find_element(By.XPATH, \"//*[@id='password-login-password']\")\n",
    "    password_field.send_keys(password)\n",
    "\n",
    "    #click on sign in:\n",
    "    driver_WSJ.find_element(By.XPATH, \"//*[@id='password-login']/div/form/div/div[5]/div[1]/button\").click()\n",
    "\n",
    "    try:\n",
    "        #accept cookies\n",
    "        time.sleep(10)\n",
    "        driver_WSJ.switch_to.frame(WebDriverWait(driver_WSJ,30).until(EC.presence_of_element_located((By.ID, 'sp_message_iframe_718122'))))\n",
    "        driver_WSJ.find_element(By.CSS_SELECTOR, \"button.message-component:nth-child(2)\").click()\n",
    "        driver_WSJ.switch_to.default_content() \n",
    "        time.sleep(3)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    scrape_time = time.time() #starting the timing\n",
    "\n",
    "    for single_date in daterange(start_date, end_date):\n",
    "        date = single_date.strftime(\"%d/%m/%Y\")\n",
    "        print(date)\n",
    "        year = \"%02d\" % (int(date.split('/')[2]),)\n",
    "        month = \"%02d\" % (int(date.split('/')[1]),)\n",
    "        day = \"%02d\" % (int(date.split('/')[0]),)\n",
    "        page = \"%02d\" % (1,) #we assume there is always at least one page\n",
    "        url = f\"https://www.wsj.com/news/archive/{year}/{month}/{day}?page={page}\"\n",
    "        test = 0\n",
    "        while(test < 100):\n",
    "            try:\n",
    "                driver_WSJ.get(url) #go to the page in the WSJ archive for the given date\n",
    "                break\n",
    "            except:\n",
    "                test += 1\n",
    "                time.sleep(10)\n",
    "\n",
    "        #check if there are multiple pages and if so, visit these as well\n",
    "        pages_load = 0\n",
    "        while(pages_load < 100):\n",
    "            try:\n",
    "                pages = WebDriverWait(driver_WSJ, 10).until(EC.presence_of_element_located((By.CLASS_NAME, \"WSJTheme--pagepicker-total--Kl350I1l \")))\n",
    "                page_total = re.findall(r'\\d+', pages.text)\n",
    "                nr_pages = int(page_total[0])\n",
    "                break\n",
    "            except:\n",
    "                pages_load += 1\n",
    "                driver_WSJ.get(url)\n",
    "\n",
    "        for p in range(1,nr_pages+1):\n",
    "            page = \"%02d\" % (p,)\n",
    "            url = f\"https://www.wsj.com/news/archive/{year}/{month}/{day}?page={page}\"\n",
    "            urls_load = 0\n",
    "            while(urls_load < 100):\n",
    "                try:\n",
    "                    driver_WSJ.get(url) #go to the page in the WSJ archive for the given date\n",
    "                    break\n",
    "                except:\n",
    "                    driver_WSJ.get(url)\n",
    "\n",
    "            #get the article urls for the articles on the other pages, if there are more than 1\n",
    "            articles = WebDriverWait(driver_WSJ,10).until(EC.presence_of_all_elements_located((By.TAG_NAME, \"article\")))\n",
    "            for a in range(0,len(articles)):\n",
    "                WSJ_link.append(articles[a].find_element(By.TAG_NAME, \"a\").get_attribute('href'))\n",
    "                WSJ_date.append(date)\n",
    "    \n",
    "    driver_WSJ.quit()\n",
    "    \n",
    "    WSJ = [\"Wall_Street_Journal\"] * len(WSJ_link)\n",
    "    data = {\"Date\" : WSJ_date, \"News Paper\" : WSJ, \"Link\" : WSJ_link}\n",
    "    Wall_Street_Journal = pd.DataFrame(data)\n",
    "\n",
    "    start_date = start_date.strftime(\"%d%m%Y\")\n",
    "    end_date = end_date.strftime(\"%d%m%Y\")\n",
    "    path = f\"Data/URLS/Wall_Street_Journal_{start_date}_{end_date}_URLS\"\n",
    "    Wall_Street_Journal.to_parquet(path) \n",
    "    \n",
    "    print(\"--- %s seconds ---\" % (time.time() - scrape_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217b3365",
   "metadata": {},
   "source": [
    "### 2.1.2. Function - Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10f6db76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWallStreetJournalArticles(username, password, dataframe):\n",
    "    WSJ_article_content = []\n",
    "    WSJ_title = []\n",
    "\n",
    "    #FIREFOX\n",
    "    #Declare the driver and go to website\n",
    "    #fireFoxOptions = webdriver.FirefoxOptions()\n",
    "    #fireFoxOptions.add_argument('--ignore-certificate-errors')\n",
    "    #fireFoxOptions.add_argument('--allow-running-insecure-content')\n",
    "    #fireFoxOptions.add_argument(\"--headless\")\n",
    "    #fireFoxOptions.add_argument(\"--disable-gpu\")\n",
    "    #fireFoxOptions.add_argument(\"--no-sandbox\")\n",
    "    #user_agent = 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.50 Safari/537.36'\n",
    "    #fireFoxOptions.add_argument(f'user-agent={user_agent}')\n",
    "    #driver_WSJ = webdriver.Firefox(options=fireFoxOptions)\n",
    "    \n",
    "    #CHROME\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--disable-extensions\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\") # linux only\n",
    "    #chrome_options.add_argument(\"--headless\")\n",
    "    user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.190 Safari/537.36\"\n",
    "    chrome_options.add_argument(f'user-agent={user_agent}')\n",
    "    driver_WSJ = webdriver.Chrome(options=chrome_options)\n",
    "   # driver_WSJ = webdriver.Chrome()\n",
    "    driver_WSJ.get(\"https://sso.accounts.dowjones.com/login-page?op=localop&scope=openid%20idp_id%20roles%20email%20given_name%20family_name%20djid%20djUsername%20djStatus%20trackid%20tags%20prts%20suuid%20updated_at&client_id=5hssEAdMy0mJTICnJNvC9TXEw3Va7jfO&response_type=code&redirect_uri=https%3A%2F%2Faccounts.wsj.com%2Fauth%2Fsso%2Flogin&nonce=5e93fe71-314a-44b9-bb2d-4a0e5a3167a4&ui_locales=en-us-x-wsj-223-2&mars=-1&ns=prod%2Faccounts-wsj&state=zJgHWictetlLHLG6.Uob1DvxnFA4kIkiqJAY80zp2N4FxS3WdBvV-VuTFdpM&protocol=oauth2&client=5hssEAdMy0mJTICnJNvC9TXEw3Va7jfO#!/signin\")\n",
    "\n",
    "    #put in user_name:\n",
    "    time.sleep(3) #makes sure field is fully loaded\n",
    "    username_field = driver_WSJ.find_element(By.XPATH, \"/html/body/div[1]/div[2]/div/div/div/div[1]/div[1]/form/div[2]/div[1]/div[2]/input\")\n",
    "    username_field.send_keys(username)\n",
    "\n",
    "    #continue to password:\n",
    "    driver_WSJ.find_element(By.XPATH, \"/html/body/div[1]/div[2]/div/div/div/div[1]/div[1]/form/div[2]/div[6]/div[1]/button[2]\").click()\n",
    "\n",
    "    #input password:\n",
    "    time.sleep(3) #makes sure field is fully loaded\n",
    "    password_field = driver_WSJ.find_element(By.XPATH, \"//*[@id='password-login-password']\")\n",
    "    password_field.send_keys(password)\n",
    "\n",
    "    #click on sign in:\n",
    "    driver_WSJ.find_element(By.XPATH, \"//*[@id='password-login']/div/form/div/div[5]/div[1]/button\").click()\n",
    "\n",
    "    try:\n",
    "        #accept cookies\n",
    "        time.sleep(20)\n",
    "        driver_WSJ.switch_to.default_content()\n",
    "        driver_WSJ.switch_to.frame(WebDriverWait(driver_WSJ,30).until(EC.presence_of_element_located((By.ID, 'sp_message_iframe_718122'))))\n",
    "        driver_WSJ.find_element(By.CSS_SELECTOR, \"button.message-component:nth-child(2)\").click()\n",
    "        print(\"accepted\")\n",
    "        time.sleep(3)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    scrape_time = time.time()\n",
    "    #go to all the articles and scrape the content\n",
    "    links  = list(dataframe[\"Link\"])\n",
    "    for u in range(0, len(links)):\n",
    "        trys = 0\n",
    "        while(trys < 100):\n",
    "            try:\n",
    "                driver_WSJ.get(links[u])\n",
    "                break\n",
    "            except:\n",
    "                trys += 1\n",
    "                time.sleep(10)\n",
    "\n",
    "        try:\n",
    "            #extract the content and add to a variables\n",
    "            content = WebDriverWait(driver_WSJ, 10).until(EC.presence_of_element_located((By.CLASS_NAME, 'article-content  ')))\n",
    "            text = content.find_elements(By.TAG_NAME, 'p')\n",
    "            article_text = \"\"\n",
    "            for t in range(0,len(text)):\n",
    "                article_text += \" \" + text[t].text\n",
    "\n",
    "            WSJ_article_content.append(article_text) #add text to the list\n",
    "            #WSJ_article_content.append(content.text) #add text to the list\n",
    "\n",
    "        except: #we don't have access to the article\n",
    "            WSJ_article_content.append(\"NO ACCESS\")\n",
    "            pass #go back to page with all articles\n",
    "\n",
    "        try:\n",
    "            #collect the title as well\n",
    "            title = WebDriverWait(driver_WSJ,10).until(EC.presence_of_element_located((By.CLASS_NAME, \"wsj-article-headline\"))).text\n",
    "            WSJ_title.append(title)\n",
    "        except:\n",
    "            try:\n",
    "                #there are 2 main formats in which the titles are present in the HTML\n",
    "                title = WebDriverWait(driver_WSJ,10).until(EC.presence_of_element_located((By.CLASS_NAME, \"bigTop__hed\"))).text\n",
    "                WSJ_title.append(title)\n",
    "\n",
    "            except:\n",
    "                #add error if there is some unexpected layout -> this way our arrays will be of the same length and we will be able\n",
    "                #to construct a dataframe in the end\n",
    "                WSJ_title.append(\"ERROR\")\n",
    "                pass\n",
    "\n",
    "    driver_WSJ.quit()\n",
    "\n",
    "    WSJ = [\"Wall_Street_Journal\"] * len(WSJ_article_content)\n",
    "    data = {\"Title\" : WSJ_title, \"Text\" : WSJ_article_content, \"Link\" : dataframe[\"Link\"]}\n",
    "    Wall_Street_Journal = pd.DataFrame(data)\n",
    "    Wall_Street_Journal = pd.merge(Wall_Street_Journal, dataframe, on = \"Link\")\n",
    "    return(Wall_Street_Journal)\n",
    "\n",
    "    print(\"--- %s seconds ---\" % (time.time() - scrape_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2cf9e1",
   "metadata": {},
   "source": [
    "## 2.2. Washington Post"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a6082c",
   "metadata": {},
   "source": [
    "### 2.2.1. Function - Get URLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a32e3729",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWashingtonPostURLS(start_date, end_date, username, password):\n",
    "    #put dates in date format\n",
    "    start_date = datetime.strptime(start_date, \"%d/%m/%Y\")   \n",
    "    end_date = datetime.strptime(end_date, \"%d/%m/%Y\")\n",
    "\n",
    "\n",
    "    #put them in correct format\n",
    "\n",
    "\n",
    "    #Declare all list variables for the output\n",
    "    WP_date = []\n",
    "    WP_link = []\n",
    "    \n",
    "    #FIREFOX\n",
    "    #Declare the driver and go to website\n",
    "    #fireFoxOptions = webdriver.FirefoxOptions()\n",
    "    #fireFoxOptions.add_argument('--ignore-certificate-errors')\n",
    "    #fireFoxOptions.add_argument('--allow-running-insecure-content')\n",
    "    #fireFoxOptions.add_argument(\"--headless\")\n",
    "    #fireFoxOptions.add_argument(\"--disable-gpu\")\n",
    "    #fireFoxOptions.add_argument(\"--no-sandbox\")\n",
    "    #user_agent = 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.50 Safari/537.36'\n",
    "    #fireFoxOptions.add_argument(f'user-agent={user_agent}')\n",
    "    #driver = webdriver.Firefox(options=fireFoxOptions)\n",
    "    \n",
    "    #CHROME\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--disable-extensions\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\") # linux only\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.190 Safari/537.36\"\n",
    "    chrome_options.add_argument(f'user-agent={user_agent}')\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    driver = webdriver.Chrome()\n",
    "    # Initialize the driver\n",
    "\n",
    "\n",
    "    #Go the the searchpage of the Washington Post\n",
    "    driver.get(\"https://www.washingtonpost.com/search/?query=+&facets=%7B%22time%22%3A%22all%22%2C%22sort%22%3A%22relevancy%22%2C%22section%22%3A%5B%5D%2C%22author%22%3A%5B%5D%7D\")\n",
    "    #Accept Cookies\n",
    "    try:\n",
    "        driver.save_screenshot(\"cookies.png\")\n",
    "        WebDriverWait(driver,20).until(EC.presence_of_element_located((By.ID, \"onetrust-accept-btn-handler\"))).click()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    #Log-in into Washington Post account\n",
    "    try: #it is possible that account is already logged in! \n",
    "        #Sign in into Washington Post account\n",
    "        #Click on Sign In\n",
    "        #WebDriverWait(driver,10).until(EC.presence_of_element_located((By.XPATH, \"//*[@id='__next']/div[1]/nav/div[4]/div[2]/a/p\"))).click()\n",
    "        driver.get(\"https://www.washingtonpost.com/subscribe/signin/?next_url=https%3A%2F%2Fwww.washingtonpost.com&nid=top_pb_signin&arcId=&itid=nav_sign_in\")\n",
    "        time.sleep(5)\n",
    "        driver.save_screenshot(\"Login.png\")\n",
    "        #located username field\n",
    "        time.sleep(2) #field needs to completely load\n",
    "        username_field = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.XPATH, \"//*[@id='username']\")))\n",
    "        username_field.send_keys(username) #send username\n",
    "        username_field.send_keys(Keys.RETURN) #press enter\n",
    "\n",
    "        #Located password field\n",
    "        time.sleep(2) #field needs to completely load\n",
    "        password_field = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.XPATH, \"//*[@id='password']\")))\n",
    "        password_field.send_keys(password)\n",
    "        password_field.send_keys(Keys.ENTER)\n",
    "\n",
    "        #Go back to the search page\n",
    "        WebDriverWait(driver,10).until(EC.presence_of_element_located((By.CLASS_NAME, \"wpds-c-jlBemH \"))).click()\n",
    "        go_to_search = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.ID, \"query\")))\n",
    "        go_to_search.send_keys(search_term)\n",
    "        go_to_search.send_keys(Keys.ENTER)\n",
    "\n",
    "    except:\n",
    "        #you already logged-in -> continue \n",
    "        pass\n",
    "    \n",
    "    search_page_load = 0\n",
    "    while(search_page_load < 100):\n",
    "        try:\n",
    "            driver.get(\"https://www.washingtonpost.com/search/?query=+&facets=%7B%22time%22%3A%22all%22%2C%22sort%22%3A%22relevancy%22%2C%22section%22%3A%5B%5D%2C%22author%22%3A%5B%5D%7D\")\n",
    "            break\n",
    "        except:\n",
    "            search_page_load += 1\n",
    "            \n",
    "    scrape_time = time.time()\n",
    "    for single_date in daterange(start_date, end_date):\n",
    "        date = single_date.strftime(\"%m/%d/%y\")\n",
    "\n",
    "        #Locate the searchbar, send searchterm and press enter\n",
    "        \n",
    "        time.sleep(10)\n",
    "        driver.save_screenshot(\"Searchbar.png\")\n",
    "        searchbar = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.CLASS_NAME, \"aa-Input\")))\n",
    "        searchbar.send_keys(\" \")\n",
    "        searchbar.send_keys(Keys.ENTER)\n",
    "\n",
    "        #Select time period\n",
    "        #select periode specification\n",
    "        WebDriverWait(driver,10).until(EC.presence_of_element_located((By.XPATH, \"//*[@id='main-content']/div[1]/section[2]/div[1]/button/div/div[2]/span\"))).click()\n",
    "\n",
    "        #select custom time\n",
    "        WebDriverWait(driver,10).until(EC.presence_of_element_located((By.XPATH, \"//*[@id='main-content']/div[1]/section[2]/div[1]/div/div/div[1]/button/div/div[2]\"))).click()\n",
    "        time.sleep(1)\n",
    "\n",
    "        #Send start date\n",
    "        WebDriverWait(driver,10).until(EC.presence_of_element_located((By.XPATH, \"//*[@id='main-content']/div[1]/section[2]/div[1]/div/div/div[2]/div[1]/div/div/div[1]/input\"))).send_keys(date)\n",
    "\n",
    "        #Empty automatic end date\n",
    "        WebDriverWait(driver,10).until(EC.presence_of_element_located((By.XPATH, \"//*[@id='main-content']/div[1]/section[2]/div[1]/div/div/div[2]/div[2]/div/div/div[1]/input\"))).clear()\n",
    "\n",
    "        #Send end date\n",
    "        WebDriverWait(driver,10).until(EC.presence_of_element_located((By.XPATH, \"//*[@id='main-content']/div[1]/section[2]/div[1]/div/div/div[2]/div[2]/div/div/div[1]/input\"))).send_keys(date)\n",
    "\n",
    "        #Press Apply\n",
    "        WebDriverWait(driver,10).until(EC.presence_of_element_located((By.XPATH, \"//*[@id='main-content']/div[1]/section[2]/div[1]/div/div/div[2]/button[2]\"))).click()\n",
    "        time.sleep(5)\n",
    "        driver.refresh()\n",
    "        #click to load more on the page untill no longer possible\n",
    "        loading = True\n",
    "        while(loading):\n",
    "            try:\n",
    "                driver.save_screenshot(\"loading.png\")\n",
    "                WebDriverWait(driver,10).until(EC.presence_of_element_located((By.XPATH, \"//*[@id='main-content']/div[1]/section[3]/button\"))).click()\n",
    "            except:\n",
    "                loading = False\n",
    "        \n",
    "        driver.save_screenshot(\"articles.png\")\n",
    "        articles = WebDriverWait(driver,10).until(EC.presence_of_all_elements_located((By.TAG_NAME, \"article\")))\n",
    "\n",
    "        for a in range(0,len(articles)):\n",
    "            WP_link.append(articles[a].find_element(By.TAG_NAME, \"a\").get_attribute('href'))\n",
    "            WP_date.append(single_date.strftime(\"%d/%m/%Y\"))\n",
    "    \n",
    "    driver.quit()\n",
    "    \n",
    "    WP = [\"Washington_Post\"] * len(WP_link)\n",
    "    data = {\"Date\" : WP_date, \"News Paper\" : WP, \"Link\" : WP_link}\n",
    "    WashingtonPost = pd.DataFrame(data)\n",
    "\n",
    "    start_date = start_date.strftime(\"%d%m%Y\")\n",
    "    end_date = end_date.strftime(\"%d%m%Y\")\n",
    "    path = f\"Data/URLS/Washington_Post_{start_date}_{end_date}_URLS\"\n",
    "    #WashingtonPost.to_parquet(path) \n",
    "    return(WashingtonPost)\n",
    "\n",
    "    print(\"--- %s seconds ---\" % (time.time() - scrape_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c850d03",
   "metadata": {},
   "source": [
    "### 2.2.2. Function - Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cc19fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWashingtonPostArticles(username, password, dataframe):\n",
    "    WP_article_content = []\n",
    "    WP_title = []\n",
    "\n",
    "    #FIREFOX\n",
    "    #Declare the driver and go to website\n",
    "    #fireFoxOptions = webdriver.FirefoxOptions()\n",
    "    #fireFoxOptions.add_argument('--ignore-certificate-errors')\n",
    "    #fireFoxOptions.add_argument('--allow-running-insecure-content')\n",
    "    #fireFoxOptions.add_argument(\"--headless\")\n",
    "    #fireFoxOptions.add_argument(\"--disable-gpu\")\n",
    "    #fireFoxOptions.add_argument(\"--no-sandbox\")\n",
    "    #user_agent = 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.50 Safari/537.36'\n",
    "    #fireFoxOptions.add_argument(f'user-agent={user_agent}')\n",
    "    #driver_WSJ = webdriver.Firefox(options=fireFoxOptions)\n",
    "\n",
    "    #CHROME\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--disable-extensions\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\") # linux only\n",
    "    #chrome_options.add_argument(\"--headless\")\n",
    "    user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.190 Safari/537.36\"\n",
    "    chrome_options.add_argument(f'user-agent={user_agent}')\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    #driver = webdriver.Chrome()\n",
    "\n",
    "    #Go the the searchpage of the Washington Post\n",
    "    driver.get(\"https://www.washingtonpost.com/search/?query=+&facets=%7B%22time%22%3A%22all%22%2C%22sort%22%3A%22relevancy%22%2C%22section%22%3A%5B%5D%2C%22author%22%3A%5B%5D%7D\")\n",
    "\n",
    "    try:\n",
    "        #Accept Cookies\n",
    "        WebDriverWait(driver,10).until(EC.presence_of_element_located((By.ID, \"onetrust-accept-btn-handler\"))).click()\n",
    "    except:\n",
    "        print(\"no cookies\")\n",
    "        pass\n",
    "    #Log-in into Washington Post account\n",
    "    try: #it is possible that account is already logged in! \n",
    "\n",
    "        #Sign in into Washington Post account\n",
    "        #Click on Sign In\n",
    "        WebDriverWait(driver,10).until(EC.presence_of_element_located((By.XPATH, \"//*[@id='__next']/div[1]/nav/div[4]/div[2]/a/p\"))).click()\n",
    "        print('signin attempted')\n",
    "        #located username field\n",
    "        time.sleep(2) #field needs to completely load\n",
    "        username_field = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.XPATH, \"//*[@id='username']\")))\n",
    "        username_field.send_keys(username) #send username\n",
    "        username_field.send_keys(Keys.RETURN) #press enter\n",
    "\n",
    "        #Located password field\n",
    "        time.sleep(2) #field needs to completely load\n",
    "        password_field = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.XPATH, \"//*[@id='password']\")))\n",
    "        password_field.send_keys(password)\n",
    "        password_field.send_keys(Keys.ENTER)\n",
    "\n",
    "        time.sleep(10)\n",
    "\n",
    "    except:\n",
    "        #you already logged-in -> continue \n",
    "        pass\n",
    "\n",
    "    scrape_time = time.time()\n",
    "    links = list(dataframe[\"Link\"])\n",
    "    for u in range(0, len(links)):\n",
    "            trys = 0\n",
    "            while(trys < 100):\n",
    "                try:\n",
    "                    #go to every article\n",
    "                    driver.get(links[u])\n",
    "                    break\n",
    "                except:\n",
    "                    trys += 1\n",
    "            try:\n",
    "                text = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.TAG_NAME, \"article\"))).text\n",
    "            except:\n",
    "                print(links[u])\n",
    "                text = \"Photo\"\n",
    "                \n",
    "            try:\n",
    "                text = text[text.index(\"Share\")+len(\"Share\")+1:text.rindex(\"Gift Article\")]\n",
    "                WP_article_content.append(text)\n",
    "            except:\n",
    "                try:\n",
    "                    text = text[text.index(\"Share\")+len(\"Share\")+1:text.rindex(\"Comments\")]\n",
    "                    WP_article_content.append(text)\n",
    "                except:\n",
    "                    try:\n",
    "                        text = text[text.index(\"tb.boedttibo\")+len(\"tb.boedttibo\")+1:text.rindex(\"Comments\")]\n",
    "                        WP_article_content.append(text)\n",
    "                    except:\n",
    "                        WP_article_content.append(text)\n",
    "                        #WP_article_content.append(\"ERROR\")\n",
    "                        pass\n",
    "            #idea is: We take the full text and before we have the article content the last pice of text is \"share\" and immediatly after the content\n",
    "            #we have \"comments\" this way we are able to only extract the article content\n",
    "            #the reasons for this is that each article has a more or less different html dependent on their \"type\"\n",
    "\n",
    "            #there are different formats of titles, in this order all titles are located\n",
    "            try:\n",
    "                title = driver.find_element(By.CSS_SELECTOR, \"#main-content > span:nth-child(2)\").text\n",
    "                WP_title.append(title)\n",
    "            except:\n",
    "                try:\n",
    "                    title = driver.find_element(By.CSS_SELECTOR, \"#main-content\").text\n",
    "                    WP_title.append(title)\n",
    "                except:\n",
    "                    try:\n",
    "                        title = driver.find_element(By.CSS_SELECTOR, \"#main-content > span\").text\n",
    "                        WP_title.append(title)\n",
    "                    except:\n",
    "                        WP_title.append(\"ERROR\")\n",
    "                        pass\n",
    "\n",
    "    #Close driver\n",
    "    driver.quit()\n",
    "\n",
    "    #How long did the scraping took? \n",
    "    print(\"--- %s seconds ---\" % (time.time() - scrape_time))\n",
    "\n",
    "    #Create output file\n",
    "    #Make the dataframe\n",
    "    WP = [\"Washington Post\"] * len(WP_article_content)\n",
    "    data = {\"Title\" : WP_title, \"Text\" : WP_article_content, \"Date\" : dataframe[\"Date\"], \"News Paper\" : dataframe[\"News Paper\"], \"Link\" : dataframe[\"Link\"]}\n",
    "    WashingtonPost = pd.DataFrame(data)\n",
    "    return(WashingtonPost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d699e9f",
   "metadata": {},
   "source": [
    "## 2.3. New York Times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c29e7e",
   "metadata": {},
   "source": [
    "### 2.3.1. Function - Get URLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec8d98ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNewYorkTimesURLS(start_date, end_date, username, password):\n",
    "    #put dates in date format\n",
    "    start_date = datetime.strptime(start_date, \"%d/%m/%Y\") \n",
    "    end_date = datetime.strptime(end_date, \"%d/%m/%Y\")\n",
    "\n",
    "    #Declare all list variables for the output\n",
    "    NYT_date = []\n",
    "    NYT_link = []\n",
    "\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--window-size=1920x1080\")\n",
    "    chrome_options.add_argument(\"--disable-extensions\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\") # linux only\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.81 Safari/537.36\"\n",
    "    chrome_options.add_argument(f'user-agent={user_agent}')\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    driver.get(\"https://myaccount.nytimes.com/auth/login?response_type=cookie&client_id=vi&redirect_uri=https%3A%2F%2Fwww.nytimes.com%2Fsubscription%2Fonboarding-offer%3FcampaignId%3D7JFJX%26EXIT_URI%3Dhttps%253A%252F%252Fwww.nytimes.com%252F&asset=masthead\")\n",
    "    time.sleep(5)\n",
    "    driver.save_screenshot(\"test.png\")\n",
    "    #not a robot\n",
    "    input(\"Press Enter to continue...\")\n",
    "\n",
    "    #Select log-in field\n",
    "    login = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.ID, \"email\")))\n",
    "\n",
    "    #Send username\n",
    "    login.send_keys(username)\n",
    "\n",
    "    #Press continue\n",
    "    WebDriverWait(driver,10).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"#myAccountAuth > div > div > div > form > div > div.css-bho3kg-buttonWrapper-buttonStyles-Button > button\"))).click()\n",
    "\n",
    "    #Select password field\n",
    "    password_field = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.ID, \"password\")))\n",
    "\n",
    "    #send password\n",
    "    password_field.send_keys(password)\n",
    "\n",
    "    #Press continue\n",
    "    WebDriverWait(driver,10).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"#myAccountAuth > div > div > form > div > div.css-1nkv26b-buttonWrapper-buttonStyles-Button > button\"))).click()\n",
    "\n",
    "    time.sleep(1)\n",
    "\n",
    "    #go to search page\n",
    "    search_page_loaded = 0\n",
    "    while(search_page_loaded < 100):\n",
    "        try:\n",
    "            driver.get(\"https://www.nytimes.com/search?dropmab=false&query=&sort=best\")\n",
    "            break\n",
    "        except:\n",
    "            search_page_loaded += 1\n",
    "\n",
    "    #Accept Cookies\n",
    "    try:\n",
    "        WebDriverWait(driver,5).until(EC.presence_of_element_located((By.CLASS_NAME, \"banner__container__cta--accept\"))).click()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    scrape_time = time.time()\n",
    "    for single_date in daterange(start_date, end_date):\n",
    "        date = single_date.strftime(\"%Y%m%d\")\n",
    "\n",
    "        url = f\"https://www.nytimes.com/search?dropmab=false&endDate={date}&query=%20&sort=best&startDate={date}\"\n",
    "        url_loaded = 0\n",
    "        while(url_loaded < 100):\n",
    "            try:\n",
    "                driver.get(url)\n",
    "                break\n",
    "            except:\n",
    "                url_loaded += 1\n",
    "\n",
    "        #click to load more on the page untill no longer possible\n",
    "        loading = True\n",
    "        while(loading):\n",
    "            try:\n",
    "                WebDriverWait(driver,10).until(EC.presence_of_element_located((By.XPATH, \"//*[@id='site-content']/div/div[2]/div[2]/div/button\"))).click()\n",
    "            except:\n",
    "                loading = False\n",
    "\n",
    "        articles = WebDriverWait(driver,10).until(EC.presence_of_all_elements_located((By.CLASS_NAME, \"css-1l4w6pd\")))\n",
    "        dates = WebDriverWait(driver,10).until(EC.presence_of_all_elements_located((By.CLASS_NAME, \"css-17ubb9w\")))\n",
    "\n",
    "        date_list = WebDriverWait(driver,10).until(EC.presence_of_all_elements_located((By.CLASS_NAME, \"css-17ubb9w\")))\n",
    "        true_date = date_list[0].text\n",
    "        for a in range(0, len(articles)):\n",
    "            if(true_date == date_list[a].text):\n",
    "                NYT_link.append(articles[a].find_element(By.TAG_NAME, \"a\").get_attribute('href'))\n",
    "                NYT_date.append(single_date.strftime(\"%d/%m/%Y\"))\n",
    "    \n",
    "    driver.quit()\n",
    "    \n",
    "    NYT = [\"New_York_Times\"] * len(NYT_link)\n",
    "    data = {\"Date\" : NYT_date, \"News Paper\" : NYT, \"Link\" : NYT_link}\n",
    "    New_York_Times = pd.DataFrame(data)\n",
    "    \n",
    "    print(\"--- %s seconds ---\" % (time.time() - scrape_time))\n",
    "    \n",
    "    return(New_York_Times)\n",
    "    start_date = start_date.strftime(\"%d%m%Y\")\n",
    "    end_date = end_date.strftime(\"%d%m%Y\")\n",
    "    path = f\"Data/URLS/New_York_Times_{start_date}_{end_date}_URLS\"\n",
    "    New_York_Times.to_parquet(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce3650f",
   "metadata": {},
   "source": [
    "## 2.4. Politico"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f600994",
   "metadata": {},
   "source": [
    "### 2.4.1. Function - Get URLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050ba84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPolitico(start_date, end_date):\n",
    "    #put dates in date format\n",
    "    end_date = datetime.strptime(end_date, \"%d/%m/%Y\")\n",
    "    start_date = datetime.strptime(start_date, \"%d/%m/%Y\")\n",
    "\n",
    "    #Declare all list variables for the output\n",
    "    P_date = []\n",
    "    P_link = []\n",
    "\n",
    "    fireFoxOptions = webdriver.FirefoxOptions()\n",
    "    fireFoxOptions.add_argument('--ignore-certificate-errors')\n",
    "    fireFoxOptions.add_argument('--allow-running-insecure-content')\n",
    "    fireFoxOptions.add_argument(\"--headless\")\n",
    "    fireFoxOptions.add_argument(\"--disable-gpu\")\n",
    "    fireFoxOptions.add_argument(\"--no-sandbox\")\n",
    "    user_agent = 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.50 Safari/537.36'\n",
    "    fireFoxOptions.add_argument(f'user-agent={user_agent}')\n",
    "    driver = webdriver.Firefox(options=fireFoxOptions)\n",
    "\n",
    "    #go to the website\n",
    "    driver.get(\"https://www.politico.com/search?adv=true\")\n",
    "\n",
    "    #accept cookies\n",
    "    try:\n",
    "        WebDriverWait(driver,10).until(EC.presence_of_element_located((By.ID, \"onetrust-accept-btn-handler\"))).click()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    scrape_time = time.time()\n",
    "    for single_date in daterange(start_date, end_date):\n",
    "        date = single_date.strftime(\"%m/%d/%Y\")\n",
    "        test = 0\n",
    "        while(test < 100):\n",
    "            try:\n",
    "                driver.get(f\"https://www.politico.com/search/1?adv=true&start={date}&end={date}\")\n",
    "                break\n",
    "            except:\n",
    "                test += 1\n",
    "                time.sleep(10)\n",
    "        articles = WebDriverWait(driver,10).until(EC.presence_of_all_elements_located((By.TAG_NAME, \"article\")))\n",
    "        for a in range(0, len(articles)):\n",
    "            P_link.append(articles[a].find_element(By.TAG_NAME, \"a\").get_attribute('href'))\n",
    "            P_date.append(date)\n",
    "\n",
    "        #get the number of pages to know how much you should click .pagination > ol:nth-child(2) > li:nth-child(3) > a:nth-child(1)\n",
    "        try:\n",
    "            nr_pages = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.CSS_SELECTOR, \".pagination > ol:nth-child(2) > li:nth-child(6) > a:nth-child(1)\"))).text\n",
    "        except:\n",
    "            nr_pages = 1\n",
    "            pass\n",
    "\n",
    "        for p in range(1, int(nr_pages)):\n",
    "            url = f\"https://www.politico.com/search/{p+1}?adv=true&start={date}&end={date}\"\n",
    "            driver.get(url)\n",
    "\n",
    "        articles = WebDriverWait(driver,10).until(EC.presence_of_all_elements_located((By.TAG_NAME, \"article\")))\n",
    "        for a in range(0, len(articles)):\n",
    "            P_link.append(articles[a].find_element(By.TAG_NAME, \"a\").get_attribute('href'))\n",
    "            P_date.append(single_date.strftime(\"%d/%m/%Y\"))\n",
    "\n",
    "    driver.quit()\n",
    "    print(\"--- %s seconds ---\" % (time.time() - scrape_time))\n",
    "\n",
    "    P = [\"Politico\"] * len(P_link)\n",
    "    data = {\"Date\" : P_date, \"News Paper\" : P, \"Link\" : P_link}\n",
    "    Politico = pd.DataFrame(data)\n",
    "\n",
    "    start_date = start_date.strftime(\"%d%m%Y\")\n",
    "    end_date = end_date.strftime(\"%d%m%Y\")\n",
    "    path = f\"Data/URLS/Politico_{start_date}_{end_date}_URLS\"\n",
    "    Politico.to_parquet(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0928dfb",
   "metadata": {},
   "source": [
    "# 3. Running Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc7e763",
   "metadata": {},
   "source": [
    "## 3.1. Wall Street Journal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa89ce63",
   "metadata": {},
   "source": [
    "## 3.2. Washington Post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b83ebf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getWashingtonPostURLS(\"01/01/2016\", \"01/01/2016\", \"tb.boedttibo@gmail.com\", \"ThesisR&T\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5515224c",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Wall_Street_Journal_2016_URLS(1)'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[79], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m#URLS_WP = pd.read_parquet(\"Washington_Post_2020_URLS\")\u001b[39;00m\n\u001b[0;32m      2\u001b[0m year\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m2016\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m URLS_WP \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_parquet(\u001b[39m\"\u001b[39;49m\u001b[39mWall_Street_Journal_\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m+\u001b[39;49myear\u001b[39m+\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m_URLS(1)\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      4\u001b[0m URLS_WP\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\thesis\\lib\\site-packages\\pandas\\io\\parquet.py:503\u001b[0m, in \u001b[0;36mread_parquet\u001b[1;34m(path, engine, columns, storage_options, use_nullable_dtypes, **kwargs)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    457\u001b[0m \u001b[39mLoad a parquet object from the file path, returning a DataFrame.\u001b[39;00m\n\u001b[0;32m    458\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    499\u001b[0m \u001b[39mDataFrame\u001b[39;00m\n\u001b[0;32m    500\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    501\u001b[0m impl \u001b[39m=\u001b[39m get_engine(engine)\n\u001b[1;32m--> 503\u001b[0m \u001b[39mreturn\u001b[39;00m impl\u001b[39m.\u001b[39mread(\n\u001b[0;32m    504\u001b[0m     path,\n\u001b[0;32m    505\u001b[0m     columns\u001b[39m=\u001b[39mcolumns,\n\u001b[0;32m    506\u001b[0m     storage_options\u001b[39m=\u001b[39mstorage_options,\n\u001b[0;32m    507\u001b[0m     use_nullable_dtypes\u001b[39m=\u001b[39muse_nullable_dtypes,\n\u001b[0;32m    508\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    509\u001b[0m )\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\thesis\\lib\\site-packages\\pandas\\io\\parquet.py:244\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[1;34m(self, path, columns, use_nullable_dtypes, storage_options, **kwargs)\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[39mif\u001b[39;00m manager \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39marray\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    242\u001b[0m     to_pandas_kwargs[\u001b[39m\"\u001b[39m\u001b[39msplit_blocks\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m  \u001b[39m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m--> 244\u001b[0m path_or_handle, handles, kwargs[\u001b[39m\"\u001b[39m\u001b[39mfilesystem\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m _get_path_or_handle(\n\u001b[0;32m    245\u001b[0m     path,\n\u001b[0;32m    246\u001b[0m     kwargs\u001b[39m.\u001b[39;49mpop(\u001b[39m\"\u001b[39;49m\u001b[39mfilesystem\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    247\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[0;32m    248\u001b[0m     mode\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    249\u001b[0m )\n\u001b[0;32m    250\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    251\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi\u001b[39m.\u001b[39mparquet\u001b[39m.\u001b[39mread_table(\n\u001b[0;32m    252\u001b[0m         path_or_handle, columns\u001b[39m=\u001b[39mcolumns, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    253\u001b[0m     )\u001b[39m.\u001b[39mto_pandas(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mto_pandas_kwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\thesis\\lib\\site-packages\\pandas\\io\\parquet.py:102\u001b[0m, in \u001b[0;36m_get_path_or_handle\u001b[1;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[0;32m     92\u001b[0m handles \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m     94\u001b[0m     \u001b[39mnot\u001b[39;00m fs\n\u001b[0;32m     95\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_dir\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[39m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[0;32m    101\u001b[0m     \u001b[39m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[1;32m--> 102\u001b[0m     handles \u001b[39m=\u001b[39m get_handle(\n\u001b[0;32m    103\u001b[0m         path_or_handle, mode, is_text\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, storage_options\u001b[39m=\u001b[39;49mstorage_options\n\u001b[0;32m    104\u001b[0m     )\n\u001b[0;32m    105\u001b[0m     fs \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    106\u001b[0m     path_or_handle \u001b[39m=\u001b[39m handles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\thesis\\lib\\site-packages\\pandas\\io\\common.py:865\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    856\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\n\u001b[0;32m    857\u001b[0m             handle,\n\u001b[0;32m    858\u001b[0m             ioargs\u001b[39m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    861\u001b[0m             newline\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    862\u001b[0m         )\n\u001b[0;32m    863\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 865\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(handle, ioargs\u001b[39m.\u001b[39;49mmode)\n\u001b[0;32m    866\u001b[0m     handles\u001b[39m.\u001b[39mappend(handle)\n\u001b[0;32m    868\u001b[0m \u001b[39m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Wall_Street_Journal_2016_URLS(1)'"
     ]
    }
   ],
   "source": [
    "#URLS_WP = pd.read_parquet(\"Washington_Post_2020_URLS\")\n",
    "year=\"2016\"\n",
    "URLS_WP = pd.read_parquet(\"Wall_Street_Journal_\"+year+\"_URLS(1)\")\n",
    "URLS_WP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d40067a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "      <th>Date</th>\n",
       "      <th>News Paper</th>\n",
       "      <th>Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>29/02/2016</td>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>https://www.washingtonpost.com/local/public-sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>By Kristen Hartke</td>\n",
       "      <td></td>\n",
       "      <td>29/02/2016</td>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>https://www.washingtonpost.com/national/health...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>29/02/2016</td>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>https://www.washingtonpost.com/sports/highscho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is what it's like to bake brownies when y...</td>\n",
       "      <td></td>\n",
       "      <td>29/02/2016</td>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>https://www.washingtonpost.com/national/health...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>By Gregory Lee Sullivan\\nFebruary 29, 2016</td>\n",
       "      <td></td>\n",
       "      <td>29/02/2016</td>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>https://www.washingtonpost.com/national/health...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11197</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>01/01/2016</td>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>https://www.washingtonpost.com/entertainment/b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11198</th>\n",
       "      <td>Its high season for online dating  plot your...</td>\n",
       "      <td></td>\n",
       "      <td>01/01/2016</td>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>https://www.washingtonpost.com/news/soloish/wp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11199</th>\n",
       "      <td>Odell Beckham Jr. vows to learn from suspens...</td>\n",
       "      <td></td>\n",
       "      <td>01/01/2016</td>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>https://www.washingtonpost.com/news/early-lead...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11200</th>\n",
       "      <td>Correction: An earlier version of this story h...</td>\n",
       "      <td></td>\n",
       "      <td>01/01/2016</td>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>https://www.washingtonpost.com/local/dc-marche...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11201</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>01/01/2016</td>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>https://www.washingtonpost.com/opinions/space-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11202 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Title Text        Date  \\\n",
       "0                                                              29/02/2016   \n",
       "1                                      By Kristen Hartke       29/02/2016   \n",
       "2                                                              29/02/2016   \n",
       "3      This is what it's like to bake brownies when y...       29/02/2016   \n",
       "4             By Gregory Lee Sullivan\\nFebruary 29, 2016       29/02/2016   \n",
       "...                                                  ...  ...         ...   \n",
       "11197                                                          01/01/2016   \n",
       "11198  Its high season for online dating  plot your...       01/01/2016   \n",
       "11199  Odell Beckham Jr. vows to learn from suspens...       01/01/2016   \n",
       "11200  Correction: An earlier version of this story h...       01/01/2016   \n",
       "11201                                                          01/01/2016   \n",
       "\n",
       "            News Paper                                               Link  \n",
       "0      Washington_Post  https://www.washingtonpost.com/local/public-sa...  \n",
       "1      Washington_Post  https://www.washingtonpost.com/national/health...  \n",
       "2      Washington_Post  https://www.washingtonpost.com/sports/highscho...  \n",
       "3      Washington_Post  https://www.washingtonpost.com/national/health...  \n",
       "4      Washington_Post  https://www.washingtonpost.com/national/health...  \n",
       "...                ...                                                ...  \n",
       "11197  Washington_Post  https://www.washingtonpost.com/entertainment/b...  \n",
       "11198  Washington_Post  https://www.washingtonpost.com/news/soloish/wp...  \n",
       "11199  Washington_Post  https://www.washingtonpost.com/news/early-lead...  \n",
       "11200  Washington_Post  https://www.washingtonpost.com/local/dc-marche...  \n",
       "11201  Washington_Post  https://www.washingtonpost.com/opinions/space-...  \n",
       "\n",
       "[11202 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "URLS_WP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "400dbd68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "      <th>Date</th>\n",
       "      <th>News Paper</th>\n",
       "      <th>Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>29/02/2016</td>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>https://www.washingtonpost.com/local/public-sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>By Kristen Hartke</td>\n",
       "      <td></td>\n",
       "      <td>29/02/2016</td>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>https://www.washingtonpost.com/national/health...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>29/02/2016</td>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>https://www.washingtonpost.com/sports/highscho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is what it's like to bake brownies when y...</td>\n",
       "      <td></td>\n",
       "      <td>29/02/2016</td>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>https://www.washingtonpost.com/national/health...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>By Gregory Lee Sullivan\\nFebruary 29, 2016</td>\n",
       "      <td></td>\n",
       "      <td>29/02/2016</td>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>https://www.washingtonpost.com/national/health...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11197</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>01/01/2016</td>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>https://www.washingtonpost.com/entertainment/b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11198</th>\n",
       "      <td>Its high season for online dating  plot your...</td>\n",
       "      <td></td>\n",
       "      <td>01/01/2016</td>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>https://www.washingtonpost.com/news/soloish/wp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11199</th>\n",
       "      <td>Odell Beckham Jr. vows to learn from suspens...</td>\n",
       "      <td></td>\n",
       "      <td>01/01/2016</td>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>https://www.washingtonpost.com/news/early-lead...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11200</th>\n",
       "      <td>Correction: An earlier version of this story h...</td>\n",
       "      <td></td>\n",
       "      <td>01/01/2016</td>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>https://www.washingtonpost.com/local/dc-marche...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11201</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>01/01/2016</td>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>https://www.washingtonpost.com/opinions/space-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11202 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Title Text        Date  \\\n",
       "0                                                              29/02/2016   \n",
       "1                                      By Kristen Hartke       29/02/2016   \n",
       "2                                                              29/02/2016   \n",
       "3      This is what it's like to bake brownies when y...       29/02/2016   \n",
       "4             By Gregory Lee Sullivan\\nFebruary 29, 2016       29/02/2016   \n",
       "...                                                  ...  ...         ...   \n",
       "11197                                                          01/01/2016   \n",
       "11198  Its high season for online dating  plot your...       01/01/2016   \n",
       "11199  Odell Beckham Jr. vows to learn from suspens...       01/01/2016   \n",
       "11200  Correction: An earlier version of this story h...       01/01/2016   \n",
       "11201                                                          01/01/2016   \n",
       "\n",
       "            News Paper                                               Link  \n",
       "0      Washington_Post  https://www.washingtonpost.com/local/public-sa...  \n",
       "1      Washington_Post  https://www.washingtonpost.com/national/health...  \n",
       "2      Washington_Post  https://www.washingtonpost.com/sports/highscho...  \n",
       "3      Washington_Post  https://www.washingtonpost.com/national/health...  \n",
       "4      Washington_Post  https://www.washingtonpost.com/national/health...  \n",
       "...                ...                                                ...  \n",
       "11197  Washington_Post  https://www.washingtonpost.com/entertainment/b...  \n",
       "11198  Washington_Post  https://www.washingtonpost.com/news/soloish/wp...  \n",
       "11199  Washington_Post  https://www.washingtonpost.com/news/early-lead...  \n",
       "11200  Washington_Post  https://www.washingtonpost.com/local/dc-marche...  \n",
       "11201  Washington_Post  https://www.washingtonpost.com/opinions/space-...  \n",
       "\n",
       "[11202 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year = \"2016\"\n",
    "source = 'Washington_Post_'\n",
    "#source = \"Wall_Street_Journal_\"\n",
    "### check the most common results for error indicators\n",
    "df = pd.read_parquet(source+year+\"_Articles\")\n",
    "# print(df.size)\n",
    "# pd.DataFrame(df['Text'].value_counts())\n",
    "\n",
    "to_re_scrape = df[df['Text']== ''].reset_index().drop(['index'], axis=1)\n",
    "URLS_WP = to_re_scrape\n",
    "URLS_WP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ad7392e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tijd = time.time()\n",
    "n=0\n",
    "import concurrent.futures\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
    "    while(n < 13):\n",
    "        k = 1000*n\n",
    "        # Submit the two instances of the scraping function to the executor\n",
    "            # future1 = executor.submit(getWallStreetJournalArticles, \"reane.delaunoy@telenet.be\", \"REenJUC0MB0\", URLS_WP[k+1:k+250])\n",
    "            # future2 = executor.submit(getWallStreetJournalArticles, \"reane.delaunoy@telenet.be\", \"REenJUC0MB0\", URLS_WP[k+251:k+500])\n",
    "            # future3 = executor.submit(getWallStreetJournalArticles, \"reane.delaunoy@telenet.be\", \"REenJUC0MB0\", URLS_WP[k+501:k+750])\n",
    "            # future4 = executor.submit(getWallStreetJournalArticles, \"reane.delaunoy@telenet.be\", \"REenJUC0MB0\", URLS_WP[k+751:k+1000])\n",
    "        future1 = executor.submit(getWashingtonPostArticles,  \"tb.boedttibo@gmail.com\", \"ThesisR&T\", URLS_WP[k+1:k+250])\n",
    "        future2 = executor.submit(getWashingtonPostArticles,  \"tb.boedttibo@gmail.com\", \"ThesisR&T\", URLS_WP[k+251:k+500])\n",
    "        future3 = executor.submit(getWashingtonPostArticles,  \"tb.boedttibo@gmail.com\", \"ThesisR&T\", URLS_WP[k+501:k+750])\n",
    "        future4 = executor.submit(getWashingtonPostArticles,  \"tb.boedttibo@gmail.com\", \"ThesisR&T\", URLS_WP[k+751:k+1000])\n",
    "        # Wait for the scraping functions to complete\n",
    "        concurrent.futures.wait([future1, future2, future3, future4])\n",
    "\n",
    "        print(\"--------------------------\")   \n",
    "        print(\"--- %s seconds ---\" % (time.time() - tijd))\n",
    "\n",
    "        result1= future1.result()\n",
    "        result2= future2.result()\n",
    "        result3= future3.result()\n",
    "        result4= future4.result()\n",
    "        frames = [result1, result2, result3, result4]\n",
    "        df = pd.concat(frames).reset_index(drop = True)\n",
    "        articles = pd.read_parquet(\"Washington_Post_\"+year+\"_Articles_re-scraped\") \n",
    "        articles = pd.concat([articles, df]).reset_index(drop = True)\n",
    "        articles.to_parquet(\"Washington_Post_\"+year+\"_Articles_re-scraped\")\n",
    "        #articles = pd.read_parquet(\"Washington_Post_\"+year+\"_Articles\")\n",
    "        #articles = pd.concat([articles, df]).reset_index(drop = True)\n",
    "        #articles.to_parquet(\"Washington_Post_\"+year+\"_Articles\")\n",
    "        n=n+1\n",
    "        print(\"--------------------------\")   \n",
    "        print(n)\n",
    "        print(\"--------------------------\")   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ea6d4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.to_parquet(\"Wall_Street_Journal_\"+year+\"_Articles_re-scraped\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecddc2d8",
   "metadata": {},
   "source": [
    "result1= future1.result()\n",
    "result2= future2.result()\n",
    "result3= future3.result()\n",
    "result4= future4.result()\n",
    "frames = [result1, result2, result3, result4]\n",
    "df = pd.concat(frames).reset_index(drop = True)\n",
    "articles = pd.read_parquet(\"Washington_Post_2016_Articles\")\n",
    "articles = pd.concat([articles, df]).reset_index(drop = True)\n",
    "articles.to_parquet(\"Washington_Post_2016_Articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "00e9a682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Date</th>\n",
       "      <th>News Paper</th>\n",
       "      <th>Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>542</td>\n",
       "      <td>05/01/2016</td>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>https://www.washingtonpost.com/news/dr-gridloc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>792</td>\n",
       "      <td>06/01/2016</td>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>https://www.washingtonpost.com/opinions/whos-r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1042</td>\n",
       "      <td>08/01/2016</td>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>https://www.washingtonpost.com/news/the-switch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1292</td>\n",
       "      <td>11/01/2016</td>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>https://www.washingtonpost.com/local/i-despise...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1542</td>\n",
       "      <td>12/01/2016</td>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>https://www.washingtonpost.com/news/early-lead...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>48792</td>\n",
       "      <td>21/12/2016</td>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>https://www.washingtonpost.com/news/true-crime...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>49042</td>\n",
       "      <td>23/12/2016</td>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>https://www.washingtonpost.com/national/for-wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>49292</td>\n",
       "      <td>27/12/2016</td>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>https://www.washingtonpost.com/lifestyle/magaz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>49542</td>\n",
       "      <td>29/12/2016</td>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>https://www.washingtonpost.com/news/sports/wp/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>49792</td>\n",
       "      <td>31/12/2016</td>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>https://www.washingtonpost.com/opinions/sidwel...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>198 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index        Date       News Paper  \\\n",
       "0      542  05/01/2016  Washington_Post   \n",
       "1      792  06/01/2016  Washington_Post   \n",
       "2     1042  08/01/2016  Washington_Post   \n",
       "3     1292  11/01/2016  Washington_Post   \n",
       "4     1542  12/01/2016  Washington_Post   \n",
       "..     ...         ...              ...   \n",
       "193  48792  21/12/2016  Washington_Post   \n",
       "194  49042  23/12/2016  Washington_Post   \n",
       "195  49292  27/12/2016  Washington_Post   \n",
       "196  49542  29/12/2016  Washington_Post   \n",
       "197  49792  31/12/2016  Washington_Post   \n",
       "\n",
       "                                                  Link  \n",
       "0    https://www.washingtonpost.com/news/dr-gridloc...  \n",
       "1    https://www.washingtonpost.com/opinions/whos-r...  \n",
       "2    https://www.washingtonpost.com/news/the-switch...  \n",
       "3    https://www.washingtonpost.com/local/i-despise...  \n",
       "4    https://www.washingtonpost.com/news/early-lead...  \n",
       "..                                                 ...  \n",
       "193  https://www.washingtonpost.com/news/true-crime...  \n",
       "194  https://www.washingtonpost.com/national/for-wo...  \n",
       "195  https://www.washingtonpost.com/lifestyle/magaz...  \n",
       "196  https://www.washingtonpost.com/news/sports/wp/...  \n",
       "197  https://www.washingtonpost.com/opinions/sidwel...  \n",
       "\n",
       "[198 rows x 4 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# which to re-scrape\n",
    "year = \"2016\"\n",
    "#source = 'Washington_Post_'\n",
    "source = \"Washington_Post_\"\n",
    "original_articles = pd.read_parquet(source+year+\"_Articles\")\n",
    "articles = pd.read_parquet(source+year+\"_Articles\")\n",
    "urls= pd.read_parquet(source+year+\"_URLS\")\n",
    "\n",
    "articles['Text'].replace('NO ACCESS', numpy.nan, inplace=True)\n",
    "articles['Text'].replace('404', numpy.nan, inplace=True)\n",
    "articles.dropna(subset=['Text'], inplace=True)\n",
    "\n",
    "df = pd.merge(urls, articles[['Link']], on=['Link','Link'], how=\"outer\", indicator=True\n",
    "              ).query('_merge==\"left_only\"').reset_index()\n",
    "to_re_scrape = df.loc[:, df.columns != '_merge']\n",
    "\n",
    "URLS_WP =to_re_scrape\n",
    "URLS_WP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3b7e9229",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "      <th>Date</th>\n",
       "      <th>News Paper</th>\n",
       "      <th>Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>29/02/2016</td>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>https://www.washingtonpost.com/local/public-sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>By Kristen Hartke</td>\n",
       "      <td></td>\n",
       "      <td>29/02/2016</td>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>https://www.washingtonpost.com/national/health...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>29/02/2016</td>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>https://www.washingtonpost.com/sports/highscho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>This is what it's like to bake brownies when y...</td>\n",
       "      <td></td>\n",
       "      <td>29/02/2016</td>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>https://www.washingtonpost.com/national/health...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>By Gregory Lee Sullivan\\nFebruary 29, 2016</td>\n",
       "      <td></td>\n",
       "      <td>29/02/2016</td>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>https://www.washingtonpost.com/national/health...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49578</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>01/01/2016</td>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>https://www.washingtonpost.com/entertainment/b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49580</th>\n",
       "      <td>Its high season for online dating  plot your...</td>\n",
       "      <td></td>\n",
       "      <td>01/01/2016</td>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>https://www.washingtonpost.com/news/soloish/wp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49582</th>\n",
       "      <td>Odell Beckham Jr. vows to learn from suspens...</td>\n",
       "      <td></td>\n",
       "      <td>01/01/2016</td>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>https://www.washingtonpost.com/news/early-lead...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49583</th>\n",
       "      <td>Correction: An earlier version of this story h...</td>\n",
       "      <td></td>\n",
       "      <td>01/01/2016</td>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>https://www.washingtonpost.com/local/dc-marche...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49585</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>01/01/2016</td>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>https://www.washingtonpost.com/opinions/space-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11202 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Title Text        Date  \\\n",
       "7                                                              29/02/2016   \n",
       "8                                      By Kristen Hartke       29/02/2016   \n",
       "9                                                              29/02/2016   \n",
       "11     This is what it's like to bake brownies when y...       29/02/2016   \n",
       "12            By Gregory Lee Sullivan\\nFebruary 29, 2016       29/02/2016   \n",
       "...                                                  ...  ...         ...   \n",
       "49578                                                          01/01/2016   \n",
       "49580  Its high season for online dating  plot your...       01/01/2016   \n",
       "49582  Odell Beckham Jr. vows to learn from suspens...       01/01/2016   \n",
       "49583  Correction: An earlier version of this story h...       01/01/2016   \n",
       "49585                                                          01/01/2016   \n",
       "\n",
       "            News Paper                                               Link  \n",
       "7      Washington_Post  https://www.washingtonpost.com/local/public-sa...  \n",
       "8      Washington_Post  https://www.washingtonpost.com/national/health...  \n",
       "9      Washington_Post  https://www.washingtonpost.com/sports/highscho...  \n",
       "11     Washington_Post  https://www.washingtonpost.com/national/health...  \n",
       "12     Washington_Post  https://www.washingtonpost.com/national/health...  \n",
       "...                ...                                                ...  \n",
       "49578  Washington_Post  https://www.washingtonpost.com/entertainment/b...  \n",
       "49580  Washington_Post  https://www.washingtonpost.com/news/soloish/wp...  \n",
       "49582  Washington_Post  https://www.washingtonpost.com/news/early-lead...  \n",
       "49583  Washington_Post  https://www.washingtonpost.com/local/dc-marche...  \n",
       "49585  Washington_Post  https://www.washingtonpost.com/opinions/space-...  \n",
       "\n",
       "[11202 rows x 5 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#which to still scrape\n",
    "\n",
    "#df_empty = df[df['Text']==''].reset_index()\n",
    "# rescrape the ''1\\nComments\\n'' text ones\n",
    "# df_weird = df[df['Text']=='1\\nComments\\n'].reset_index()\n",
    "# df_weird\n",
    "#df_empty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e0d84f",
   "metadata": {},
   "source": [
    "#which to still scrape\n",
    "year = \"2016\"\n",
    "#source = 'Washington_Post_'\n",
    "source = \"Wall_Street_Journal_\"\n",
    "original_articles = pd.read_parquet(source+year+\"_Articles\")\n",
    "articles = pd.read_parquet(source+year+\"_Articles\")\n",
    "urls= pd.read_parquet(source+year+\"_URLS\")\n",
    "\n",
    "\n",
    "\n",
    "len(URLS_WP)\n",
    "articles\n",
    "\n",
    "articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3a9c74a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(articles.duplicated(keep='last', subset = ['Link']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e8733d2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36428"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(articles['Title']== \"404\")\n",
    "len(articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb21e2bd",
   "metadata": {},
   "source": [
    "## 3.3. New York Times"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fc311562",
   "metadata": {},
   "source": [
    "## 4. Clean scrapet articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70da76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_set(set):\n",
    "    # delete no-access and flag them\n",
    "\n",
    "    # delete empty scrapes and flag them\n",
    "\n",
    "    #check 404 title and flag them\n",
    "\n",
    "    # check for short articles?\n",
    "    return set\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "e85556cd639849d6c1361db0a433433fe588bb9e07514b044682fe76c87df726"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
