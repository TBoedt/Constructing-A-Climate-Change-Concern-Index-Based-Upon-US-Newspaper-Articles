{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b21dbe2",
   "metadata": {},
   "source": [
    "# 0. Packages & Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0154ec9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "from nltk import ngrams\n",
    "from collections import Counter\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, pipeline, AutoModelForSequenceClassification, AutoModelForSeq2SeqLM\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pyarrow\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d4be568",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre-process text for lexicon based approaches\n",
    "def preprocess_text(text):\n",
    "    # remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # convert to lower case\n",
    "    text = text.lower()\n",
    "    # remove blank spaces\n",
    "    text = ' '.join(text.split())\n",
    "    # remove newline characters\n",
    "    text = text.replace('\\n', '')\n",
    "    return text\n",
    "\n",
    "def text_cleaning_final(text):\n",
    "    # remove blank spaces\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    # remove newline characters\n",
    "    text = text.replace('\\n', '')\n",
    "    \n",
    "    # Use regular expressions to match punctuation marks\n",
    "    pattern = r'([!\"#$%&\\'()*+,-./:;<=>?@\\[\\\\\\]^_`{|}~])'\n",
    "    # Replace punctuation marks with whitespaces before and after them\n",
    "    text = re.sub(pattern, r' \\1 ', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "25fa7ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_lexicon_words(text_df, lexicon):\n",
    "    lexicon = lexicon[\"Lexicon\"]\n",
    "    frequency = []\n",
    "    present = []\n",
    "    rfrequency = []\n",
    "    rpresent = []\n",
    "\n",
    "    for text in tqdm(text_df[\"Text\"]):\n",
    "        lexicon_counts = 0\n",
    "        present_count = 0\n",
    "        for word in lexicon:\n",
    "            lexicon_counts += text.lower().count(word.lower())\n",
    "            if(text.lower().count(word.lower()) > 0):\n",
    "                present_count += 1\n",
    "        \n",
    "        word_list = text.split() \n",
    "        word_count = len(word_list)\n",
    "\n",
    "        frequency.append(lexicon_counts)\n",
    "        present.append(present_count)\n",
    "        rfrequency.append((lexicon_counts/word_count)*100)\n",
    "        rpresent.append((present_count/word_count)*100)\n",
    "        \n",
    "        \n",
    "    text_df[\"Absolute Frequency\"] = frequency\n",
    "    text_df[\"Absolute Present\"] = present\n",
    "    text_df[\"Relative Frequency\"] = rfrequency\n",
    "    text_df[\"Relative Present\"] = rpresent\n",
    "    \n",
    "    return(text_df)\n",
    "\n",
    "def get_metrics(df, colname, threshold):\n",
    "    target = []\n",
    "    values = df[colname]\n",
    "    \n",
    "    for v in values:\n",
    "        if v >= threshold:\n",
    "            target.append(\"Yes\")\n",
    "        else:\n",
    "            target.append(\"No\")\n",
    "    \n",
    "    df[\"Estimate\"] = target\n",
    "    \n",
    "    cross_table = pd.crosstab(df['Target'], df['Estimate'], margins=True)\n",
    "    accuracy = (cross_table.iloc[0, 0] + cross_table.iloc[1, 1]) / cross_table.loc['All', 'All'] if cross_table.shape == (3,3) else cross_table.iloc[1,0] / (cross_table.iloc[1, 0] + cross_table.iloc[0,0]) \n",
    "    precision = cross_table.iloc[1,1] / (cross_table.iloc[0,1] + cross_table.iloc[1,1]) if cross_table.shape == (3,3) else 0\n",
    "    recall = cross_table.iloc[1,1] / (cross_table.iloc[1,0] + cross_table.iloc[1,1]) if cross_table.shape == (3,3) != 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "    return([accuracy, precision, recall, f1_score])\n",
    "\n",
    "def get_gross_table_data(df, colname, threshold, binary):\n",
    "    target = []\n",
    "    values = df[colname]\n",
    "    \n",
    "    for v in values:\n",
    "        if v >= threshold:\n",
    "            target.append(\"Yes\")\n",
    "        else:\n",
    "            target.append(\"No\")\n",
    "    \n",
    "    df[\"Estimate\"] = target\n",
    "    \n",
    "    if(binary):\n",
    "        cross_table = pd.crosstab(df['Target'], df['Estimate'], margins=True)\n",
    "    else:\n",
    "        cross_table = pd.crosstab(df['Final_Climate_Change_Level_Label'], df['Estimate'], margins=True)\n",
    "    return(cross_table)\n",
    "    \n",
    "def find_optimal_threshold(df, lexicon, lexicon_name):\n",
    "    df = count_lexicon_words(df, lexicon)\n",
    "    \n",
    "    #absolute frequency\n",
    "    af_accuracy = 0\n",
    "    af_th = 0\n",
    "    found = False\n",
    "    while(found == False):\n",
    "        metrics = get_metrics(df, \"Absolute Frequency\", af_th)\n",
    "        if metrics[0] > af_accuracy:\n",
    "            af_accuracy = metrics[0]\n",
    "            af_precision = metrics[1]\n",
    "            af_recall = metrics[2]\n",
    "            af_f1 = metrics[3]\n",
    "            af_th += 1\n",
    "        else:\n",
    "            found = True\n",
    "            \n",
    "    #absolute present\n",
    "    ap_accuracy = 0\n",
    "    ap_th = 0\n",
    "    found = False\n",
    "    while(found == False):\n",
    "        metrics = get_metrics(df, \"Absolute Present\", ap_th)\n",
    "        if metrics[0] > ap_accuracy:\n",
    "            ap_accuracy = metrics[0]\n",
    "            ap_precision = metrics[1]\n",
    "            ap_recall = metrics[2]\n",
    "            ap_f1 = metrics[3]\n",
    "            ap_th += 1\n",
    "        else:\n",
    "            found = True\n",
    "            \n",
    "    #relative frequency\n",
    "    rf_accuracy = 0\n",
    "    rf_th = 0\n",
    "    found = False\n",
    "    while(found == False):\n",
    "        metrics = get_metrics(df, \"Relative Frequency\", rf_th)\n",
    "        if metrics[0] > rf_accuracy:\n",
    "            rf_accuracy = metrics[0]\n",
    "            rf_precision = metrics[1]\n",
    "            rf_recall = metrics[2]\n",
    "            rf_f1 = metrics[3]\n",
    "            rf_th += 0.1\n",
    "        else:\n",
    "            found = True\n",
    "            \n",
    "    #relative present\n",
    "    rp_accuracy = 0\n",
    "    rp_th = 0\n",
    "    found = False\n",
    "    while(found == False):\n",
    "        metrics = get_metrics(df, \"Relative Present\", rp_th)\n",
    "        if metrics[0] > rp_accuracy:\n",
    "            rp_accuracy = metrics[0]\n",
    "            rp_precision = metrics[1]\n",
    "            rp_recall = metrics[2]\n",
    "            rp_f1 = metrics[3]\n",
    "            rp_th += 0.1\n",
    "        else:\n",
    "            found = True\n",
    "    \n",
    "    return(pd.DataFrame({\"Lexicon\" : [lexicon_name] * 4, \n",
    "                         \"Technique\" : [\"Absolute Frequency\", \"Absolute Present\", \"Relative Frequency\", \"Relative Present\"],\n",
    "                         \"Threshold\" : [af_th - 1, ap_th - 1, rf_th - 0.1, rp_th - 0.1], \n",
    "                 \"Accuracy\" : [af_accuracy, ap_accuracy, rf_accuracy, rp_accuracy], \n",
    "                        \"Precision\" : [af_precision, ap_precision, rf_precision, rp_precision], \n",
    "                        \"Recall\" : [af_recall, ap_recall, rf_recall, rp_recall], \n",
    "                        \"F1 Score\" : [af_f1, ap_f1, rf_f1, rp_f1]}))\n",
    "    \n",
    "def test_lexicon(test_df, results_df, lexicon, lexicon_name):\n",
    "    techniques = [\"Absolute Frequency\", \"Absolute Present\", \"Relative Frequency\", \"Relative Present\"]\n",
    "    df = count_lexicon_words(test_df, lexicon)\n",
    "    accuracy = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1 = []\n",
    "    th_df = []\n",
    "    for t in range(len(techniques)):\n",
    "        th = results_df[results_df[\"Technique\"] == techniques[t]][\"Threshold\"].iloc[0]\n",
    "        th_df.append(th)\n",
    "        accuracy.append(get_metrics(df, techniques[t], th)[0])\n",
    "        precision.append(get_metrics(df, techniques[t], th)[1])\n",
    "        recall.append(get_metrics(df, techniques[t], th)[2])\n",
    "        f1.append(get_metrics(df, techniques[t], th)[3])\n",
    "    \n",
    "    return(pd.DataFrame({\"Lexicon\" : [lexicon_name] * 4, \"Technique\" : techniques, \"Threshold\" : th_df ,\n",
    "                         \"Test Accuracy\" : accuracy, \n",
    "                        \"Test Precision\" : precision, \n",
    "                        \"Test Recall\" : recall, \n",
    "                        \"Test F1 Score\" : f1}))\n",
    "\n",
    "\n",
    "\n",
    "def get_cross_table(text_df, lexicon, threshold, colname, binary):\n",
    "    df = count_lexicon_words(text_df, lexicon)\n",
    "    return(get_gross_table_data(df, colname, threshold, binary))\n",
    "\n",
    "def get_most_words_used(text_df, lexicon):\n",
    "    lexicon = lexicon[\"Lexicon\"]\n",
    "    output = {}\n",
    "\n",
    "    for text in tqdm(text_df[\"Text\"]):\n",
    "        for word in lexicon:\n",
    "            if(text.lower().count(word.lower()) > 0):\n",
    "                if word in output:\n",
    "                    # Increment the value by 1\n",
    "                    output[word] += 1\n",
    "                else:\n",
    "                    # Add the value to the dictionary with an initial count of 1\n",
    "                    output[word] = 1\n",
    "\n",
    "    \n",
    "    return(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25c89c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary(df_with_text, name, model_name, max_lenght_input=-1):\n",
    "    data_in_list = df_with_text[name].tolist()\n",
    "    tokenizer_sum = AutoTokenizer.from_pretrained(model_name)\n",
    "    model_sum = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "    summarizer = pipeline('summarization', model=model_sum, tokenizer = tokenizer_sum) \n",
    "\n",
    "    if max_lenght_input>=0:\n",
    "        df_with_text['summary'] = summarizer(data_in_list, max_length=max_lenght_input)\n",
    "\n",
    "    else:\n",
    "        df_with_text['summary'] = summarizer(data_in_list)\n",
    "\n",
    "def classification(df_with_text, name, model_name, max_lenght_input=-1):\n",
    "    data_in_list = df_with_text[name].tolist()\n",
    "    tokenizer_clas = AutoTokenizer.from_pretrained(model_name)\n",
    "    model_clas = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "    classification = pipeline('text-classification', model=model_clas, tokenizer = tokenizer_clas) \n",
    "\n",
    "    if max_lenght_input>=0:\n",
    "        df_with_text['classification'] = classification(data_in_list, max_length=max_lenght_input, truncation=True)\n",
    "\n",
    "    else:\n",
    "        df_with_text['classification'] = classification(data_in_list)\n",
    "        \n",
    "    return(df_with_text)\n",
    "\n",
    "def get_metrics_hugging_face(text_df, text_column, model, tokens):\n",
    "    \n",
    "    df = classification(text_df, text_column, model, tokens)\n",
    "    label_list = list(df[\"classification\"])\n",
    "    labels = [entry['label'] for entry in label_list]\n",
    "    df[\"Label_Hugging\"] = labels\n",
    "    \n",
    "    cross_table = pd.crosstab(df['Target'], df['Label_Hugging'], margins=True)\n",
    "    \n",
    "    # calculate classification metrics using scikit-learn\n",
    "    accuracy = (cross_table.iloc[0, 0] + cross_table.iloc[1, 1]) / cross_table.loc['All', 'All'] if cross_table.shape == (3,3) else cross_table.iloc[1,0] / (cross_table.iloc[1, 0] + cross_table.iloc[0,0]) \n",
    "    precision = cross_table.iloc[1,1] / (cross_table.iloc[0,1] + cross_table.iloc[1,1]) if cross_table.shape == (3,3) else 0\n",
    "    recall = cross_table.iloc[1,1] / (cross_table.iloc[1,0] + cross_table.iloc[1,1]) if cross_table.shape == (3,3) != 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "\n",
    "\n",
    "    # print the metrics\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1 score:\", f1_score)\n",
    "    print(cross_table)\n",
    "    print(\"\\n\")\n",
    "    print(pd.crosstab(df['Final_Climate_Change_Level_Label'], df['Label_Hugging'], margins=True))\n",
    "    \n",
    "\n",
    "def get_metrics_df_hugging_face(text_df, text_column, model, tokens, model_name):\n",
    "    df = classification(text_df, text_column, model, tokens)\n",
    "    label_list = list(df[\"classification\"])\n",
    "    labels = [entry['label'] for entry in label_list]\n",
    "    df[\"Label_Hugging\"] = labels\n",
    "    \n",
    "    cross_table = pd.crosstab(df['Target'], df['Label_Hugging'], margins=True)\n",
    "    \n",
    "    # calculate classification metrics using scikit-learn\n",
    "    accuracy = (cross_table.iloc[0, 0] + cross_table.iloc[1, 1]) / cross_table.loc['All', 'All'] if cross_table.shape == (3,3) else cross_table.iloc[1,0] / (cross_table.iloc[1, 0] + cross_table.iloc[0,0]) \n",
    "    precision = cross_table.iloc[1,1] / (cross_table.iloc[0,1] + cross_table.iloc[1,1]) if cross_table.shape == (3,3) else 0\n",
    "    recall = cross_table.iloc[1,1] / (cross_table.iloc[1,0] + cross_table.iloc[1,1]) if cross_table.shape == (3,3) != 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "    \n",
    "    return(pd.DataFrame({\"Model\" : [model_name], \"Accuracy\" : [accuracy], \"Precision\" : [precision], \"Recall\" : [recall],\n",
    "                         \"F1 Score\" : [f1_score]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfa34b3",
   "metadata": {},
   "source": [
    "# 1. Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2bf7db0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Link</th>\n",
       "      <th>Final_Climate_Change_Level_Label</th>\n",
       "      <th>Final_Sentiment_Label</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>On a Train trip north toward Aberdeen, the Sc...</td>\n",
       "      <td>https://www.wsj.com/articles/surfacing-review-...</td>\n",
       "      <td>Medium</td>\n",
       "      <td>-1</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A container area at the Yangshan Deep Water Po...</td>\n",
       "      <td>https://www.washingtonpost.com/news/monkey-cag...</td>\n",
       "      <td>No Climate</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This week, New York City observed an annual r...</td>\n",
       "      <td>https://www.wsj.com/articles/lets-get-the-un-o...</td>\n",
       "      <td>No Climate</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>During a visit to Detroit last year, Presiden...</td>\n",
       "      <td>https://www.wsj.com/articles/make-cars-great-a...</td>\n",
       "      <td>High</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Investors holding more than $5 billion in Exx...</td>\n",
       "      <td>http://www.wsj.com/articles/calpers-pushes-exx...</td>\n",
       "      <td>High</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>No spending cuts to Medicaid? Then no tax cuts...</td>\n",
       "      <td>https://www.washingtonpost.com/news/fact-check...</td>\n",
       "      <td>No Climate</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>The U.S. dollar edged higher Tuesday, maintai...</td>\n",
       "      <td>https://www.wsj.com/articles/u-s-dollar-edges-...</td>\n",
       "      <td>No Climate</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>Lots of today’s board games are just jazzed-u...</td>\n",
       "      <td>http://www.wsj.com/articles/the-many-guises-of...</td>\n",
       "      <td>No Climate</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>The new $1,000 iPhone 11 Pro and the $1,100 iP...</td>\n",
       "      <td>https://www.washingtonpost.com/technology/2019...</td>\n",
       "      <td>No Climate</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>Oil driller Transocean Ltd. swung to a first-...</td>\n",
       "      <td>http://www.wsj.com/articles/transocean-swings-...</td>\n",
       "      <td>No Climate</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Text  \\\n",
       "0     On a Train trip north toward Aberdeen, the Sc...   \n",
       "1    A container area at the Yangshan Deep Water Po...   \n",
       "2     This week, New York City observed an annual r...   \n",
       "3     During a visit to Detroit last year, Presiden...   \n",
       "4     Investors holding more than $5 billion in Exx...   \n",
       "..                                                 ...   \n",
       "495  No spending cuts to Medicaid? Then no tax cuts...   \n",
       "496   The U.S. dollar edged higher Tuesday, maintai...   \n",
       "497   Lots of today’s board games are just jazzed-u...   \n",
       "498  The new $1,000 iPhone 11 Pro and the $1,100 iP...   \n",
       "499   Oil driller Transocean Ltd. swung to a first-...   \n",
       "\n",
       "                                                  Link  \\\n",
       "0    https://www.wsj.com/articles/surfacing-review-...   \n",
       "1    https://www.washingtonpost.com/news/monkey-cag...   \n",
       "2    https://www.wsj.com/articles/lets-get-the-un-o...   \n",
       "3    https://www.wsj.com/articles/make-cars-great-a...   \n",
       "4    http://www.wsj.com/articles/calpers-pushes-exx...   \n",
       "..                                                 ...   \n",
       "495  https://www.washingtonpost.com/news/fact-check...   \n",
       "496  https://www.wsj.com/articles/u-s-dollar-edges-...   \n",
       "497  http://www.wsj.com/articles/the-many-guises-of...   \n",
       "498  https://www.washingtonpost.com/technology/2019...   \n",
       "499  http://www.wsj.com/articles/transocean-swings-...   \n",
       "\n",
       "    Final_Climate_Change_Level_Label  Final_Sentiment_Label Target  \n",
       "0                             Medium                     -1    Yes  \n",
       "1                         No Climate                      0     No  \n",
       "2                         No Climate                      0     No  \n",
       "3                               High                      1    Yes  \n",
       "4                               High                      1    Yes  \n",
       "..                               ...                    ...    ...  \n",
       "495                       No Climate                      0     No  \n",
       "496                       No Climate                      0     No  \n",
       "497                       No Climate                      0     No  \n",
       "498                       No Climate                      0     No  \n",
       "499                       No Climate                      0     No  \n",
       "\n",
       "[500 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_climate_df = pd.read_parquet(\"Climate_Labels_Dataset.parquet\")\n",
    "\n",
    "#Clean the tabel\n",
    "tag_climate_df['Final_Climate_Change_Level_Label'] = tag_climate_df['Final_Climate_Change_Level_Label'].str.strip()\n",
    "tag_climate_df.loc[tag_climate_df[\"Final_Climate_Change_Level_Label\"] == \"NA\", \"Final_Climate_Change_Level_Label\"] = \"Na\"\n",
    "tag_climate_df.loc[tag_climate_df[\"Final_Climate_Change_Level_Label\"] == \"0\", \"Final_Climate_Change_Level_Label\"] = \"Na\"\n",
    "tag_climate_df.loc[tag_climate_df[\"Final_Climate_Change_Level_Label\"] == \"Na\", \"Final_Climate_Change_Level_Label\"] = \"No Climate\"\n",
    "tag_climate_df[\"Target\"] = tag_climate_df[\"Final_Climate_Change_Level_Label\"].apply(lambda x: \"Yes\" if x in [\"High\", \"Medium\"] else \"No\")\n",
    "tag_climate_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5418e92f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Final_Climate_Change_Level_Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>High</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Medium</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No Climate</td>\n",
       "      <td>307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Small</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Final_Climate_Change_Level_Label  Text\n",
       "0                             High    57\n",
       "1                           Medium    42\n",
       "2                       No Climate   307\n",
       "3                            Small    94"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overview_labels_hms = tag_climate_df.groupby(\"Final_Climate_Change_Level_Label\")[\"Text\"].count().reset_index()\n",
    "overview_labels_hms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b25c3c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No</td>\n",
       "      <td>401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yes</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Target  Text\n",
       "0     No   401\n",
       "1    Yes    99"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overview_labels = tag_climate_df.groupby(\"Target\")[\"Text\"].count().reset_index()\n",
    "overview_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d5bef2",
   "metadata": {},
   "source": [
    "## Splits in Train en Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5931a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataframe into two sets\n",
    "df_train, df_test = train_test_split(tag_climate_df, test_size = 0.3, random_state = 23)\n",
    "df_train = df_train.reset_index(drop = True)\n",
    "df_test = df_test.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0765752f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Final_Climate_Change_Level_Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>High</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Medium</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No Climate</td>\n",
       "      <td>213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Small</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Final_Climate_Change_Level_Label  Text\n",
       "0                             High    38\n",
       "1                           Medium    35\n",
       "2                       No Climate   213\n",
       "3                            Small    64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.groupby(\"Final_Climate_Change_Level_Label\")[\"Text\"].count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6a621a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No</td>\n",
       "      <td>277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yes</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Target  Text\n",
       "0     No   277\n",
       "1    Yes    73"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.groupby(\"Target\")[\"Text\"].count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd3722a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Final_Climate_Change_Level_Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>High</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Medium</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No Climate</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Small</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Final_Climate_Change_Level_Label  Text\n",
       "0                             High    19\n",
       "1                           Medium     7\n",
       "2                       No Climate    94\n",
       "3                            Small    30"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.groupby(\"Final_Climate_Change_Level_Label\")[\"Text\"].count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3320d1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yes</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Target  Text\n",
       "0     No   124\n",
       "1    Yes    26"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.groupby(\"Target\")[\"Text\"].count().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ad56cb",
   "metadata": {},
   "source": [
    "# 2. Lexicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54fc2163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lexicon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>weather</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>climate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>greenhouse gases</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>greenhouse gas emmisions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>global warming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>climate change</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>climate crisis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>feedback loop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>tipping point</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>climate overshoot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>mitigation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>adaptation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>resilience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>carbon footprint</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>climate justice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>nature-based solutions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>indigenous knowledge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>loss and damage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>climate security</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>climate finance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>net zero</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>decarbonization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>renewable energy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>carbon sink</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>carbon removal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>carbon capture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>carbon markets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>regenerative agriculture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>reforestation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>afforestation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>rewilding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>circular economy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>blue economy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>green jobs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>greenwashing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>just transition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>unfccc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>conference of the parties</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>cop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>paris agreement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>nationally determined contributions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>transparent reporting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>transparency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>national adaptation plans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>long-term strategies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>intergovernmental panel on climate change</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Lexicon\n",
       "0                                     weather\n",
       "1                                     climate\n",
       "2                            greenhouse gases\n",
       "3                    greenhouse gas emmisions\n",
       "4                              global warming\n",
       "5                              climate change\n",
       "6                              climate crisis\n",
       "7                               feedback loop\n",
       "8                               tipping point\n",
       "9                           climate overshoot\n",
       "10                                 mitigation\n",
       "11                                 adaptation\n",
       "12                                 resilience\n",
       "13                           carbon footprint\n",
       "14                            climate justice\n",
       "15                     nature-based solutions\n",
       "16                       indigenous knowledge\n",
       "17                            loss and damage\n",
       "18                           climate security\n",
       "19                            climate finance\n",
       "20                                   net zero\n",
       "21                            decarbonization\n",
       "22                           renewable energy\n",
       "23                                carbon sink\n",
       "24                             carbon removal\n",
       "25                             carbon capture\n",
       "26                             carbon markets\n",
       "27                   regenerative agriculture\n",
       "28                              reforestation\n",
       "29                              afforestation\n",
       "30                                  rewilding\n",
       "31                           circular economy\n",
       "32                               blue economy\n",
       "33                                 green jobs\n",
       "34                               greenwashing\n",
       "35                            just transition\n",
       "36                                    unfccc \n",
       "37                  conference of the parties\n",
       "38                                       cop \n",
       "39                            paris agreement\n",
       "40        nationally determined contributions\n",
       "41                      transparent reporting\n",
       "42                               transparency\n",
       "43                  national adaptation plans\n",
       "44                       long-term strategies\n",
       "45  intergovernmental panel on climate change"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the lexicon\n",
    "UNDP_Lexicon = pd.read_csv(\"Lexicons/UNDP_Lexicon\")\n",
    "UNDP_Lexicon = UNDP_Lexicon.drop_duplicates().reset_index(drop = True)\n",
    "UNDP_Lexicon[\"Lexicon\"] = UNDP_Lexicon[\"Lexicon\"].str.lower()\n",
    "UNDP_Lexicon = pd.DataFrame(UNDP_Lexicon[\"Lexicon\"])\n",
    "UNDP_Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dabba077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lexicon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>acceptability of policy or system change</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adaptability</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>adaptation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>adaptation behaviour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>adaptation limits</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>for climate change mitigation and adaptation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>sea level rise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>sea level fall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>social</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>unfccc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>343 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Lexicon\n",
       "0        acceptability of policy or system change\n",
       "1                                    adaptability\n",
       "2                                      adaptation\n",
       "3                            adaptation behaviour\n",
       "4                               adaptation limits\n",
       "..                                            ...\n",
       "338  for climate change mitigation and adaptation\n",
       "339                                sea level rise\n",
       "340                                sea level fall\n",
       "341                                        social\n",
       "342                                       unfccc \n",
       "\n",
       "[343 rows x 1 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the lexicon\n",
    "IPCC_Lexicon = pd.read_csv(\"Lexicons/IPCC_Lexicon\")\n",
    "IPCC_Lexicon = IPCC_Lexicon.drop_duplicates().reset_index(drop = True)\n",
    "IPCC_Lexicon[\"Lexicon\"] = IPCC_Lexicon[\"Lexicon\"].str.lower()\n",
    "IPCC_Lexicon = pd.DataFrame(IPCC_Lexicon[\"Lexicon\"])\n",
    "IPCC_Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a69ff34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lexicon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abrupt climate change</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adaptation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>adaptive capacity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aerosols</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>afforestation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>wastewater</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>water vapor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>weather</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>100-year flood levels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>or earth system</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>153 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Lexicon\n",
       "0    abrupt climate change\n",
       "1               adaptation\n",
       "2        adaptive capacity\n",
       "3                 aerosols\n",
       "4            afforestation\n",
       "..                     ...\n",
       "148             wastewater\n",
       "149            water vapor\n",
       "150                weather\n",
       "151  100-year flood levels\n",
       "152        or earth system\n",
       "\n",
       "[153 rows x 1 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the lexicon\n",
    "EPA_Lexicon = pd.read_csv(\"Lexicons/EPA_Lexicon\")\n",
    "EPA_Lexicon = EPA_Lexicon.drop_duplicates().reset_index(drop = True)\n",
    "EPA_Lexicon[\"Lexicon\"] = EPA_Lexicon[\"Lexicon\"].str.lower()\n",
    "EPA_Lexicon = pd.DataFrame(EPA_Lexicon[\"Lexicon\"])\n",
    "\n",
    "EPA_Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e55e2ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lexicon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100,000-year problem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adaptation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>additionality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>albedo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>anoxic event</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>volcanism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>water vapor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>weather</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>world climate report</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>younger dryas</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>164 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Lexicon\n",
       "0    100,000-year problem\n",
       "1              adaptation\n",
       "2           additionality\n",
       "3                  albedo\n",
       "4            anoxic event\n",
       "..                    ...\n",
       "159             volcanism\n",
       "160           water vapor\n",
       "161               weather\n",
       "162  world climate report\n",
       "163         younger dryas\n",
       "\n",
       "[164 rows x 1 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the lexicon\n",
    "Wikipedia_Lexicon = pd.read_csv(\"Lexicons/Wikipedia_Lexicon\")\n",
    "Wikipedia_Lexicon = Wikipedia_Lexicon.drop_duplicates().reset_index(drop = True)\n",
    "Wikipedia_Lexicon[\"Lexicon\"] = Wikipedia_Lexicon[\"Lexicon\"].str.lower()\n",
    "Wikipedia_Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2627bec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lexicon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100-year flood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>emissions scenario</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>adaptation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>adaptation science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>adaptive capacity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>vector</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>vulnerability</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>vulnerability assessment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>water security</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>water stress</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>105 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Lexicon\n",
       "0              100-year flood\n",
       "1          emissions scenario\n",
       "2                  adaptation\n",
       "3          adaptation science\n",
       "4           adaptive capacity\n",
       "..                        ...\n",
       "100                    vector\n",
       "101             vulnerability\n",
       "102  vulnerability assessment\n",
       "103            water security\n",
       "104              water stress\n",
       "\n",
       "[105 rows x 1 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the lexicon\n",
    "Global_Change_Lexicon = pd.read_csv(\"Lexicons/Global_Change_Lexicon\")\n",
    "Global_Change_Lexicon = Global_Change_Lexicon.drop_duplicates().reset_index(drop = True)\n",
    "Global_Change_Lexicon[\"Lexicon\"] = Global_Change_Lexicon[\"Lexicon\"].str.lower()\n",
    "Global_Change_Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "61bc5049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lexicon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adaptation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adaptation fund</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>annex i countries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>annex ii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>anthropogenic climate change</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>technology transfer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>tipping point</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>twenty-twenty-twenty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>350/450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>20-20-20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Lexicon\n",
       "0                     adaptation\n",
       "1                adaptation fund\n",
       "2              annex i countries\n",
       "3                       annex ii\n",
       "4   anthropogenic climate change\n",
       "..                           ...\n",
       "57           technology transfer\n",
       "58                 tipping point\n",
       "59          twenty-twenty-twenty\n",
       "60                       350/450\n",
       "61                      20-20-20\n",
       "\n",
       "[62 rows x 1 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the lexicon\n",
    "BBC_Lexicon = pd.read_csv(\"Lexicons/BBC_Lexicon\")\n",
    "BBC_Lexicon = BBC_Lexicon.drop_duplicates().reset_index(drop = True)\n",
    "BBC_Lexicon[\"Lexicon\"] = BBC_Lexicon[\"Lexicon\"].str.lower()\n",
    "\n",
    "BBC_Lexicon = pd.DataFrame(BBC_Lexicon[\"Lexicon\"])\n",
    "BBC_Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dca15ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([EPA_Lexicon, BBC_Lexicon]).drop_duplicates().reset_index(drop = True).to_csv(\"Lexicons/EPA_BBC_Lexicon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62316160",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lexicon</th>\n",
       "      <th>Global Change</th>\n",
       "      <th>IPCC</th>\n",
       "      <th>Wikipedia</th>\n",
       "      <th>EPA</th>\n",
       "      <th>BBC</th>\n",
       "      <th>UNDP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Global Change</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.110787</td>\n",
       "      <td>0.073171</td>\n",
       "      <td>0.130719</td>\n",
       "      <td>0.177419</td>\n",
       "      <td>0.152174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IPCC</td>\n",
       "      <td>0.361905</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.170732</td>\n",
       "      <td>0.261438</td>\n",
       "      <td>0.306452</td>\n",
       "      <td>0.391304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.114286</td>\n",
       "      <td>0.081633</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.215686</td>\n",
       "      <td>0.241935</td>\n",
       "      <td>0.217391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EPA</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.116618</td>\n",
       "      <td>0.201220</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.225806</td>\n",
       "      <td>0.282609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BBC</td>\n",
       "      <td>0.104762</td>\n",
       "      <td>0.055394</td>\n",
       "      <td>0.091463</td>\n",
       "      <td>0.091503</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.217391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>UNDP</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.052478</td>\n",
       "      <td>0.060976</td>\n",
       "      <td>0.084967</td>\n",
       "      <td>0.161290</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Lexicon  Global Change      IPCC  Wikipedia       EPA       BBC  \\\n",
       "0  Global Change       1.000000  0.110787   0.073171  0.130719  0.177419   \n",
       "1           IPCC       0.361905  1.000000   0.170732  0.261438  0.306452   \n",
       "2      Wikipedia       0.114286  0.081633   1.000000  0.215686  0.241935   \n",
       "3            EPA       0.190476  0.116618   0.201220  1.000000  0.225806   \n",
       "4            BBC       0.104762  0.055394   0.091463  0.091503  1.000000   \n",
       "5           UNDP       0.066667  0.052478   0.060976  0.084967  0.161290   \n",
       "\n",
       "       UNDP  \n",
       "0  0.152174  \n",
       "1  0.391304  \n",
       "2  0.217391  \n",
       "3  0.282609  \n",
       "4  0.217391  \n",
       "5  1.000000  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create an empty dataframe and write a function to fill with the values\n",
    "\n",
    "common_words_df = pd.DataFrame({\"Lexicon\" : [\"Global Change\", \"IPCC\", \"Wikipedia\", \"EPA\", \"BBC\", \"UNDP\"], \n",
    "                               \"Global Change\": [0, 0, 0, 0, 0, 0], \"IPCC\" : [0, 0, 0, 0, 0, 0], \n",
    "                               \"Wikipedia\" : [0, 0, 0, 0, 0, 0], \"EPA\" : [0, 0, 0, 0, 0, 0], \n",
    "                               \"BBC\" : [0, 0, 0, 0, 0, 0], \"UNDP\" : [0, 0, 0, 0, 0, 0]})\n",
    "\n",
    "dfs = [Global_Change_Lexicon, IPCC_Lexicon, Wikipedia_Lexicon, EPA_Lexicon, BBC_Lexicon, UNDP_Lexicon]\n",
    "\n",
    "for r in range(0, len(dfs)):\n",
    "    for c in range(0, len(dfs)):\n",
    "        # Get the common values between the two columns\n",
    "        common_words = set(dfs[r]['Lexicon']).intersection(set(dfs[c]['Lexicon']))\n",
    "        common_words_df.loc[r, common_words_df.columns[c +1]] = len(common_words)/len(dfs[c]['Lexicon'])\n",
    "\n",
    "common_words_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b01b5d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lexicon</th>\n",
       "      <th>Global Change</th>\n",
       "      <th>IPCC</th>\n",
       "      <th>Wikipedia</th>\n",
       "      <th>EPA</th>\n",
       "      <th>BBC</th>\n",
       "      <th>UNDP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Global Change</td>\n",
       "      <td>105</td>\n",
       "      <td>38</td>\n",
       "      <td>12</td>\n",
       "      <td>20</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IPCC</td>\n",
       "      <td>38</td>\n",
       "      <td>343</td>\n",
       "      <td>28</td>\n",
       "      <td>40</td>\n",
       "      <td>19</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>12</td>\n",
       "      <td>28</td>\n",
       "      <td>164</td>\n",
       "      <td>33</td>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EPA</td>\n",
       "      <td>20</td>\n",
       "      <td>40</td>\n",
       "      <td>33</td>\n",
       "      <td>153</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BBC</td>\n",
       "      <td>11</td>\n",
       "      <td>19</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>62</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>UNDP</td>\n",
       "      <td>7</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Lexicon  Global Change  IPCC  Wikipedia  EPA  BBC  UNDP\n",
       "0  Global Change            105    38         12   20   11     7\n",
       "1           IPCC             38   343         28   40   19    18\n",
       "2      Wikipedia             12    28        164   33   15    10\n",
       "3            EPA             20    40         33  153   14    13\n",
       "4            BBC             11    19         15   14   62    10\n",
       "5           UNDP              7    18         10   13   10    46"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create an empty dataframe and write a function to fill with the values\n",
    "\n",
    "common_words_df = pd.DataFrame({\"Lexicon\" : [\"Global Change\", \"IPCC\", \"Wikipedia\", \"EPA\", \"BBC\", \"UNDP\"], \n",
    "                               \"Global Change\": [0, 0, 0, 0, 0, 0], \"IPCC\" : [0, 0, 0, 0, 0, 0], \n",
    "                               \"Wikipedia\" : [0, 0, 0, 0, 0, 0], \"EPA\" : [0, 0, 0, 0, 0, 0], \n",
    "                               \"BBC\" : [0, 0, 0, 0, 0, 0], \"UNDP\" : [0, 0, 0, 0, 0, 0]})\n",
    "\n",
    "dfs = [Global_Change_Lexicon, IPCC_Lexicon, Wikipedia_Lexicon, EPA_Lexicon, BBC_Lexicon, UNDP_Lexicon]\n",
    "\n",
    "for r in range(0, len(dfs)):\n",
    "    for c in range(0, len(dfs)):\n",
    "        # Get the common values between the two columns\n",
    "        common_words = set(dfs[r]['Lexicon']).intersection(set(dfs[c]['Lexicon']))\n",
    "        common_words_df.loc[r, common_words_df.columns[c +1]] = len(common_words)\n",
    "\n",
    "common_words_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1106d880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lexicon</th>\n",
       "      <th>Non unique words</th>\n",
       "      <th>unique words</th>\n",
       "      <th>total_words</th>\n",
       "      <th>Richness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Global Change</td>\n",
       "      <td>45</td>\n",
       "      <td>60</td>\n",
       "      <td>105</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IPCC</td>\n",
       "      <td>80</td>\n",
       "      <td>263</td>\n",
       "      <td>343</td>\n",
       "      <td>0.766764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>47</td>\n",
       "      <td>117</td>\n",
       "      <td>164</td>\n",
       "      <td>0.713415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EPA</td>\n",
       "      <td>59</td>\n",
       "      <td>94</td>\n",
       "      <td>153</td>\n",
       "      <td>0.614379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BBC</td>\n",
       "      <td>30</td>\n",
       "      <td>32</td>\n",
       "      <td>62</td>\n",
       "      <td>0.516129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>UNDP</td>\n",
       "      <td>25</td>\n",
       "      <td>21</td>\n",
       "      <td>46</td>\n",
       "      <td>0.456522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Lexicon  Non unique words  unique words  total_words  Richness\n",
       "0  Global Change                45            60          105  0.571429\n",
       "1           IPCC                80           263          343  0.766764\n",
       "2      Wikipedia                47           117          164  0.713415\n",
       "3            EPA                59            94          153  0.614379\n",
       "4            BBC                30            32           62  0.516129\n",
       "5           UNDP                25            21           46  0.456522"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs = [Global_Change_Lexicon, IPCC_Lexicon, Wikipedia_Lexicon, EPA_Lexicon, BBC_Lexicon, UNDP_Lexicon]\n",
    "non_unique_words = []\n",
    "unique_words = []\n",
    "total_words = []\n",
    "for r in range(len(dfs)):\n",
    "    common_words = []\n",
    "    for c in range(len(dfs)):\n",
    "        if c != r:\n",
    "            # Get the common values between the two columns\n",
    "            common_words.extend(list(set(dfs[r]['Lexicon']).intersection(set(dfs[c]['Lexicon']))))\n",
    "    common_words = list(set(common_words))  # Remove duplicates by converting to a set and back to a list\n",
    "    total_words.append(len(dfs[r][\"Lexicon\"]))\n",
    "    unique_words.append(len(dfs[r][\"Lexicon\"]) - len(common_words))\n",
    "    non_unique_words.append(len(common_words))\n",
    "\n",
    "non_unique_words\n",
    "\n",
    "unique_words_df = pd.DataFrame({\"Lexicon\" : [\"Global Change\", \"IPCC\", \"Wikipedia\", \"EPA\", \"BBC\", \"UNDP\"], \n",
    "                                \"Non unique words\" : non_unique_words, \"unique words\" : unique_words, \n",
    "                               \"total_words\" : total_words})\n",
    "\n",
    "unique_words_df[\"Richness\"] = unique_words_df[\"unique words\"] / unique_words_df[\"total_words\"]\n",
    "\n",
    "unique_words_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fb7b13",
   "metadata": {},
   "source": [
    "# 3. Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6340f21",
   "metadata": {},
   "source": [
    "## 3.1. Lexicons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518f9459",
   "metadata": {},
   "source": [
    "### 3.1.1. Train Lexicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e42af9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Text_df = df_train.copy()\n",
    "Text_df[\"Text\"] = Text_df[\"Text\"].apply(preprocess_text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3ebadcc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.2932164669036865 seconds ---\n",
      "--- 1741.7407014369965 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "names = [\"IPCC\", \"Global_Change\", \"UNDP\", \"EPA\", \"Wikipedia\", \"BBC\"]\n",
    "All_names = []\n",
    "# Generate all combinations\n",
    "all_combinations = []\n",
    "for r in range(1, len(names) + 1):\n",
    "    combinations_r = combinations(names, r)\n",
    "    all_combinations.extend(combinations_r)\n",
    "\n",
    "# Print all combinations\n",
    "for combination in all_combinations:\n",
    "    combined_string = \"_\".join(combination)\n",
    "    All_names.append(combined_string)\n",
    "    \n",
    "Lexicons = [IPCC_Lexicon, Global_Change_Lexicon, UNDP_Lexicon, EPA_Lexicon, Wikipedia_Lexicon, BBC_Lexicon]\n",
    "All_Lexicons = []\n",
    "# Generate all combinations\n",
    "all_combinations = []\n",
    "for r in range(1, len(Lexicons) + 1):\n",
    "    combinations_r = combinations(Lexicons, r)\n",
    "    all_combinations.extend(combinations_r)\n",
    "\n",
    "# Concatenate and print all combinations\n",
    "for combination in all_combinations:\n",
    "    combined_df = pd.concat(combination, axis=0).drop_duplicates().reset_index()\n",
    "    All_Lexicons.append(combined_df)\n",
    "    \n",
    "results_df = pd.DataFrame()\n",
    "\n",
    "start_results = time.time()\n",
    "\n",
    "print(\"--- %s seconds ---\" % (start_results - start))\n",
    "\n",
    "for i in range(len(All_Lexicons)):\n",
    "    r_df = find_optimal_threshold(Text_df, All_Lexicons[i], All_names[i])\n",
    "    results_df = pd.concat([results_df, r_df])\n",
    "\n",
    "results_df = results_df.reset_index(drop = True)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "82c26fee",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lexicon</th>\n",
       "      <th>Technique</th>\n",
       "      <th>Threshold</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>EPA_BBC</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.739726</td>\n",
       "      <td>0.812030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>EPA_Wikipedia_BBC</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.739726</td>\n",
       "      <td>0.812030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>EPA_BBC</td>\n",
       "      <td>Relative Frequency</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.925714</td>\n",
       "      <td>0.790123</td>\n",
       "      <td>0.876712</td>\n",
       "      <td>0.831169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>UNDP_EPA_Wikipedia</td>\n",
       "      <td>Relative Frequency</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.925714</td>\n",
       "      <td>0.813333</td>\n",
       "      <td>0.835616</td>\n",
       "      <td>0.824324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>EPA</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.925714</td>\n",
       "      <td>0.861538</td>\n",
       "      <td>0.767123</td>\n",
       "      <td>0.811594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>IPCC_Wikipedia_BBC</td>\n",
       "      <td>Absolute Present</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.834286</td>\n",
       "      <td>0.572816</td>\n",
       "      <td>0.808219</td>\n",
       "      <td>0.670455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IPCC</td>\n",
       "      <td>Absolute Present</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.834286</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.753425</td>\n",
       "      <td>0.654762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>IPCC_Wikipedia</td>\n",
       "      <td>Absolute Present</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.834286</td>\n",
       "      <td>0.574257</td>\n",
       "      <td>0.794521</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>IPCC_BBC</td>\n",
       "      <td>Absolute Present</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.834286</td>\n",
       "      <td>0.577320</td>\n",
       "      <td>0.767123</td>\n",
       "      <td>0.658824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>IPCC_Global_Change</td>\n",
       "      <td>Absolute Present</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.822857</td>\n",
       "      <td>0.566265</td>\n",
       "      <td>0.643836</td>\n",
       "      <td>0.602564</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>252 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Lexicon           Technique  Threshold  Accuracy  Precision  \\\n",
       "76              EPA_BBC  Absolute Frequency        7.0  0.928571   0.900000   \n",
       "160   EPA_Wikipedia_BBC  Absolute Frequency        7.0  0.928571   0.900000   \n",
       "78              EPA_BBC  Relative Frequency        0.5  0.925714   0.790123   \n",
       "150  UNDP_EPA_Wikipedia  Relative Frequency        0.6  0.925714   0.813333   \n",
       "12                  EPA  Absolute Frequency        6.0  0.925714   0.861538   \n",
       "..                  ...                 ...        ...       ...        ...   \n",
       "121  IPCC_Wikipedia_BBC    Absolute Present        6.0  0.834286   0.572816   \n",
       "1                  IPCC    Absolute Present        6.0  0.834286   0.578947   \n",
       "37       IPCC_Wikipedia    Absolute Present        6.0  0.834286   0.574257   \n",
       "41             IPCC_BBC    Absolute Present        6.0  0.834286   0.577320   \n",
       "25   IPCC_Global_Change    Absolute Present        7.0  0.822857   0.566265   \n",
       "\n",
       "       Recall  F1 Score  \n",
       "76   0.739726  0.812030  \n",
       "160  0.739726  0.812030  \n",
       "78   0.876712  0.831169  \n",
       "150  0.835616  0.824324  \n",
       "12   0.767123  0.811594  \n",
       "..        ...       ...  \n",
       "121  0.808219  0.670455  \n",
       "1    0.753425  0.654762  \n",
       "37   0.794521  0.666667  \n",
       "41   0.767123  0.658824  \n",
       "25   0.643836  0.602564  \n",
       "\n",
       "[252 rows x 7 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.sort_values(by = \"Accuracy\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4e93d6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_parquet(\"Classification_Results\\Lexicon_Tagging_Train_Results.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68434da",
   "metadata": {},
   "source": [
    "### 3.1.2. Test Lexicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0f74ff59",
   "metadata": {},
   "outputs": [],
   "source": [
    "Text_df_test = df_test.copy()\n",
    "Text_df_test[\"Text\"] = Text_df_test[\"Text\"].apply(preprocess_text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "49cd1c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.16655516624450684 seconds ---\n",
      "--- 600.1615781784058 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "names = [\"IPCC\", \"Global_Change\", \"UNDP\", \"EPA\", \"Wikipedia\", \"BBC\"]\n",
    "All_names = []\n",
    "# Generate all combinations\n",
    "all_combinations = []\n",
    "for r in range(1, len(names) + 1):\n",
    "    combinations_r = combinations(names, r)\n",
    "    all_combinations.extend(combinations_r)\n",
    "\n",
    "# Print all combinations\n",
    "for combination in all_combinations:\n",
    "    combined_string = \"_\".join(combination)\n",
    "    All_names.append(combined_string)\n",
    "    \n",
    "Lexicons = [IPCC_Lexicon, Global_Change_Lexicon, UNDP_Lexicon, EPA_Lexicon, Wikipedia_Lexicon, BBC_Lexicon]\n",
    "All_Lexicons = []\n",
    "# Generate all combinations\n",
    "all_combinations = []\n",
    "for r in range(1, len(Lexicons) + 1):\n",
    "    combinations_r = combinations(Lexicons, r)\n",
    "    all_combinations.extend(combinations_r)\n",
    "\n",
    "# Concatenate and print all combinations\n",
    "for combination in all_combinations:\n",
    "    combined_df = pd.concat(combination, axis=0).drop_duplicates().reset_index()\n",
    "    All_Lexicons.append(combined_df)\n",
    "    \n",
    "test_result_df = pd.DataFrame()\n",
    "\n",
    "start_results = time.time()\n",
    "\n",
    "print(\"--- %s seconds ---\" % (start_results - start))\n",
    "\n",
    "for n in range(0, len(All_names)):\n",
    "    test_lexicon_result = test_lexicon(Text_df_test, results_df[results_df[\"Lexicon\"] == All_names[n]], All_Lexicons[All_names.index(All_names[n])], All_names[n])\n",
    "    test_result_df = pd.concat([test_result_df, test_lexicon_result])\n",
    "    \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "38fdeef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lexicon</th>\n",
       "      <th>Technique</th>\n",
       "      <th>Threshold</th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Test Precision</th>\n",
       "      <th>Test Recall</th>\n",
       "      <th>Test F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EPA_BBC</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.901961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UNDP_EPA_BBC</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.892857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UNDP_EPA_Wikipedia_BBC</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UNDP_EPA_Wikipedia</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EPA_Wikipedia_BBC</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IPCC</td>\n",
       "      <td>Absolute Present</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.793333</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.563380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IPCC_UNDP</td>\n",
       "      <td>Absolute Present</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.793333</td>\n",
       "      <td>0.446809</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.575342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IPCC_UNDP_EPA_BBC</td>\n",
       "      <td>Absolute Present</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.793333</td>\n",
       "      <td>0.448980</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.586667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IPCC_EPA</td>\n",
       "      <td>Absolute Present</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.793333</td>\n",
       "      <td>0.448980</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.586667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IPCC_UNDP_Wikipedia_BBC</td>\n",
       "      <td>Absolute Present</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.793333</td>\n",
       "      <td>0.446809</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.575342</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>252 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Lexicon           Technique  Threshold  Test Accuracy  \\\n",
       "0                   EPA_BBC  Absolute Frequency        7.0       0.966667   \n",
       "0              UNDP_EPA_BBC  Absolute Frequency        6.0       0.960000   \n",
       "0    UNDP_EPA_Wikipedia_BBC  Absolute Frequency        7.0       0.960000   \n",
       "0        UNDP_EPA_Wikipedia  Absolute Frequency        6.0       0.960000   \n",
       "0         EPA_Wikipedia_BBC  Absolute Frequency        7.0       0.960000   \n",
       "..                      ...                 ...        ...            ...   \n",
       "1                      IPCC    Absolute Present        6.0       0.793333   \n",
       "1                 IPCC_UNDP    Absolute Present        6.0       0.793333   \n",
       "1         IPCC_UNDP_EPA_BBC    Absolute Present        6.0       0.793333   \n",
       "1                  IPCC_EPA    Absolute Present        6.0       0.793333   \n",
       "1   IPCC_UNDP_Wikipedia_BBC    Absolute Present        6.0       0.793333   \n",
       "\n",
       "    Test Precision  Test Recall  Test F1 Score  \n",
       "0         0.920000     0.884615       0.901961  \n",
       "0         0.833333     0.961538       0.892857  \n",
       "0         0.884615     0.884615       0.884615  \n",
       "0         0.857143     0.923077       0.888889  \n",
       "0         0.884615     0.884615       0.884615  \n",
       "..             ...          ...            ...  \n",
       "1         0.444444     0.769231       0.563380  \n",
       "1         0.446809     0.807692       0.575342  \n",
       "1         0.448980     0.846154       0.586667  \n",
       "1         0.448980     0.846154       0.586667  \n",
       "1         0.446809     0.807692       0.575342  \n",
       "\n",
       "[252 rows x 7 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_result_df.sort_values(by = \"Test Accuracy\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c86b1827",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result_df.to_parquet(\"Classification_Results\\Lexicon_Tagging_Test_Results.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9ec1e5",
   "metadata": {},
   "source": [
    "## 3.2. Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "84af67e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_test_df = df_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "de7964bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81e8a333e4df4b969c425ccb5a559c13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.34k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Boedt\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:127: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Boedt\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "379810257e0d439aac05856b470efa27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d377904ec3c421cb5b4bd30a56dfe06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2324ddfa56ab43f4bfaeface89d166e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.15M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1fadd541649458a91fd6a9326f9be75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/4.98k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d9418e74ef542db8a055514c47faf93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/280 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c097c61ea54b46c89bd67b90a58f9a49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/854 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58e599b5a9d44a0dabbdebc1d58bb794",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/329M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bert1 = get_metrics_df_hugging_face(bert_test_df, 'Text',\"climatebert/environmental-claims\",512, \"climatebert/environmental-claims\")\n",
    "bert2 = get_metrics_df_hugging_face(bert_test_df, \"Text\", \"climatebert/distilroberta-base-climate-detector\", 512, \"climatebert/distilroberta-base-climate-detector\")\n",
    "bert_df = pd.concat([bert1, bert2]).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "64a8f4dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>climatebert/environmental-claims</td>\n",
       "      <td>0.173333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>climatebert/distilroberta-base-climate-detector</td>\n",
       "      <td>0.886667</td>\n",
       "      <td>0.604651</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.753623</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Model  Accuracy  Precision  \\\n",
       "0                 climatebert/environmental-claims  0.173333   0.000000   \n",
       "1  climatebert/distilroberta-base-climate-detector  0.886667   0.604651   \n",
       "\n",
       "   Recall  F1 Score  \n",
       "0     0.0  0.000000  \n",
       "1     1.0  0.753623  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bf9786",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_df.to_parquet(\"Classification_Results\\Bert_Tagging_Test_Results.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cedaec8",
   "metadata": {},
   "source": [
    "# 4. Comparing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "61d2e135",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lexicon_train_result_df = pd.read_parquet(\"Classification_Results\\Lexicon_Tagging_Train_Results.parquet\")\n",
    "Lexicon_test_result_df = pd.read_parquet(\"Classification_Results\\Lexicon_Tagging_Test_Results.parquet\")\n",
    "Bert_test_result_df = pd.read_parquet(\"Classification_Results\\Bert_Tagging_Test_Results.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e17a2237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lexicon</th>\n",
       "      <th>Technique</th>\n",
       "      <th>Threshold</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EPA_BBC</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.739726</td>\n",
       "      <td>0.812030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EPA_Wikipedia_BBC</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.739726</td>\n",
       "      <td>0.812030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EPA_BBC</td>\n",
       "      <td>Relative Frequency</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.925714</td>\n",
       "      <td>0.790123</td>\n",
       "      <td>0.876712</td>\n",
       "      <td>0.831169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UNDP_EPA_Wikipedia</td>\n",
       "      <td>Relative Frequency</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.925714</td>\n",
       "      <td>0.813333</td>\n",
       "      <td>0.835616</td>\n",
       "      <td>0.824324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EPA</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.925714</td>\n",
       "      <td>0.861538</td>\n",
       "      <td>0.767123</td>\n",
       "      <td>0.811594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>UNDP_EPA</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.925714</td>\n",
       "      <td>0.861538</td>\n",
       "      <td>0.767123</td>\n",
       "      <td>0.811594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>UNDP_EPA_BBC</td>\n",
       "      <td>Relative Frequency</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.925714</td>\n",
       "      <td>0.783133</td>\n",
       "      <td>0.890411</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>UNDP_EPA</td>\n",
       "      <td>Relative Frequency</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.925714</td>\n",
       "      <td>0.821918</td>\n",
       "      <td>0.821918</td>\n",
       "      <td>0.821918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>UNDP_EPA_BBC</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.925714</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.780822</td>\n",
       "      <td>0.814286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>EPA_Wikipedia</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.925714</td>\n",
       "      <td>0.898305</td>\n",
       "      <td>0.726027</td>\n",
       "      <td>0.803030</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Lexicon           Technique  Threshold  Accuracy  Precision  \\\n",
       "0             EPA_BBC  Absolute Frequency        7.0  0.928571   0.900000   \n",
       "1   EPA_Wikipedia_BBC  Absolute Frequency        7.0  0.928571   0.900000   \n",
       "2             EPA_BBC  Relative Frequency        0.5  0.925714   0.790123   \n",
       "3  UNDP_EPA_Wikipedia  Relative Frequency        0.6  0.925714   0.813333   \n",
       "4                 EPA  Absolute Frequency        6.0  0.925714   0.861538   \n",
       "5            UNDP_EPA  Absolute Frequency        6.0  0.925714   0.861538   \n",
       "6        UNDP_EPA_BBC  Relative Frequency        0.5  0.925714   0.783133   \n",
       "7            UNDP_EPA  Relative Frequency        0.6  0.925714   0.821918   \n",
       "8        UNDP_EPA_BBC  Absolute Frequency        6.0  0.925714   0.850746   \n",
       "9       EPA_Wikipedia  Absolute Frequency        7.0  0.925714   0.898305   \n",
       "\n",
       "     Recall  F1 Score  \n",
       "0  0.739726  0.812030  \n",
       "1  0.739726  0.812030  \n",
       "2  0.876712  0.831169  \n",
       "3  0.835616  0.824324  \n",
       "4  0.767123  0.811594  \n",
       "5  0.767123  0.811594  \n",
       "6  0.890411  0.833333  \n",
       "7  0.821918  0.821918  \n",
       "8  0.780822  0.814286  \n",
       "9  0.726027  0.803030  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Lexicon_train_result_df.sort_values(by = \"Accuracy\", ascending = False).head(10).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "31f13431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lexicon</th>\n",
       "      <th>Technique</th>\n",
       "      <th>Threshold</th>\n",
       "      <th>Test Precision</th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Test Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.653846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EPA_BBC</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.884615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UNDP_Wikipedia</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.769231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wikipedia_BBC</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.769231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UNDP_Wikipedia_BBC</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.769231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>UNDP_Wikipedia_BBC</td>\n",
       "      <td>Relative Present</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.692308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>BBC</td>\n",
       "      <td>Relative Present</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.926667</td>\n",
       "      <td>0.653846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Wikipedia_BBC</td>\n",
       "      <td>Relative Present</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.926667</td>\n",
       "      <td>0.653846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Global_Change_UNDP_Wikipedia</td>\n",
       "      <td>Relative Frequency</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.926667</td>\n",
       "      <td>0.653846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>EPA_Wikipedia_BBC</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.884615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Lexicon           Technique  Threshold  \\\n",
       "0                     Wikipedia  Absolute Frequency        7.0   \n",
       "1                       EPA_BBC  Absolute Frequency        7.0   \n",
       "2                UNDP_Wikipedia  Absolute Frequency        7.0   \n",
       "3                 Wikipedia_BBC  Absolute Frequency        7.0   \n",
       "4            UNDP_Wikipedia_BBC  Absolute Frequency        7.0   \n",
       "5            UNDP_Wikipedia_BBC    Relative Present        0.4   \n",
       "6                           BBC    Relative Present        0.2   \n",
       "7                 Wikipedia_BBC    Relative Present        0.4   \n",
       "8  Global_Change_UNDP_Wikipedia  Relative Frequency        1.1   \n",
       "9             EPA_Wikipedia_BBC  Absolute Frequency        7.0   \n",
       "\n",
       "   Test Precision  Test Accuracy  Test Recall  \n",
       "0        0.944444       0.933333     0.653846  \n",
       "1        0.920000       0.966667     0.884615  \n",
       "2        0.909091       0.946667     0.769231  \n",
       "3        0.909091       0.946667     0.769231  \n",
       "4        0.909091       0.946667     0.769231  \n",
       "5        0.900000       0.933333     0.692308  \n",
       "6        0.894737       0.926667     0.653846  \n",
       "7        0.894737       0.926667     0.653846  \n",
       "8        0.894737       0.926667     0.653846  \n",
       "9        0.884615       0.960000     0.884615  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Lexicon_test_result_df.sort_values(by = [\"Test Precision\", \"Test Accuracy\"], ascending = False).head(10).reset_index(drop = True)[[\"Lexicon\", \"Technique\", \"Threshold\", \"Test Precision\", \"Test Accuracy\", \"Test Recall\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0eef69c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lexicon</th>\n",
       "      <th>Technique</th>\n",
       "      <th>Threshold</th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Test Precision</th>\n",
       "      <th>Test Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EPA_BBC</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.884615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EPA_Wikipedia_BBC</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UNDP_EPA_Wikipedia_BBC</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EPA</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UNDP_EPA</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>UNDP_EPA_Wikipedia</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>UNDP_EPA_BBC</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.961538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>EPA_Wikipedia</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.953333</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.846154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>BBC</td>\n",
       "      <td>Relative Frequency</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.953333</td>\n",
       "      <td>0.827586</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>UNDP_Wikipedia</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.769231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Lexicon           Technique  Threshold  Test Accuracy  \\\n",
       "0                 EPA_BBC  Absolute Frequency        7.0       0.966667   \n",
       "1       EPA_Wikipedia_BBC  Absolute Frequency        7.0       0.960000   \n",
       "2  UNDP_EPA_Wikipedia_BBC  Absolute Frequency        7.0       0.960000   \n",
       "3                     EPA  Absolute Frequency        6.0       0.960000   \n",
       "4                UNDP_EPA  Absolute Frequency        6.0       0.960000   \n",
       "5      UNDP_EPA_Wikipedia  Absolute Frequency        6.0       0.960000   \n",
       "6            UNDP_EPA_BBC  Absolute Frequency        6.0       0.960000   \n",
       "7           EPA_Wikipedia  Absolute Frequency        7.0       0.953333   \n",
       "8                     BBC  Relative Frequency        0.2       0.953333   \n",
       "9          UNDP_Wikipedia  Absolute Frequency        7.0       0.946667   \n",
       "\n",
       "   Test Precision  Test Recall  \n",
       "0        0.920000     0.884615  \n",
       "1        0.884615     0.884615  \n",
       "2        0.884615     0.884615  \n",
       "3        0.857143     0.923077  \n",
       "4        0.857143     0.923077  \n",
       "5        0.857143     0.923077  \n",
       "6        0.833333     0.961538  \n",
       "7        0.880000     0.846154  \n",
       "8        0.827586     0.923077  \n",
       "9        0.909091     0.769231  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Lexicon_test_result_df.sort_values(by = [\"Test Accuracy\", \"Test Precision\"], ascending = False).head(10).reset_index(drop = True)[[\"Lexicon\", \"Technique\", \"Threshold\", \"Test Accuracy\", \"Test Precision\", \"Test Recall\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7508a2cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lexicon</th>\n",
       "      <th>Technique</th>\n",
       "      <th>Threshold</th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Test Precision</th>\n",
       "      <th>Test Recall</th>\n",
       "      <th>Test F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EPA_BBC</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.901961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UNDP_EPA_BBC</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.892857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EPA</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UNDP_EPA</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UNDP_EPA_Wikipedia</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>EPA_Wikipedia_BBC</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>UNDP_EPA_Wikipedia_BBC</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>BBC</td>\n",
       "      <td>Relative Frequency</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.953333</td>\n",
       "      <td>0.827586</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.872727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>EPA_Wikipedia</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.953333</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.862745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>EPA</td>\n",
       "      <td>Relative Frequency</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.862069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Lexicon           Technique  Threshold  Test Accuracy  \\\n",
       "0                 EPA_BBC  Absolute Frequency        7.0       0.966667   \n",
       "1            UNDP_EPA_BBC  Absolute Frequency        6.0       0.960000   \n",
       "2                     EPA  Absolute Frequency        6.0       0.960000   \n",
       "3                UNDP_EPA  Absolute Frequency        6.0       0.960000   \n",
       "4      UNDP_EPA_Wikipedia  Absolute Frequency        6.0       0.960000   \n",
       "5       EPA_Wikipedia_BBC  Absolute Frequency        7.0       0.960000   \n",
       "6  UNDP_EPA_Wikipedia_BBC  Absolute Frequency        7.0       0.960000   \n",
       "7                     BBC  Relative Frequency        0.2       0.953333   \n",
       "8           EPA_Wikipedia  Absolute Frequency        7.0       0.953333   \n",
       "9                     EPA  Relative Frequency        0.5       0.946667   \n",
       "\n",
       "   Test Precision  Test Recall  Test F1 Score  \n",
       "0        0.920000     0.884615       0.901961  \n",
       "1        0.833333     0.961538       0.892857  \n",
       "2        0.857143     0.923077       0.888889  \n",
       "3        0.857143     0.923077       0.888889  \n",
       "4        0.857143     0.923077       0.888889  \n",
       "5        0.884615     0.884615       0.884615  \n",
       "6        0.884615     0.884615       0.884615  \n",
       "7        0.827586     0.923077       0.872727  \n",
       "8        0.880000     0.846154       0.862745  \n",
       "9        0.781250     0.961538       0.862069  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Lexicon_test_result_df.sort_values(by = [\"Test F1 Score\", \"Test Accuracy\", \"Test Precision\"], ascending = False).head(10).reset_index(drop = True)[[\"Lexicon\", \"Technique\", \"Threshold\", \"Test Accuracy\", \"Test Precision\", \"Test Recall\", \"Test F1 Score\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f3fae941",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lexicons_names = [\"BBC\", \"EPA\", \"Wikipedia\", \"UNDP\", \"IPCC\", \"Global_Change\"]\n",
    "Solo_Lexicons_df = Lexicon_test_result_df[Lexicon_test_result_df[\"Lexicon\"].isin(Lexicons_names)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fecb9654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test Accuracy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lexicon</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>EPA</th>\n",
       "      <td>0.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BBC</th>\n",
       "      <td>0.953333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UNDP</th>\n",
       "      <td>0.940000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wikipedia</th>\n",
       "      <td>0.940000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IPCC</th>\n",
       "      <td>0.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Global_Change</th>\n",
       "      <td>0.853333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Test Accuracy\n",
       "Lexicon                     \n",
       "EPA                 0.960000\n",
       "BBC                 0.953333\n",
       "UNDP                0.940000\n",
       "Wikipedia           0.940000\n",
       "IPCC                0.920000\n",
       "Global_Change       0.853333"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(Solo_Lexicons_df.groupby(\"Lexicon\")[\"Test Accuracy\"].max()).sort_values(by = \"Test Accuracy\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "74a1d2f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Technique\n",
       "Absolute Frequency    0.966667\n",
       "Absolute Present      0.926667\n",
       "Relative Frequency    0.953333\n",
       "Relative Present      0.940000\n",
       "Name: Test Accuracy, dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Lexicon_test_result_df.groupby(\"Technique\")[\"Test Accuracy\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "58729d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 150/150 [00:01<00:00, 146.54it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 150/150 [00:00<00:00, 378.98it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 150/150 [00:00<00:00, 461.59it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 150/150 [00:01<00:00, 145.96it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 150/150 [00:02<00:00, 70.89it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 150/150 [00:00<00:00, 208.93it/s]\n"
     ]
    }
   ],
   "source": [
    "data = get_most_words_used(Text_df_test, EPA_Lexicon)\n",
    "EPA_words_used = pd.DataFrame.from_dict(data, orient='index', columns=['Count']).reset_index().rename(columns={'index': 'Word'})\n",
    "\n",
    "data = get_most_words_used(Text_df_test, BBC_Lexicon)\n",
    "BBC_words_used = pd.DataFrame.from_dict(data, orient='index', columns=['Count']).reset_index().rename(columns={'index': 'Word'})\n",
    "\n",
    "data = get_most_words_used(Text_df_test, UNDP_Lexicon)\n",
    "UNDP_words_used = pd.DataFrame.from_dict(data, orient='index', columns=['Count']).reset_index().rename(columns={'index': 'Word'})\n",
    "\n",
    "data = get_most_words_used(Text_df_test, Wikipedia_Lexicon)\n",
    "Wikipedia_words_used = pd.DataFrame.from_dict(data, orient='index', columns=['Count']).reset_index().rename(columns={'index': 'Word'})\n",
    "\n",
    "data = get_most_words_used(Text_df_test, IPCC_Lexicon)\n",
    "IPCC_words_used = pd.DataFrame.from_dict(data, orient='index', columns=['Count']).reset_index().rename(columns={'index': 'Word'})\n",
    "\n",
    "data = get_most_words_used(Text_df_test, Global_Change_Lexicon)\n",
    "Global_Change_words_used = pd.DataFrame.from_dict(data, orient='index', columns=['Count']).reset_index().rename(columns={'index': 'Word'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a2687333",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPA_top = EPA_words_used.sort_values(\"Count\", ascending = False).head(10).reset_index(drop = True)\n",
    "EPA_top.columns = [\"EPA\", \"Count\"]\n",
    "BBC_top = BBC_words_used.sort_values(\"Count\", ascending = False).head(10).reset_index(drop = True)\n",
    "BBC_top.columns = [\"BBC\", \"Count\"]\n",
    "Wikipedia_top = Wikipedia_words_used.sort_values(\"Count\", ascending = False).head(10).reset_index(drop = True)\n",
    "Wikipedia_top.columns = [\"Wikipedia\", \"Count\"]\n",
    "UNDP_top = UNDP_words_used.sort_values(\"Count\", ascending = False).head(10).reset_index(drop = True)\n",
    "UNDP_top.columns = [\"UNDP\", \"Count\"]\n",
    "IPCC_top = IPCC_words_used.sort_values(\"Count\", ascending = False).head(10).reset_index(drop = True)\n",
    "IPCC_top.columns = [\"IPCC\", \"Count\"]\n",
    "Global_Change_top = Global_Change_words_used.sort_values(\"Count\", ascending = False).head(10).reset_index(drop = True)\n",
    "Global_Change_top.columns = [\"Global Change\", \"Count\"]\n",
    "\n",
    "top_df = pd.concat([EPA_top, BBC_top, Wikipedia_top, UNDP_top, IPCC_top, Global_Change_top], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f9e27fa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EPA</th>\n",
       "      <th>Count</th>\n",
       "      <th>BBC</th>\n",
       "      <th>Count</th>\n",
       "      <th>Wikipedia</th>\n",
       "      <th>Count</th>\n",
       "      <th>UNDP</th>\n",
       "      <th>Count</th>\n",
       "      <th>IPCC</th>\n",
       "      <th>Count</th>\n",
       "      <th>Global Change</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>climate</td>\n",
       "      <td>62</td>\n",
       "      <td>climate change</td>\n",
       "      <td>59</td>\n",
       "      <td>climate</td>\n",
       "      <td>62</td>\n",
       "      <td>climate</td>\n",
       "      <td>62</td>\n",
       "      <td>climate</td>\n",
       "      <td>62</td>\n",
       "      <td>climate change</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>climate change</td>\n",
       "      <td>59</td>\n",
       "      <td>fossil fuels</td>\n",
       "      <td>7</td>\n",
       "      <td>climate change</td>\n",
       "      <td>59</td>\n",
       "      <td>climate change</td>\n",
       "      <td>59</td>\n",
       "      <td>climate change</td>\n",
       "      <td>59</td>\n",
       "      <td>risk</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>emissions</td>\n",
       "      <td>13</td>\n",
       "      <td>global warming</td>\n",
       "      <td>7</td>\n",
       "      <td>weather</td>\n",
       "      <td>10</td>\n",
       "      <td>weather</td>\n",
       "      <td>10</td>\n",
       "      <td>risk</td>\n",
       "      <td>38</td>\n",
       "      <td>value</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>weather</td>\n",
       "      <td>10</td>\n",
       "      <td>renewable energy</td>\n",
       "      <td>5</td>\n",
       "      <td>fossil fuel</td>\n",
       "      <td>8</td>\n",
       "      <td>global warming</td>\n",
       "      <td>7</td>\n",
       "      <td>agreement</td>\n",
       "      <td>30</td>\n",
       "      <td>evolution</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fossil fuel</td>\n",
       "      <td>8</td>\n",
       "      <td>methane</td>\n",
       "      <td>3</td>\n",
       "      <td>global warming</td>\n",
       "      <td>7</td>\n",
       "      <td>renewable energy</td>\n",
       "      <td>5</td>\n",
       "      <td>social</td>\n",
       "      <td>28</td>\n",
       "      <td>uncertainty</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>global warming</td>\n",
       "      <td>7</td>\n",
       "      <td>greenhouse gases</td>\n",
       "      <td>2</td>\n",
       "      <td>greenhouse gas</td>\n",
       "      <td>5</td>\n",
       "      <td>transparency</td>\n",
       "      <td>3</td>\n",
       "      <td>region</td>\n",
       "      <td>24</td>\n",
       "      <td>global warming</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>atmosphere</td>\n",
       "      <td>6</td>\n",
       "      <td>mitigation</td>\n",
       "      <td>2</td>\n",
       "      <td>ozone</td>\n",
       "      <td>4</td>\n",
       "      <td>adaptation</td>\n",
       "      <td>2</td>\n",
       "      <td>model</td>\n",
       "      <td>24</td>\n",
       "      <td>feedback</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>renewable energy</td>\n",
       "      <td>5</td>\n",
       "      <td>adaptation</td>\n",
       "      <td>2</td>\n",
       "      <td>argo</td>\n",
       "      <td>4</td>\n",
       "      <td>mitigation</td>\n",
       "      <td>2</td>\n",
       "      <td>policies</td>\n",
       "      <td>23</td>\n",
       "      <td>scenario</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>greenhouse gas</td>\n",
       "      <td>5</td>\n",
       "      <td>carbon dioxide</td>\n",
       "      <td>2</td>\n",
       "      <td>methane</td>\n",
       "      <td>3</td>\n",
       "      <td>climate crisis</td>\n",
       "      <td>2</td>\n",
       "      <td>institution</td>\n",
       "      <td>17</td>\n",
       "      <td>sink</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ozone</td>\n",
       "      <td>4</td>\n",
       "      <td>hockey stick</td>\n",
       "      <td>1</td>\n",
       "      <td>climate crisis</td>\n",
       "      <td>2</td>\n",
       "      <td>decarbonization</td>\n",
       "      <td>2</td>\n",
       "      <td>evidence</td>\n",
       "      <td>16</td>\n",
       "      <td>ozone</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                EPA  Count               BBC  Count       Wikipedia  Count  \\\n",
       "0           climate     62    climate change     59         climate     62   \n",
       "1    climate change     59      fossil fuels      7  climate change     59   \n",
       "2         emissions     13    global warming      7         weather     10   \n",
       "3           weather     10  renewable energy      5     fossil fuel      8   \n",
       "4       fossil fuel      8           methane      3  global warming      7   \n",
       "5    global warming      7  greenhouse gases      2  greenhouse gas      5   \n",
       "6        atmosphere      6        mitigation      2           ozone      4   \n",
       "7  renewable energy      5        adaptation      2            argo      4   \n",
       "8    greenhouse gas      5    carbon dioxide      2         methane      3   \n",
       "9             ozone      4      hockey stick      1  climate crisis      2   \n",
       "\n",
       "               UNDP  Count            IPCC  Count   Global Change  Count  \n",
       "0           climate     62         climate     62  climate change     59  \n",
       "1    climate change     59  climate change     59            risk     38  \n",
       "2           weather     10            risk     38           value     26  \n",
       "3    global warming      7       agreement     30       evolution     13  \n",
       "4  renewable energy      5          social     28     uncertainty     12  \n",
       "5      transparency      3          region     24  global warming      7  \n",
       "6        adaptation      2           model     24        feedback      5  \n",
       "7        mitigation      2        policies     23        scenario      5  \n",
       "8    climate crisis      2     institution     17            sink      4  \n",
       "9   decarbonization      2        evidence     16           ozone      4  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "62826592",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 26/26 [00:00<00:00, 141.87it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 26/26 [00:00<00:00, 367.19it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 26/26 [00:00<00:00, 481.26it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 26/26 [00:00<00:00, 145.61it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 26/26 [00:00<00:00, 67.49it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 26/26 [00:00<00:00, 219.51it/s]\n"
     ]
    }
   ],
   "source": [
    "data = get_most_words_used(Text_df_test[Text_df_test[\"Target\"] == \"Yes\"], EPA_Lexicon)\n",
    "EPA_words_used_Yes = pd.DataFrame.from_dict(data, orient='index', columns=['Count']).reset_index().rename(columns={'index': 'Word'})\n",
    "\n",
    "data = get_most_words_used(Text_df_test[Text_df_test[\"Target\"] == \"Yes\"], BBC_Lexicon)\n",
    "BBC_words_used_Yes = pd.DataFrame.from_dict(data, orient='index', columns=['Count']).reset_index().rename(columns={'index': 'Word'})\n",
    "\n",
    "data = get_most_words_used(Text_df_test[Text_df_test[\"Target\"] == \"Yes\"], UNDP_Lexicon)\n",
    "UNDP_words_used_Yes = pd.DataFrame.from_dict(data, orient='index', columns=['Count']).reset_index().rename(columns={'index': 'Word'})\n",
    "\n",
    "data = get_most_words_used(Text_df_test[Text_df_test[\"Target\"] == \"Yes\"], Wikipedia_Lexicon)\n",
    "Wikipedia_words_used_Yes = pd.DataFrame.from_dict(data, orient='index', columns=['Count']).reset_index().rename(columns={'index': 'Word'})\n",
    "\n",
    "data = get_most_words_used(Text_df_test[Text_df_test[\"Target\"] == \"Yes\"], IPCC_Lexicon)\n",
    "IPCC_words_used_Yes = pd.DataFrame.from_dict(data, orient='index', columns=['Count']).reset_index().rename(columns={'index': 'Word'})\n",
    "\n",
    "data = get_most_words_used(Text_df_test[Text_df_test[\"Target\"] == \"Yes\"], Global_Change_Lexicon)\n",
    "Global_Change_words_used_Yes = pd.DataFrame.from_dict(data, orient='index', columns=['Count']).reset_index().rename(columns={'index': 'Word'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4d34b855",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPA_top_Yes = EPA_words_used_Yes.sort_values(\"Count\", ascending = False).head(10).reset_index(drop = True)\n",
    "EPA_top_Yes.columns = [\"EPA\", \"Count\"]\n",
    "BBC_top_Yes = BBC_words_used_Yes.sort_values(\"Count\", ascending = False).head(10).reset_index(drop = True)\n",
    "BBC_top_Yes.columns = [\"BBC\", \"Count\"]\n",
    "Wikipedia_top_Yes = Wikipedia_words_used_Yes.sort_values(\"Count\", ascending = False).head(10).reset_index(drop = True)\n",
    "Wikipedia_top_Yes.columns = [\"Wikipedia\", \"Count\"]\n",
    "UNDP_top_Yes = UNDP_words_used_Yes.sort_values(\"Count\", ascending = False).head(10).reset_index(drop = True)\n",
    "UNDP_top_Yes.columns = [\"UNDP\", \"Count\"]\n",
    "IPCC_top_Yes = IPCC_words_used_Yes.sort_values(\"Count\", ascending = False).head(10).reset_index(drop = True)\n",
    "IPCC_top_Yes.columns = [\"IPCC\", \"Count\"]\n",
    "Global_Change_top_Yes = Global_Change_words_used_Yes.sort_values(\"Count\", ascending = False).head(10).reset_index(drop = True)\n",
    "Global_Change_top_Yes.columns = [\"Global Change\", \"Count\"]\n",
    "\n",
    "top_df_Yes = pd.concat([EPA_top_Yes, BBC_top_Yes, Wikipedia_top_Yes, UNDP_top_Yes, IPCC_top_Yes, Global_Change_top_Yes], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8bf6cad4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EPA</th>\n",
       "      <th>Count</th>\n",
       "      <th>BBC</th>\n",
       "      <th>Count</th>\n",
       "      <th>Wikipedia</th>\n",
       "      <th>Count</th>\n",
       "      <th>UNDP</th>\n",
       "      <th>Count</th>\n",
       "      <th>IPCC</th>\n",
       "      <th>Count</th>\n",
       "      <th>Global Change</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>climate</td>\n",
       "      <td>25</td>\n",
       "      <td>climate change</td>\n",
       "      <td>25</td>\n",
       "      <td>climate</td>\n",
       "      <td>25</td>\n",
       "      <td>climate</td>\n",
       "      <td>25</td>\n",
       "      <td>climate change</td>\n",
       "      <td>25</td>\n",
       "      <td>climate change</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>climate change</td>\n",
       "      <td>25</td>\n",
       "      <td>fossil fuels</td>\n",
       "      <td>6</td>\n",
       "      <td>climate change</td>\n",
       "      <td>25</td>\n",
       "      <td>climate change</td>\n",
       "      <td>25</td>\n",
       "      <td>climate</td>\n",
       "      <td>25</td>\n",
       "      <td>risk</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>emissions</td>\n",
       "      <td>13</td>\n",
       "      <td>global warming</td>\n",
       "      <td>6</td>\n",
       "      <td>fossil fuel</td>\n",
       "      <td>7</td>\n",
       "      <td>global warming</td>\n",
       "      <td>6</td>\n",
       "      <td>agreement</td>\n",
       "      <td>10</td>\n",
       "      <td>value</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fossil fuel</td>\n",
       "      <td>7</td>\n",
       "      <td>renewable energy</td>\n",
       "      <td>4</td>\n",
       "      <td>global warming</td>\n",
       "      <td>6</td>\n",
       "      <td>weather</td>\n",
       "      <td>6</td>\n",
       "      <td>risk</td>\n",
       "      <td>10</td>\n",
       "      <td>global warming</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>global warming</td>\n",
       "      <td>6</td>\n",
       "      <td>methane</td>\n",
       "      <td>3</td>\n",
       "      <td>weather</td>\n",
       "      <td>6</td>\n",
       "      <td>renewable energy</td>\n",
       "      <td>4</td>\n",
       "      <td>region</td>\n",
       "      <td>8</td>\n",
       "      <td>scenario</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>weather</td>\n",
       "      <td>6</td>\n",
       "      <td>greenhouse gases</td>\n",
       "      <td>2</td>\n",
       "      <td>greenhouse gas</td>\n",
       "      <td>5</td>\n",
       "      <td>greenhouse gases</td>\n",
       "      <td>2</td>\n",
       "      <td>policies</td>\n",
       "      <td>7</td>\n",
       "      <td>uncertainty</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>greenhouse gas</td>\n",
       "      <td>5</td>\n",
       "      <td>mitigation</td>\n",
       "      <td>2</td>\n",
       "      <td>methane</td>\n",
       "      <td>3</td>\n",
       "      <td>decarbonization</td>\n",
       "      <td>2</td>\n",
       "      <td>fossil fuels</td>\n",
       "      <td>6</td>\n",
       "      <td>evolution</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>renewable energy</td>\n",
       "      <td>4</td>\n",
       "      <td>carbon dioxide</td>\n",
       "      <td>2</td>\n",
       "      <td>argo</td>\n",
       "      <td>2</td>\n",
       "      <td>mitigation</td>\n",
       "      <td>2</td>\n",
       "      <td>model</td>\n",
       "      <td>6</td>\n",
       "      <td>greenhouse gases</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>natural gas</td>\n",
       "      <td>4</td>\n",
       "      <td>hockey stick</td>\n",
       "      <td>1</td>\n",
       "      <td>ozone</td>\n",
       "      <td>2</td>\n",
       "      <td>paris agreement</td>\n",
       "      <td>2</td>\n",
       "      <td>global warming</td>\n",
       "      <td>6</td>\n",
       "      <td>mitigation</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>atmosphere</td>\n",
       "      <td>4</td>\n",
       "      <td>carbon neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>carbon dioxide</td>\n",
       "      <td>2</td>\n",
       "      <td>tipping point</td>\n",
       "      <td>1</td>\n",
       "      <td>greenhouse gas</td>\n",
       "      <td>5</td>\n",
       "      <td>ozone</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                EPA  Count               BBC  Count       Wikipedia  Count  \\\n",
       "0           climate     25    climate change     25         climate     25   \n",
       "1    climate change     25      fossil fuels      6  climate change     25   \n",
       "2         emissions     13    global warming      6     fossil fuel      7   \n",
       "3       fossil fuel      7  renewable energy      4  global warming      6   \n",
       "4    global warming      6           methane      3         weather      6   \n",
       "5           weather      6  greenhouse gases      2  greenhouse gas      5   \n",
       "6    greenhouse gas      5        mitigation      2         methane      3   \n",
       "7  renewable energy      4    carbon dioxide      2            argo      2   \n",
       "8       natural gas      4      hockey stick      1           ozone      2   \n",
       "9        atmosphere      4    carbon neutral      1  carbon dioxide      2   \n",
       "\n",
       "               UNDP  Count            IPCC  Count     Global Change  Count  \n",
       "0           climate     25  climate change     25    climate change     25  \n",
       "1    climate change     25         climate     25              risk     10  \n",
       "2    global warming      6       agreement     10             value      8  \n",
       "3           weather      6            risk     10    global warming      6  \n",
       "4  renewable energy      4          region      8          scenario      4  \n",
       "5  greenhouse gases      2        policies      7       uncertainty      3  \n",
       "6   decarbonization      2    fossil fuels      6         evolution      3  \n",
       "7        mitigation      2           model      6  greenhouse gases      2  \n",
       "8   paris agreement      2  global warming      6        mitigation      2  \n",
       "9     tipping point      1  greenhouse gas      5             ozone      2  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_df_Yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c8b6eb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = Text_df_test[Text_df_test[\"Target\"] == \"Yes\"].copy()\n",
    "pmax = []\n",
    "pmin = []\n",
    "fmax = []\n",
    "fmin = []\n",
    "pmean = []\n",
    "fmean = []\n",
    "\n",
    "df = count_lexicon_words(input_df, EPA_Lexicon)\n",
    "pmax.append(df[\"Absolute Present\"].max())\n",
    "pmin.append(df[\"Absolute Present\"].min())\n",
    "fmax.append(df[\"Absolute Frequency\"].max())\n",
    "fmin.append(df[\"Absolute Frequency\"].min())\n",
    "pmean.append(df[\"Absolute Present\"].mean())\n",
    "fmean.append(df[\"Absolute Frequency\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "209ff76a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Estimate</th>\n",
       "      <th>No</th>\n",
       "      <th>Yes</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Target</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>No</th>\n",
       "      <td>122</td>\n",
       "      <td>2</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Yes</th>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>125</td>\n",
       "      <td>25</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Estimate   No  Yes  All\n",
       "Target                 \n",
       "No        122    2  124\n",
       "Yes         3   23   26\n",
       "All       125   25  150"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#IPCC_UNDP_BBC\n",
    "Lexicon_cross = pd.concat([EPA_Lexicon, BBC_Lexicon]).drop_duplicates()\n",
    "get_cross_table(Text_df_test, Lexicon_cross, 7, \"Absolute Frequency\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d5913769",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Estimate</th>\n",
       "      <th>No</th>\n",
       "      <th>Yes</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Target</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>No</th>\n",
       "      <td>119</td>\n",
       "      <td>5</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Yes</th>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>122</td>\n",
       "      <td>28</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Estimate   No  Yes  All\n",
       "Target                 \n",
       "No        119    5  124\n",
       "Yes         3   23   26\n",
       "All       122   28  150"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_cross_table(Text_df_test, EPA_Lexicon, 0.6, \"Relative Frequency\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6a6ca1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5d7a57a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>climatebert/environmental-claims</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>climatebert/distilroberta-base-climate-detector</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.758621</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Model  Accuracy  Precision  \\\n",
       "0                 climatebert/environmental-claims      0.36   0.000000   \n",
       "1  climatebert/distilroberta-base-climate-detector      0.79   0.647059   \n",
       "\n",
       "     Recall  F1 Score  \n",
       "0  0.000000  0.000000  \n",
       "1  0.916667  0.758621  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Bert_test_result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602d8cad",
   "metadata": {},
   "source": [
    "# 5. Classify Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461db6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "WP = pd.read_parquet(\"C:/Users/Boedt/OneDrive/Bureaublad/Scraped_Articles/Final/WP_Final_Articles.parquet\")\n",
    "WP_clean = WP.copy()\n",
    "WP_clean[\"Text\"] = WP_clean[\"Text\"].apply(preprocess_text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5367f0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "WSJ = pd.read_parquet(\"C:/Users/Boedt/OneDrive/Bureaublad/Scraped_Articles/Final/WSJ_Final\")\n",
    "WSJ_clean = WSJ.copy()\n",
    "WSJ_clean[\"Text\"] = WSJ_clean[\"Text\"].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "48bf495e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 199943/199943 [38:47<00:00, 85.89it/s]\n"
     ]
    }
   ],
   "source": [
    "Lexicon = pd.concat([EPA_Lexicon, BBC_Lexicon]).drop_duplicates()\n",
    "test = count_lexicon_words(WSJ_clean, Lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c7463261",
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"Climate\"] = \"No\"\n",
    "test.loc[test[\"Absolute Frequency\"] >= 7, \"Climate\"] = \"Yes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6af3ce84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "      <th>Link</th>\n",
       "      <th>Date</th>\n",
       "      <th>News Paper</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Transcript: Wells Fargo CEO John Stumpf’s Hous...</td>\n",
       "      <td>John Stumpf, chairman and CEO of Wells Fargo W...</td>\n",
       "      <td>http://www.wsj.com/articles/transcript-wells-f...</td>\n",
       "      <td>2016-09-29</td>\n",
       "      <td>Wall_Street_Journal</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Transcript: Jerome Powell Fields Questions at ...</td>\n",
       "      <td>Federal Reserve Chairman Jerome Powell testifi...</td>\n",
       "      <td>https://www.wsj.com/articles/transcript-jerome...</td>\n",
       "      <td>2018-02-28</td>\n",
       "      <td>Wall_Street_Journal</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Transcript: Janet Yellen’s House Testimony on ...</td>\n",
       "      <td>Federal Reserve Chairwoman Janet Yellen testif...</td>\n",
       "      <td>http://www.wsj.com/articles/transcript-janet-y...</td>\n",
       "      <td>2016-09-28</td>\n",
       "      <td>Wall_Street_Journal</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Transcript of Yellen’s Feb. 10, 2016, Appearan...</td>\n",
       "      <td>Federal Reserve Chairwoman Janet Yellen delive...</td>\n",
       "      <td>http://www.wsj.com/articles/transcript-of-yell...</td>\n",
       "      <td>2016-11-02</td>\n",
       "      <td>Wall_Street_Journal</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Transcript: Wells Fargo Hearing, Panel 1</td>\n",
       "      <td>Note to Readers: This is an unedited rush tran...</td>\n",
       "      <td>http://www.wsj.com/articles/transcript-wells-f...</td>\n",
       "      <td>2016-09-20</td>\n",
       "      <td>Wall_Street_Journal</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199938</th>\n",
       "      <td>Catholic Church Differs With Donald Trump on I...</td>\n",
       "      <td>LOS ANGELES—When President Donald Trump cuttin...</td>\n",
       "      <td>http://www.wsj.com/articles/catholic-church-di...</td>\n",
       "      <td>2017-01-30</td>\n",
       "      <td>Wall_Street_Journal</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199939</th>\n",
       "      <td>Gold Pares Losses After Fed Minutes</td>\n",
       "      <td>Gold prices pared losses Wednesday, after minu...</td>\n",
       "      <td>https://www.wsj.com/articles/gold-edges-lower-...</td>\n",
       "      <td>2017-11-10</td>\n",
       "      <td>Wall_Street_Journal</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199940</th>\n",
       "      <td>Qualcomm to Buy Back $10 Billion of Its Shares...</td>\n",
       "      <td>Qualcomm Inc. QCOM -0.80% on Tuesday unveiled ...</td>\n",
       "      <td>https://www.wsj.com/articles/qualcomm-to-buy-b...</td>\n",
       "      <td>2018-07-31</td>\n",
       "      <td>Wall_Street_Journal</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199941</th>\n",
       "      <td>Pooling and Payment Systems</td>\n",
       "      <td>Advertisement - Scroll to Continue Policy make...</td>\n",
       "      <td>http://www.wsj.com/articles/pooling-and-paymen...</td>\n",
       "      <td>2016-08-31</td>\n",
       "      <td>Wall_Street_Journal</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199942</th>\n",
       "      <td>Oil Steadies After Fresh Morning Slump in Asia</td>\n",
       "      <td>—A fresh Monday morning slump in oil futures s...</td>\n",
       "      <td>https://www.wsj.com/articles/u-s-oil-futures-a...</td>\n",
       "      <td>2018-05-27</td>\n",
       "      <td>Wall_Street_Journal</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>199943 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Title  \\\n",
       "0       Transcript: Wells Fargo CEO John Stumpf’s Hous...   \n",
       "1       Transcript: Jerome Powell Fields Questions at ...   \n",
       "2       Transcript: Janet Yellen’s House Testimony on ...   \n",
       "3       Transcript of Yellen’s Feb. 10, 2016, Appearan...   \n",
       "4                Transcript: Wells Fargo Hearing, Panel 1   \n",
       "...                                                   ...   \n",
       "199938  Catholic Church Differs With Donald Trump on I...   \n",
       "199939                Gold Pares Losses After Fed Minutes   \n",
       "199940  Qualcomm to Buy Back $10 Billion of Its Shares...   \n",
       "199941                        Pooling and Payment Systems   \n",
       "199942     Oil Steadies After Fresh Morning Slump in Asia   \n",
       "\n",
       "                                                     Text  \\\n",
       "0       John Stumpf, chairman and CEO of Wells Fargo W...   \n",
       "1       Federal Reserve Chairman Jerome Powell testifi...   \n",
       "2       Federal Reserve Chairwoman Janet Yellen testif...   \n",
       "3       Federal Reserve Chairwoman Janet Yellen delive...   \n",
       "4       Note to Readers: This is an unedited rush tran...   \n",
       "...                                                   ...   \n",
       "199938  LOS ANGELES—When President Donald Trump cuttin...   \n",
       "199939  Gold prices pared losses Wednesday, after minu...   \n",
       "199940  Qualcomm Inc. QCOM -0.80% on Tuesday unveiled ...   \n",
       "199941  Advertisement - Scroll to Continue Policy make...   \n",
       "199942  —A fresh Monday morning slump in oil futures s...   \n",
       "\n",
       "                                                     Link       Date  \\\n",
       "0       http://www.wsj.com/articles/transcript-wells-f... 2016-09-29   \n",
       "1       https://www.wsj.com/articles/transcript-jerome... 2018-02-28   \n",
       "2       http://www.wsj.com/articles/transcript-janet-y... 2016-09-28   \n",
       "3       http://www.wsj.com/articles/transcript-of-yell... 2016-11-02   \n",
       "4       http://www.wsj.com/articles/transcript-wells-f... 2016-09-20   \n",
       "...                                                   ...        ...   \n",
       "199938  http://www.wsj.com/articles/catholic-church-di... 2017-01-30   \n",
       "199939  https://www.wsj.com/articles/gold-edges-lower-... 2017-11-10   \n",
       "199940  https://www.wsj.com/articles/qualcomm-to-buy-b... 2018-07-31   \n",
       "199941  http://www.wsj.com/articles/pooling-and-paymen... 2016-08-31   \n",
       "199942  https://www.wsj.com/articles/u-s-oil-futures-a... 2018-05-27   \n",
       "\n",
       "                 News Paper  Year  \n",
       "0       Wall_Street_Journal  2016  \n",
       "1       Wall_Street_Journal  2018  \n",
       "2       Wall_Street_Journal  2016  \n",
       "3       Wall_Street_Journal  2016  \n",
       "4       Wall_Street_Journal  2016  \n",
       "...                     ...   ...  \n",
       "199938  Wall_Street_Journal  2017  \n",
       "199939  Wall_Street_Journal  2017  \n",
       "199940  Wall_Street_Journal  2018  \n",
       "199941  Wall_Street_Journal  2016  \n",
       "199942  Wall_Street_Journal  2018  \n",
       "\n",
       "[199943 rows x 6 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WSJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "37143101",
   "metadata": {},
   "outputs": [],
   "source": [
    "WSJ[\"Text\"] = WSJ[\"Text\"].apply(text_cleaning_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0785b0a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "      <th>Link</th>\n",
       "      <th>Date</th>\n",
       "      <th>News Paper</th>\n",
       "      <th>Year</th>\n",
       "      <th>Climate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Transcript: Wells Fargo CEO John Stumpf’s Hous...</td>\n",
       "      <td>John Stumpf ,  chairman and CEO of Wells Fargo...</td>\n",
       "      <td>http://www.wsj.com/articles/transcript-wells-f...</td>\n",
       "      <td>2016-09-29</td>\n",
       "      <td>Wall_Street_Journal</td>\n",
       "      <td>2016</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Transcript: Jerome Powell Fields Questions at ...</td>\n",
       "      <td>Federal Reserve Chairman Jerome Powell testifi...</td>\n",
       "      <td>https://www.wsj.com/articles/transcript-jerome...</td>\n",
       "      <td>2018-02-28</td>\n",
       "      <td>Wall_Street_Journal</td>\n",
       "      <td>2018</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Transcript: Janet Yellen’s House Testimony on ...</td>\n",
       "      <td>Federal Reserve Chairwoman Janet Yellen testif...</td>\n",
       "      <td>http://www.wsj.com/articles/transcript-janet-y...</td>\n",
       "      <td>2016-09-28</td>\n",
       "      <td>Wall_Street_Journal</td>\n",
       "      <td>2016</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Transcript of Yellen’s Feb. 10, 2016, Appearan...</td>\n",
       "      <td>Federal Reserve Chairwoman Janet Yellen delive...</td>\n",
       "      <td>http://www.wsj.com/articles/transcript-of-yell...</td>\n",
       "      <td>2016-11-02</td>\n",
       "      <td>Wall_Street_Journal</td>\n",
       "      <td>2016</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Transcript: Wells Fargo Hearing, Panel 1</td>\n",
       "      <td>Note to Readers :  This is an unedited rush tr...</td>\n",
       "      <td>http://www.wsj.com/articles/transcript-wells-f...</td>\n",
       "      <td>2016-09-20</td>\n",
       "      <td>Wall_Street_Journal</td>\n",
       "      <td>2016</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199938</th>\n",
       "      <td>Catholic Church Differs With Donald Trump on I...</td>\n",
       "      <td>LOS ANGELES—When President Donald Trump cuttin...</td>\n",
       "      <td>http://www.wsj.com/articles/catholic-church-di...</td>\n",
       "      <td>2017-01-30</td>\n",
       "      <td>Wall_Street_Journal</td>\n",
       "      <td>2017</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199939</th>\n",
       "      <td>Gold Pares Losses After Fed Minutes</td>\n",
       "      <td>Gold prices pared losses Wednesday ,  after mi...</td>\n",
       "      <td>https://www.wsj.com/articles/gold-edges-lower-...</td>\n",
       "      <td>2017-11-10</td>\n",
       "      <td>Wall_Street_Journal</td>\n",
       "      <td>2017</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199940</th>\n",
       "      <td>Qualcomm to Buy Back $10 Billion of Its Shares...</td>\n",
       "      <td>Qualcomm Inc .  QCOM  - 0 . 80 %  on Tuesday u...</td>\n",
       "      <td>https://www.wsj.com/articles/qualcomm-to-buy-b...</td>\n",
       "      <td>2018-07-31</td>\n",
       "      <td>Wall_Street_Journal</td>\n",
       "      <td>2018</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199941</th>\n",
       "      <td>Pooling and Payment Systems</td>\n",
       "      <td>Advertisement  -  Scroll to Continue Policy ma...</td>\n",
       "      <td>http://www.wsj.com/articles/pooling-and-paymen...</td>\n",
       "      <td>2016-08-31</td>\n",
       "      <td>Wall_Street_Journal</td>\n",
       "      <td>2016</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199942</th>\n",
       "      <td>Oil Steadies After Fresh Morning Slump in Asia</td>\n",
       "      <td>—A fresh Monday morning slump in oil futures s...</td>\n",
       "      <td>https://www.wsj.com/articles/u-s-oil-futures-a...</td>\n",
       "      <td>2018-05-27</td>\n",
       "      <td>Wall_Street_Journal</td>\n",
       "      <td>2018</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>199943 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Title  \\\n",
       "0       Transcript: Wells Fargo CEO John Stumpf’s Hous...   \n",
       "1       Transcript: Jerome Powell Fields Questions at ...   \n",
       "2       Transcript: Janet Yellen’s House Testimony on ...   \n",
       "3       Transcript of Yellen’s Feb. 10, 2016, Appearan...   \n",
       "4                Transcript: Wells Fargo Hearing, Panel 1   \n",
       "...                                                   ...   \n",
       "199938  Catholic Church Differs With Donald Trump on I...   \n",
       "199939                Gold Pares Losses After Fed Minutes   \n",
       "199940  Qualcomm to Buy Back $10 Billion of Its Shares...   \n",
       "199941                        Pooling and Payment Systems   \n",
       "199942     Oil Steadies After Fresh Morning Slump in Asia   \n",
       "\n",
       "                                                     Text  \\\n",
       "0       John Stumpf ,  chairman and CEO of Wells Fargo...   \n",
       "1       Federal Reserve Chairman Jerome Powell testifi...   \n",
       "2       Federal Reserve Chairwoman Janet Yellen testif...   \n",
       "3       Federal Reserve Chairwoman Janet Yellen delive...   \n",
       "4       Note to Readers :  This is an unedited rush tr...   \n",
       "...                                                   ...   \n",
       "199938  LOS ANGELES—When President Donald Trump cuttin...   \n",
       "199939  Gold prices pared losses Wednesday ,  after mi...   \n",
       "199940  Qualcomm Inc .  QCOM  - 0 . 80 %  on Tuesday u...   \n",
       "199941  Advertisement  -  Scroll to Continue Policy ma...   \n",
       "199942  —A fresh Monday morning slump in oil futures s...   \n",
       "\n",
       "                                                     Link       Date  \\\n",
       "0       http://www.wsj.com/articles/transcript-wells-f... 2016-09-29   \n",
       "1       https://www.wsj.com/articles/transcript-jerome... 2018-02-28   \n",
       "2       http://www.wsj.com/articles/transcript-janet-y... 2016-09-28   \n",
       "3       http://www.wsj.com/articles/transcript-of-yell... 2016-11-02   \n",
       "4       http://www.wsj.com/articles/transcript-wells-f... 2016-09-20   \n",
       "...                                                   ...        ...   \n",
       "199938  http://www.wsj.com/articles/catholic-church-di... 2017-01-30   \n",
       "199939  https://www.wsj.com/articles/gold-edges-lower-... 2017-11-10   \n",
       "199940  https://www.wsj.com/articles/qualcomm-to-buy-b... 2018-07-31   \n",
       "199941  http://www.wsj.com/articles/pooling-and-paymen... 2016-08-31   \n",
       "199942  https://www.wsj.com/articles/u-s-oil-futures-a... 2018-05-27   \n",
       "\n",
       "                 News Paper  Year Climate  \n",
       "0       Wall_Street_Journal  2016      No  \n",
       "1       Wall_Street_Journal  2018      No  \n",
       "2       Wall_Street_Journal  2016      No  \n",
       "3       Wall_Street_Journal  2016      No  \n",
       "4       Wall_Street_Journal  2016      No  \n",
       "...                     ...   ...     ...  \n",
       "199938  Wall_Street_Journal  2017      No  \n",
       "199939  Wall_Street_Journal  2017      No  \n",
       "199940  Wall_Street_Journal  2018      No  \n",
       "199941  Wall_Street_Journal  2016      No  \n",
       "199942  Wall_Street_Journal  2018      No  \n",
       "\n",
       "[199943 rows x 7 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WSJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c22f9518",
   "metadata": {},
   "outputs": [],
   "source": [
    "WSJ[WSJ[\"Year\"] == 2020].reset_index(drop = True).to_parquet(\"C:/Users/Boedt/OneDrive/Bureaublad/Scraped_Articles/Final/WSJ_Final_2020.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1167d365",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
