{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b21dbe2",
   "metadata": {},
   "source": [
    "# 0. Packages & Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0154ec9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "from nltk import ngrams\n",
    "from collections import Counter\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, pipeline, AutoModelForSequenceClassification, AutoModelForSeq2SeqLM\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pyarrow\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "5d4be568",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre-process text for lexicon based approaches\n",
    "def preprocess_text(text):\n",
    "    # remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # convert to lower case\n",
    "    text = text.lower()\n",
    "    # remove blank spaces\n",
    "    text = ' '.join(text.split())\n",
    "    # remove newline characters\n",
    "    text = text.replace('\\n', '')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "25fa7ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_lexicon_words(text_df, lexicon):\n",
    "    lexicon = lexicon[\"Lexicon\"]\n",
    "    frequency = []\n",
    "    present = []\n",
    "    rfrequency = []\n",
    "    rpresent = []\n",
    "\n",
    "    for text in text_df[\"Text\"]:\n",
    "        lexicon_counts = 0\n",
    "        present_count = 0\n",
    "        for word in lexicon:\n",
    "            lexicon_counts += text.lower().count(word.lower())\n",
    "            if(text.lower().count(word.lower()) > 0):\n",
    "                present_count += 1\n",
    "        \n",
    "        word_list = text.split() \n",
    "        word_count = len(word_list)\n",
    "\n",
    "        frequency.append(lexicon_counts)\n",
    "        present.append(present_count)\n",
    "        rfrequency.append((lexicon_counts/word_count)*100)\n",
    "        rpresent.append((present_count/word_count)*100)\n",
    "        \n",
    "        \n",
    "    text_df[\"Absolute Frequency\"] = frequency\n",
    "    text_df[\"Absolute Present\"] = present\n",
    "    text_df[\"Relative Frequency\"] = rfrequency\n",
    "    text_df[\"Relative Present\"] = rpresent\n",
    "    \n",
    "    return(text_df)\n",
    "\n",
    "def get_metrics(df, colname, threshold):\n",
    "    target = []\n",
    "    values = df[colname]\n",
    "    \n",
    "    for v in values:\n",
    "        if v >= threshold:\n",
    "            target.append(\"Yes\")\n",
    "        else:\n",
    "            target.append(\"No\")\n",
    "    \n",
    "    df[\"Estimate\"] = target\n",
    "    \n",
    "    cross_table = pd.crosstab(df['Target'], df['Estimate'], margins=True)\n",
    "    accuracy = (cross_table.iloc[0, 0] + cross_table.iloc[1, 1]) / cross_table.loc['All', 'All'] if cross_table.shape == (3,3) else cross_table.iloc[1,0] / (cross_table.iloc[1, 0] + cross_table.iloc[0,0]) \n",
    "    precision = cross_table.iloc[1,1] / (cross_table.iloc[0,1] + cross_table.iloc[1,1]) if cross_table.shape == (3,3) else 0\n",
    "    recall = cross_table.iloc[1,1] / (cross_table.iloc[1,0] + cross_table.iloc[1,1]) if cross_table.shape == (3,3) != 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "    return([accuracy, precision, recall, f1_score])\n",
    "    \n",
    "def find_optimal_threshold(df, lexicon, lexicon_name):\n",
    "    df = count_lexicon_words(df, lexicon)\n",
    "    \n",
    "    #absolute frequency\n",
    "    af_accuracy = 0\n",
    "    af_th = 0\n",
    "    found = False\n",
    "    while(found == False):\n",
    "        metrics = get_metrics(df, \"Absolute Frequency\", af_th)\n",
    "        if metrics[0] > af_accuracy:\n",
    "            af_accuracy = metrics[0]\n",
    "            af_precision = metrics[1]\n",
    "            af_recall = metrics[2]\n",
    "            af_f1 = metrics[3]\n",
    "            af_th += 1\n",
    "        else:\n",
    "            found = True\n",
    "            \n",
    "    #absolute present\n",
    "    ap_accuracy = 0\n",
    "    ap_th = 0\n",
    "    found = False\n",
    "    while(found == False):\n",
    "        metrics = get_metrics(df, \"Absolute Present\", ap_th)\n",
    "        if metrics[0] > ap_accuracy:\n",
    "            ap_accuracy = metrics[0]\n",
    "            ap_precision = metrics[1]\n",
    "            ap_recall = metrics[2]\n",
    "            ap_f1 = metrics[3]\n",
    "            ap_th += 1\n",
    "        else:\n",
    "            found = True\n",
    "            \n",
    "    #relative frequency\n",
    "    rf_accuracy = 0\n",
    "    rf_th = 0\n",
    "    found = False\n",
    "    while(found == False):\n",
    "        metrics = get_metrics(df, \"Relative Frequency\", rf_th)\n",
    "        if metrics[0] > rf_accuracy:\n",
    "            rf_accuracy = metrics[0]\n",
    "            rf_precision = metrics[1]\n",
    "            rf_recall = metrics[2]\n",
    "            rf_f1 = metrics[3]\n",
    "            rf_th += 0.1\n",
    "        else:\n",
    "            found = True\n",
    "            \n",
    "    #relative present\n",
    "    rp_accuracy = 0\n",
    "    rp_th = 0\n",
    "    found = False\n",
    "    while(found == False):\n",
    "        metrics = get_metrics(df, \"Relative Present\", rp_th)\n",
    "        if metrics[0] > rp_accuracy:\n",
    "            rp_accuracy = metrics[0]\n",
    "            rp_precision = metrics[1]\n",
    "            rp_recall = metrics[2]\n",
    "            rp_f1 = metrics[3]\n",
    "            rp_th += 0.1\n",
    "        else:\n",
    "            found = True\n",
    "    \n",
    "    return(pd.DataFrame({\"Lexicon\" : [lexicon_name] * 4, \n",
    "                         \"Technique\" : [\"Absolute Frequency\", \"Absolute Present\", \"Relative Frequency\", \"Relative Present\"],\n",
    "                         \"Threshold\" : [af_th - 1, ap_th - 1, rf_th - 0.1, rp_th - 0.1], \n",
    "                 \"Accuracy\" : [af_accuracy, ap_accuracy, rf_accuracy, rp_accuracy], \n",
    "                        \"Precision\" : [af_precision, ap_precision, rf_precision, rp_precision], \n",
    "                        \"Recall\" : [af_recall, ap_recall, rf_recall, rp_recall], \n",
    "                        \"F1 Score\" : [af_f1, ap_f1, rf_f1, rp_f1]}))\n",
    "    \n",
    "def test_lexicon(test_df, results_df, lexicon, lexicon_name):\n",
    "    techniques = [\"Absolute Frequency\", \"Absolute Present\", \"Relative Frequency\", \"Relative Present\"]\n",
    "    df = count_lexicon_words(test_df, lexicon)\n",
    "    accuracy = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1 = []\n",
    "    th_df = []\n",
    "    for t in range(len(techniques)):\n",
    "        th = results_df[results_df[\"Technique\"] == techniques[t]][\"Threshold\"].iloc[0]\n",
    "        th_df.append(th)\n",
    "        accuracy.append(get_metrics(df, techniques[t], th)[0])\n",
    "        precision.append(get_metrics(df, techniques[t], th)[1])\n",
    "        recall.append(get_metrics(df, techniques[t], th)[2])\n",
    "        f1.append(get_metrics(df, techniques[t], th)[3])\n",
    "    \n",
    "    return(pd.DataFrame({\"Lexicon\" : [lexicon_name] * 4, \"Technique\" : techniques, \"Threshold\" : th_df ,\n",
    "                         \"Test Accuracy\" : accuracy, \n",
    "                        \"Test Precision\" : precision, \n",
    "                        \"Test Recall\" : recall, \n",
    "                        \"Test F1 Score\" : f1}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "25c89c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary(df_with_text, name, model_name, max_lenght_input=-1):\n",
    "    data_in_list = df_with_text[name].tolist()\n",
    "    tokenizer_sum = AutoTokenizer.from_pretrained(model_name)\n",
    "    model_sum = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "    summarizer = pipeline('summarization', model=model_sum, tokenizer = tokenizer_sum) \n",
    "\n",
    "    if max_lenght_input>=0:\n",
    "        df_with_text['summary'] = summarizer(data_in_list, max_length=max_lenght_input)\n",
    "\n",
    "    else:\n",
    "        df_with_text['summary'] = summarizer(data_in_list)\n",
    "\n",
    "def classification(df_with_text, name, model_name, max_lenght_input=-1):\n",
    "    data_in_list = df_with_text[name].tolist()\n",
    "    tokenizer_clas = AutoTokenizer.from_pretrained(model_name)\n",
    "    model_clas = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "    classification = pipeline('text-classification', model=model_clas, tokenizer = tokenizer_clas) \n",
    "\n",
    "    if max_lenght_input>=0:\n",
    "        df_with_text['classification'] = classification(data_in_list, max_length=max_lenght_input, truncation=True)\n",
    "\n",
    "    else:\n",
    "        df_with_text['classification'] = classification(data_in_list)\n",
    "        \n",
    "    return(df_with_text)\n",
    "\n",
    "def get_metrics_hugging_face(text_df, text_column, model, tokens):\n",
    "    \n",
    "    df = classification(text_df, text_column, model, tokens)\n",
    "    label_list = list(df[\"classification\"])\n",
    "    labels = [entry['label'] for entry in label_list]\n",
    "    df[\"Label_Hugging\"] = labels\n",
    "    \n",
    "    cross_table = pd.crosstab(df['Target'], df['Label_Hugging'], margins=True)\n",
    "    \n",
    "    # calculate classification metrics using scikit-learn\n",
    "    accuracy = (cross_table.iloc[0, 0] + cross_table.iloc[1, 1]) / cross_table.loc['All', 'All'] if cross_table.shape == (3,3) else cross_table.iloc[1,0] / (cross_table.iloc[1, 0] + cross_table.iloc[0,0]) \n",
    "    precision = cross_table.iloc[1,1] / (cross_table.iloc[0,1] + cross_table.iloc[1,1]) if cross_table.shape == (3,3) else 0\n",
    "    recall = cross_table.iloc[1,1] / (cross_table.iloc[1,0] + cross_table.iloc[1,1]) if cross_table.shape == (3,3) != 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "\n",
    "\n",
    "    # print the metrics\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1 score:\", f1_score)\n",
    "    print(cross_table)\n",
    "    print(\"\\n\")\n",
    "    print(pd.crosstab(df['Final_Climate_Change_Level_Label'], df['Label_Hugging'], margins=True))\n",
    "    \n",
    "\n",
    "def get_metrics_df_hugging_face(text_df, text_column, model, tokens, model_name):\n",
    "    df = classification(text_df, text_column, model, tokens)\n",
    "    label_list = list(df[\"classification\"])\n",
    "    labels = [entry['label'] for entry in label_list]\n",
    "    df[\"Label_Hugging\"] = labels\n",
    "    \n",
    "    cross_table = pd.crosstab(df['Target'], df['Label_Hugging'], margins=True)\n",
    "    \n",
    "    # calculate classification metrics using scikit-learn\n",
    "    accuracy = (cross_table.iloc[0, 0] + cross_table.iloc[1, 1]) / cross_table.loc['All', 'All'] if cross_table.shape == (3,3) else cross_table.iloc[1,0] / (cross_table.iloc[1, 0] + cross_table.iloc[0,0]) \n",
    "    precision = cross_table.iloc[1,1] / (cross_table.iloc[0,1] + cross_table.iloc[1,1]) if cross_table.shape == (3,3) else 0\n",
    "    recall = cross_table.iloc[1,1] / (cross_table.iloc[1,0] + cross_table.iloc[1,1]) if cross_table.shape == (3,3) != 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "    \n",
    "    return(pd.DataFrame({\"Model\" : [model_name], \"Accuracy\" : [accuracy], \"Precision\" : [precision], \"Recall\" : [recall],\n",
    "                         \"F1 Score\" : [f1_score]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfa34b3",
   "metadata": {},
   "source": [
    "# 1. Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e2bf7db0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Final_Climate_Change_Level_Label</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>More than a dozen state attorneys general gath...</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sen. Jeff Merkley of Oregon endorsed Bernie S...</td>\n",
       "      <td>Small</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>When Carmen Luna moved to a neighborhood on t...</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>As ocean warming continues to trigger widespre...</td>\n",
       "      <td>High</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PG&amp;E Corp. told California regulators that it...</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>U.S. government bond prices swung Wednesday, u...</td>\n",
       "      <td>No Climate</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>Japan’s corporate governance reforms are start...</td>\n",
       "      <td>No Climate</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>While President Trump is out there wheezing hi...</td>\n",
       "      <td>No Climate</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>The South is home to three schools ranked four...</td>\n",
       "      <td>No Climate</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>One of the last jokes I wrote for President Ob...</td>\n",
       "      <td>No Climate</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Text  \\\n",
       "0    More than a dozen state attorneys general gath...   \n",
       "1     Sen. Jeff Merkley of Oregon endorsed Bernie S...   \n",
       "2     When Carmen Luna moved to a neighborhood on t...   \n",
       "3    As ocean warming continues to trigger widespre...   \n",
       "4     PG&E Corp. told California regulators that it...   \n",
       "..                                                 ...   \n",
       "295  U.S. government bond prices swung Wednesday, u...   \n",
       "296  Japan’s corporate governance reforms are start...   \n",
       "297  While President Trump is out there wheezing hi...   \n",
       "298  The South is home to three schools ranked four...   \n",
       "299  One of the last jokes I wrote for President Ob...   \n",
       "\n",
       "    Final_Climate_Change_Level_Label Target  \n",
       "0                             Medium    Yes  \n",
       "1                              Small     No  \n",
       "2                             Medium    Yes  \n",
       "3                               High    Yes  \n",
       "4                             Medium    Yes  \n",
       "..                               ...    ...  \n",
       "295                       No Climate     No  \n",
       "296                       No Climate     No  \n",
       "297                       No Climate     No  \n",
       "298                       No Climate     No  \n",
       "299                       No Climate     No  \n",
       "\n",
       "[300 rows x 3 columns]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_climate_df = pd.read_parquet(\"Climate_Labels_Dataset.parquet\")\n",
    "tag_climate_df.head(5)\n",
    "#Only keep the required columns\n",
    "tag_climate_df = tag_climate_df[[\"Text\", \"Final_Climate_Change_Level_Label\"]]\n",
    "#Clean the tabel\n",
    "tag_climate_df['Final_Climate_Change_Level_Label'] = tag_climate_df['Final_Climate_Change_Level_Label'].str.strip()\n",
    "tag_climate_df.loc[tag_climate_df[\"Final_Climate_Change_Level_Label\"] == \"NA\", \"Final_Climate_Change_Level_Label\"] = \"Na\"\n",
    "tag_climate_df.loc[tag_climate_df[\"Final_Climate_Change_Level_Label\"] == \"0\", \"Final_Climate_Change_Level_Label\"] = \"Na\"\n",
    "tag_climate_df.loc[tag_climate_df[\"Final_Climate_Change_Level_Label\"] == \"Na\", \"Final_Climate_Change_Level_Label\"] = \"No Climate\"\n",
    "tag_climate_df[\"Target\"] = tag_climate_df[\"Final_Climate_Change_Level_Label\"].apply(lambda x: \"Yes\" if x in [\"High\", \"Medium\"] else \"No\")\n",
    "tag_climate_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5418e92f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Final_Climate_Change_Level_Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>High</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Medium</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No Climate</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Small</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Final_Climate_Change_Level_Label  Text\n",
       "0                             High    65\n",
       "1                           Medium    35\n",
       "2                       No Climate   109\n",
       "3                            Small    91"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overview_labels_hms = tag_climate_df.groupby(\"Final_Climate_Change_Level_Label\")[\"Text\"].count().reset_index()\n",
    "overview_labels_hms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "8b25c3c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yes</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Target  Text\n",
       "0     No   200\n",
       "1    Yes   100"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overview_labels = tag_climate_df.groupby(\"Target\")[\"Text\"].count().reset_index()\n",
    "overview_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d5bef2",
   "metadata": {},
   "source": [
    "## Splits in Train en Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e5931a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataframe into two sets\n",
    "df_train, df_test = train_test_split(tag_climate_df, test_size = 1/3, random_state = 23)\n",
    "df_train = df_train.reset_index(drop = True)\n",
    "df_test = df_test.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "0765752f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Final_Climate_Change_Level_Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>High</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Medium</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No Climate</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Small</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Final_Climate_Change_Level_Label  Text\n",
       "0                             High    46\n",
       "1                           Medium    18\n",
       "2                       No Climate    75\n",
       "3                            Small    61"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.groupby(\"Final_Climate_Change_Level_Label\")[\"Text\"].count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d6a621a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yes</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Target  Text\n",
       "0     No   136\n",
       "1    Yes    64"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.groupby(\"Target\")[\"Text\"].count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "cd3722a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Final_Climate_Change_Level_Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>High</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Medium</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No Climate</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Small</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Final_Climate_Change_Level_Label  Text\n",
       "0                             High    19\n",
       "1                           Medium    17\n",
       "2                       No Climate    34\n",
       "3                            Small    30"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.groupby(\"Final_Climate_Change_Level_Label\")[\"Text\"].count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e3320d1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yes</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Target  Text\n",
       "0     No    64\n",
       "1    Yes    36"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.groupby(\"Target\")[\"Text\"].count().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ad56cb",
   "metadata": {},
   "source": [
    "# 2. Lexicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "54fc2163",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the lexicon\n",
    "UNDP_Lexicon = pd.read_csv(\"Lexicons/UNDP_Lexicon\")\n",
    "UNDP_Lexicon = UNDP_Lexicon.drop_duplicates().reset_index(drop = True)\n",
    "UNDP_Lexicon[\"Lexicon\"] = UNDP_Lexicon[\"Lexicon\"].str.lower()\n",
    "\n",
    "UNDP_Lexicon = pd.DataFrame(UNDP_Lexicon[\"Lexicon\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "dabba077",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the lexicon\n",
    "IPCC_Lexicon = pd.read_csv(\"Lexicons/IPCC_Lexicon\")\n",
    "IPCC_Lexicon = IPCC_Lexicon.drop_duplicates().reset_index(drop = True)\n",
    "IPCC_Lexicon[\"Lexicon\"] = IPCC_Lexicon[\"Lexicon\"].str.lower()\n",
    "\n",
    "IPCC_Lexicon = pd.DataFrame(IPCC_Lexicon[\"Lexicon\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "4a69ff34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the lexicon\n",
    "EPA_Lexicon = pd.read_csv(\"Lexicons/EPA_Lexicon\")\n",
    "EPA_Lexicon = EPA_Lexicon.drop_duplicates().reset_index(drop = True)\n",
    "EPA_Lexicon[\"Lexicon\"] = EPA_Lexicon[\"Lexicon\"].str.lower()\n",
    "\n",
    "EPA_Lexicon = pd.DataFrame(EPA_Lexicon[\"Lexicon\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "4e55e2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the lexicon\n",
    "Wikipedia_Lexicon = pd.read_csv(\"Lexicons/Wikipedia_Lexicon\")\n",
    "Wikipedia_Lexicon = Wikipedia_Lexicon.drop_duplicates().reset_index(drop = True)\n",
    "Wikipedia_Lexicon[\"Lexicon\"] = Wikipedia_Lexicon[\"Lexicon\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "2627bec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the lexicon\n",
    "Global_Change_Lexicon = pd.read_csv(\"Lexicons/Global_Change_Lexicon\")\n",
    "Global_Change_Lexicon = Global_Change_Lexicon.drop_duplicates().reset_index(drop = True)\n",
    "Global_Change_Lexicon[\"Lexicon\"] = Global_Change_Lexicon[\"Lexicon\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "61bc5049",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the lexicon\n",
    "BBC_Lexicon = pd.read_csv(\"Lexicons/BBC_Lexicon\")\n",
    "BBC_Lexicon = BBC_Lexicon.drop_duplicates().reset_index(drop = True)\n",
    "BBC_Lexicon[\"Lexicon\"] = BBC_Lexicon[\"Lexicon\"].str.lower()\n",
    "\n",
    "BBC_Lexicon = pd.DataFrame(BBC_Lexicon[\"Lexicon\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "62316160",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lexicon</th>\n",
       "      <th>Global Change</th>\n",
       "      <th>IPCC</th>\n",
       "      <th>Wikipedia</th>\n",
       "      <th>EPA</th>\n",
       "      <th>BBC</th>\n",
       "      <th>UNDP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Global Change</td>\n",
       "      <td>105</td>\n",
       "      <td>38</td>\n",
       "      <td>12</td>\n",
       "      <td>20</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IPCC</td>\n",
       "      <td>38</td>\n",
       "      <td>405</td>\n",
       "      <td>28</td>\n",
       "      <td>46</td>\n",
       "      <td>22</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>12</td>\n",
       "      <td>28</td>\n",
       "      <td>164</td>\n",
       "      <td>33</td>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EPA</td>\n",
       "      <td>20</td>\n",
       "      <td>46</td>\n",
       "      <td>33</td>\n",
       "      <td>176</td>\n",
       "      <td>16</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BBC</td>\n",
       "      <td>11</td>\n",
       "      <td>22</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>79</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>UNDP</td>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>11</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Lexicon  Global Change  IPCC  Wikipedia  EPA  BBC  UNDP\n",
       "0  Global Change            105    38         12   20   11     7\n",
       "1           IPCC             38   405         28   46   22    19\n",
       "2      Wikipedia             12    28        164   33   15    10\n",
       "3            EPA             20    46         33  176   16    14\n",
       "4            BBC             11    22         15   16   79    11\n",
       "5           UNDP              7    19         10   14   11    47"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create an empty dataframe and write a function to fill with the values\n",
    "\n",
    "common_words_df = pd.DataFrame({\"Lexicon\" : [\"Global Change\", \"IPCC\", \"Wikipedia\", \"EPA\", \"BBC\", \"UNDP\"], \n",
    "                               \"Global Change\": [0, 0, 0, 0, 0, 0], \"IPCC\" : [0, 0, 0, 0, 0, 0], \n",
    "                               \"Wikipedia\" : [0, 0, 0, 0, 0, 0], \"EPA\" : [0, 0, 0, 0, 0, 0], \n",
    "                               \"BBC\" : [0, 0, 0, 0, 0, 0], \"UNDP\" : [0, 0, 0, 0, 0, 0]})\n",
    "\n",
    "dfs = [Global_Change_Lexicon, IPCC_Lexicon, Wikipedia_Lexicon, EPA_Lexicon, BBC_Lexicon, UNDP_Lexicon]\n",
    "\n",
    "for r in range(0, len(dfs)):\n",
    "    for c in range(0, len(dfs)):\n",
    "        # Get the common values between the two columns\n",
    "        common_words = set(dfs[r]['Lexicon']).intersection(set(dfs[c]['Lexicon']))\n",
    "        common_words_df.loc[r, common_words_df.columns[c +1]] = len(common_words)\n",
    "\n",
    "common_words_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "1106d880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lexicon</th>\n",
       "      <th>Non unique words</th>\n",
       "      <th>unique words</th>\n",
       "      <th>total_words</th>\n",
       "      <th>Richness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Global Change</td>\n",
       "      <td>45</td>\n",
       "      <td>60</td>\n",
       "      <td>105</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IPCC</td>\n",
       "      <td>88</td>\n",
       "      <td>317</td>\n",
       "      <td>405</td>\n",
       "      <td>0.782716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>47</td>\n",
       "      <td>117</td>\n",
       "      <td>164</td>\n",
       "      <td>0.713415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EPA</td>\n",
       "      <td>66</td>\n",
       "      <td>110</td>\n",
       "      <td>176</td>\n",
       "      <td>0.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BBC</td>\n",
       "      <td>34</td>\n",
       "      <td>45</td>\n",
       "      <td>79</td>\n",
       "      <td>0.569620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>UNDP</td>\n",
       "      <td>26</td>\n",
       "      <td>21</td>\n",
       "      <td>47</td>\n",
       "      <td>0.446809</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Lexicon  Non unique words  unique words  total_words  Richness\n",
       "0  Global Change                45            60          105  0.571429\n",
       "1           IPCC                88           317          405  0.782716\n",
       "2      Wikipedia                47           117          164  0.713415\n",
       "3            EPA                66           110          176  0.625000\n",
       "4            BBC                34            45           79  0.569620\n",
       "5           UNDP                26            21           47  0.446809"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs = [Global_Change_Lexicon, IPCC_Lexicon, Wikipedia_Lexicon, EPA_Lexicon, BBC_Lexicon, UNDP_Lexicon]\n",
    "non_unique_words = []\n",
    "unique_words = []\n",
    "total_words = []\n",
    "for r in range(len(dfs)):\n",
    "    common_words = []\n",
    "    for c in range(len(dfs)):\n",
    "        if c != r:\n",
    "            # Get the common values between the two columns\n",
    "            common_words.extend(list(set(dfs[r]['Lexicon']).intersection(set(dfs[c]['Lexicon']))))\n",
    "    common_words = list(set(common_words))  # Remove duplicates by converting to a set and back to a list\n",
    "    total_words.append(len(dfs[r][\"Lexicon\"]))\n",
    "    unique_words.append(len(dfs[r][\"Lexicon\"]) - len(common_words))\n",
    "    non_unique_words.append(len(common_words))\n",
    "\n",
    "non_unique_words\n",
    "\n",
    "unique_words_df = pd.DataFrame({\"Lexicon\" : [\"Global Change\", \"IPCC\", \"Wikipedia\", \"EPA\", \"BBC\", \"UNDP\"], \n",
    "                                \"Non unique words\" : non_unique_words, \"unique words\" : unique_words, \n",
    "                               \"total_words\" : total_words})\n",
    "\n",
    "unique_words_df[\"Richness\"] = unique_words_df[\"unique words\"] / unique_words_df[\"total_words\"]\n",
    "\n",
    "unique_words_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fb7b13",
   "metadata": {},
   "source": [
    "# 3. Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6340f21",
   "metadata": {},
   "source": [
    "## 3.1. Lexicons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518f9459",
   "metadata": {},
   "source": [
    "### 3.1.1. Train Lexicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "e42af9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Text_df = df_train.copy()\n",
    "Text_df[\"Text\"] = Text_df[\"Text\"].apply(preprocess_text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "3ebadcc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.1525893211364746 seconds ---\n",
      "--- 998.0370950698853 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "names = [\"IPCC\", \"Global_Change\", \"UNDP\", \"EPA\", \"Wikipedia\", \"BBC\"]\n",
    "All_names = []\n",
    "# Generate all combinations\n",
    "all_combinations = []\n",
    "for r in range(1, len(names) + 1):\n",
    "    combinations_r = combinations(names, r)\n",
    "    all_combinations.extend(combinations_r)\n",
    "\n",
    "# Print all combinations\n",
    "for combination in all_combinations:\n",
    "    combined_string = \"_\".join(combination)\n",
    "    All_names.append(combined_string)\n",
    "    \n",
    "Lexicons = [IPCC_Lexicon, Global_Change_Lexicon, UNDP_Lexicon, EPA_Lexicon, Wikipedia_Lexicon, BBC_Lexicon]\n",
    "All_Lexicons = []\n",
    "# Generate all combinations\n",
    "all_combinations = []\n",
    "for r in range(1, len(Lexicons) + 1):\n",
    "    combinations_r = combinations(Lexicons, r)\n",
    "    all_combinations.extend(combinations_r)\n",
    "\n",
    "# Concatenate and print all combinations\n",
    "for combination in all_combinations:\n",
    "    combined_df = pd.concat(combination, axis=0).drop_duplicates().reset_index()\n",
    "    All_Lexicons.append(combined_df)\n",
    "    \n",
    "results_df = pd.DataFrame()\n",
    "\n",
    "start_results = time.time()\n",
    "\n",
    "print(\"--- %s seconds ---\" % (start_results - start))\n",
    "\n",
    "for i in range(len(All_Lexicons)):\n",
    "    r_df = find_optimal_threshold(Text_df, All_Lexicons[i], All_names[i])\n",
    "    results_df = pd.concat([results_df, r_df])\n",
    "\n",
    "results_df = results_df.reset_index(drop = True)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "82c26fee",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lexicon</th>\n",
       "      <th>Technique</th>\n",
       "      <th>Threshold</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>Wikipedia_BBC</td>\n",
       "      <td>Relative Frequency</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.920</td>\n",
       "      <td>0.842857</td>\n",
       "      <td>0.921875</td>\n",
       "      <td>0.880597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>UNDP_Wikipedia_BBC</td>\n",
       "      <td>Relative Frequency</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.915</td>\n",
       "      <td>0.830986</td>\n",
       "      <td>0.921875</td>\n",
       "      <td>0.874074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>UNDP_BBC</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.905</td>\n",
       "      <td>0.835821</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.854962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>UNDP_Wikipedia</td>\n",
       "      <td>Relative Frequency</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.905</td>\n",
       "      <td>0.835821</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.854962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>UNDP_Wikipedia</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.892857</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>IPCC_Global_Change_BBC</td>\n",
       "      <td>Absolute Present</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.730</td>\n",
       "      <td>0.580645</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>IPCC_Global_Change</td>\n",
       "      <td>Absolute Present</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.725</td>\n",
       "      <td>0.576271</td>\n",
       "      <td>0.531250</td>\n",
       "      <td>0.552846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>IPCC_Wikipedia</td>\n",
       "      <td>Absolute Present</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.720</td>\n",
       "      <td>0.554054</td>\n",
       "      <td>0.640625</td>\n",
       "      <td>0.594203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>IPCC_BBC</td>\n",
       "      <td>Absolute Present</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.715</td>\n",
       "      <td>0.547945</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.583942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IPCC</td>\n",
       "      <td>Absolute Present</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.710</td>\n",
       "      <td>0.542857</td>\n",
       "      <td>0.593750</td>\n",
       "      <td>0.567164</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>252 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Lexicon           Technique  Threshold  Accuracy  \\\n",
       "82            Wikipedia_BBC  Relative Frequency        0.6     0.920   \n",
       "158      UNDP_Wikipedia_BBC  Relative Frequency        0.6     0.915   \n",
       "68                 UNDP_BBC  Absolute Frequency        4.0     0.905   \n",
       "66           UNDP_Wikipedia  Relative Frequency        0.6     0.905   \n",
       "64           UNDP_Wikipedia  Absolute Frequency        5.0     0.900   \n",
       "..                      ...                 ...        ...       ...   \n",
       "97   IPCC_Global_Change_BBC    Absolute Present        7.0     0.730   \n",
       "25       IPCC_Global_Change    Absolute Present        7.0     0.725   \n",
       "37           IPCC_Wikipedia    Absolute Present        6.0     0.720   \n",
       "41                 IPCC_BBC    Absolute Present        6.0     0.715   \n",
       "1                      IPCC    Absolute Present        6.0     0.710   \n",
       "\n",
       "     Precision    Recall  F1 Score  \n",
       "82    0.842857  0.921875  0.880597  \n",
       "158   0.830986  0.921875  0.874074  \n",
       "68    0.835821  0.875000  0.854962  \n",
       "66    0.835821  0.875000  0.854962  \n",
       "64    0.892857  0.781250  0.833333  \n",
       "..         ...       ...       ...  \n",
       "97    0.580645  0.562500  0.571429  \n",
       "25    0.576271  0.531250  0.552846  \n",
       "37    0.554054  0.640625  0.594203  \n",
       "41    0.547945  0.625000  0.583942  \n",
       "1     0.542857  0.593750  0.567164  \n",
       "\n",
       "[252 rows x 7 columns]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.sort_values(by = \"Accuracy\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "4e93d6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_parquet(\"Lexicon_Tagging_Train_Results.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68434da",
   "metadata": {},
   "source": [
    "### 3.1.2. Test Lexicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "0f74ff59",
   "metadata": {},
   "outputs": [],
   "source": [
    "Text_df_test = df_test.copy()\n",
    "Text_df_test[\"Text\"] = Text_df_test[\"Text\"].apply(preprocess_text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "49cd1c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.14688491821289062 seconds ---\n",
      "--- 292.85168409347534 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "names = [\"IPCC\", \"Global_Change\", \"UNDP\", \"EPA\", \"Wikipedia\", \"BBC\"]\n",
    "All_names = []\n",
    "# Generate all combinations\n",
    "all_combinations = []\n",
    "for r in range(1, len(names) + 1):\n",
    "    combinations_r = combinations(names, r)\n",
    "    all_combinations.extend(combinations_r)\n",
    "\n",
    "# Print all combinations\n",
    "for combination in all_combinations:\n",
    "    combined_string = \"_\".join(combination)\n",
    "    All_names.append(combined_string)\n",
    "    \n",
    "Lexicons = [IPCC_Lexicon, Global_Change_Lexicon, UNDP_Lexicon, EPA_Lexicon, Wikipedia_Lexicon, BBC_Lexicon]\n",
    "All_Lexicons = []\n",
    "# Generate all combinations\n",
    "all_combinations = []\n",
    "for r in range(1, len(Lexicons) + 1):\n",
    "    combinations_r = combinations(Lexicons, r)\n",
    "    all_combinations.extend(combinations_r)\n",
    "\n",
    "# Concatenate and print all combinations\n",
    "for combination in all_combinations:\n",
    "    combined_df = pd.concat(combination, axis=0).drop_duplicates().reset_index()\n",
    "    All_Lexicons.append(combined_df)\n",
    "    \n",
    "test_result_df = pd.DataFrame()\n",
    "\n",
    "start_results = time.time()\n",
    "\n",
    "print(\"--- %s seconds ---\" % (start_results - start))\n",
    "\n",
    "for n in range(0, len(All_names)):\n",
    "    test_lexicon_result = test_lexicon(Text_df_test, results_df[results_df[\"Lexicon\"] == All_names[n]], All_Lexicons[All_names.index(All_names[n])], All_names[n])\n",
    "    test_result_df = pd.concat([test_result_df, test_lexicon_result])\n",
    "    \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "38fdeef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lexicon</th>\n",
       "      <th>Technique</th>\n",
       "      <th>Threshold</th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Test Precision</th>\n",
       "      <th>Test Recall</th>\n",
       "      <th>Test F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UNDP_EPA_BBC</td>\n",
       "      <td>Relative Frequency</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.861111</td>\n",
       "      <td>0.861111</td>\n",
       "      <td>0.861111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UNDP_EPA</td>\n",
       "      <td>Relative Frequency</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.861111</td>\n",
       "      <td>0.861111</td>\n",
       "      <td>0.861111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Global_Change_UNDP_EPA</td>\n",
       "      <td>Relative Frequency</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.820513</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.853333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UNDP_EPA_Wikipedia</td>\n",
       "      <td>Relative Frequency</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.837838</td>\n",
       "      <td>0.861111</td>\n",
       "      <td>0.849315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EPA_BBC</td>\n",
       "      <td>Relative Frequency</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.845070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Global_Change</td>\n",
       "      <td>Relative Present</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.527778</td>\n",
       "      <td>0.575758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Global_Change_UNDP</td>\n",
       "      <td>Relative Present</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IPCC_UNDP</td>\n",
       "      <td>Absolute Present</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.408163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IPCC_Global_Change_BBC</td>\n",
       "      <td>Absolute Present</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.594595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IPCC_Global_Change</td>\n",
       "      <td>Absolute Present</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.567568</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.575342</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>252 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Lexicon           Technique  Threshold  Test Accuracy  \\\n",
       "2             UNDP_EPA_BBC  Relative Frequency        0.6           0.90   \n",
       "2                 UNDP_EPA  Relative Frequency        0.6           0.90   \n",
       "2   Global_Change_UNDP_EPA  Relative Frequency        0.8           0.89   \n",
       "2       UNDP_EPA_Wikipedia  Relative Frequency        0.6           0.89   \n",
       "2                  EPA_BBC  Relative Frequency        0.6           0.89   \n",
       "..                     ...                 ...        ...            ...   \n",
       "3            Global_Change    Relative Present        0.3           0.72   \n",
       "3       Global_Change_UNDP    Relative Present        0.6           0.72   \n",
       "1                IPCC_UNDP    Absolute Present        9.0           0.71   \n",
       "1   IPCC_Global_Change_BBC    Absolute Present        7.0           0.70   \n",
       "1       IPCC_Global_Change    Absolute Present        7.0           0.69   \n",
       "\n",
       "    Test Precision  Test Recall  Test F1 Score  \n",
       "2         0.861111     0.861111       0.861111  \n",
       "2         0.861111     0.861111       0.861111  \n",
       "2         0.820513     0.888889       0.853333  \n",
       "2         0.837838     0.861111       0.849315  \n",
       "2         0.857143     0.833333       0.845070  \n",
       "..             ...          ...            ...  \n",
       "3         0.633333     0.527778       0.575758  \n",
       "3         0.700000     0.388889       0.500000  \n",
       "1         0.769231     0.277778       0.408163  \n",
       "1         0.578947     0.611111       0.594595  \n",
       "1         0.567568     0.583333       0.575342  \n",
       "\n",
       "[252 rows x 7 columns]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_result_df.sort_values(by = \"Test Accuracy\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "c86b1827",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result_df.to_parquet(\"Lexicon_Tagging_Test_Results.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9ec1e5",
   "metadata": {},
   "source": [
    "## 3.2. Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "84af67e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_test_df = tag_climate_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "de7964bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert1 = get_metrics_df_hugging_face(best_test_df, 'Text',\"climatebert/environmental-claims\",512, \"climatebert/environmental-claims\")\n",
    "bert2 = get_metrics_df_hugging_face(best_test_df, \"Text\", \"climatebert/distilroberta-base-climate-detector\", 512, \"climatebert/distilroberta-base-climate-detector\")\n",
    "bert_df = pd.concat([bert1, bert2]).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "64a8f4dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>climatebert/environmental-claims</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>climatebert/distilroberta-base-climate-detector</td>\n",
       "      <td>0.773333</td>\n",
       "      <td>0.603896</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.732283</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Model  Accuracy  Precision  \\\n",
       "0                 climatebert/environmental-claims  0.333333   0.000000   \n",
       "1  climatebert/distilroberta-base-climate-detector  0.773333   0.603896   \n",
       "\n",
       "   Recall  F1 Score  \n",
       "0    0.00  0.000000  \n",
       "1    0.93  0.732283  "
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "93bf9786",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_df.to_parquet(\"Bert_Tagging_Test_Results.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adcad85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d2e135",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f13431",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
