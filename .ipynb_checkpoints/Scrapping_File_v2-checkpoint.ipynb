{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "285c4d9c",
   "metadata": {},
   "source": [
    "# 0. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39ea7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "from datetime import date, timedelta\n",
    "import random\n",
    "import threading\n",
    "import multiprocessing as np\n",
    "from multiprocessing import Pool\n",
    "import concurrent.futures\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91c53ad",
   "metadata": {},
   "source": [
    "# 1. Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e0c218",
   "metadata": {},
   "source": [
    "## 1.1. Loop Through Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1770ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def daterange(start_date, end_date):\n",
    "    for n in range(int((end_date - start_date).days) + 1):\n",
    "        yield start_date + timedelta(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cd9278",
   "metadata": {},
   "source": [
    "# 2. Scraping News Media"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb1b56a",
   "metadata": {},
   "source": [
    "## 2.1. Wall Street Journal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746f9a4c",
   "metadata": {},
   "source": [
    "### 2.1.1. Function - Get URLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27213ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWallStreetJournalURLS(start_date, end_date, username, password):\n",
    "    #put dates in date format\n",
    "    start_date = datetime.strptime(start_date, \"%d/%m/%Y\")\n",
    "    end_date = datetime.strptime(end_date, \"%d/%m/%Y\")\n",
    "\n",
    "    #Define list of variables to store \n",
    "    WSJ_date = []\n",
    "    WSJ_link = []\n",
    "\n",
    "    #Go to the website\n",
    "    fireFoxOptions = webdriver.FirefoxOptions()\n",
    "    fireFoxOptions.add_argument('--ignore-certificate-errors')\n",
    "    fireFoxOptions.add_argument('--allow-running-insecure-content')\n",
    "    fireFoxOptions.add_argument(\"--headless\")\n",
    "    fireFoxOptions.add_argument(\"--disable-gpu\")\n",
    "    fireFoxOptions.add_argument(\"--no-sandbox\")\n",
    "    user_agent = 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.50 Safari/537.36'\n",
    "    fireFoxOptions.add_argument(f'user-agent={user_agent}')\n",
    "    driver_WSJ = webdriver.Firefox(options=fireFoxOptions)\n",
    "    #driver_WSJ = webdriver.Firefox()\n",
    "    driver_WSJ.get(\"https://sso.accounts.dowjones.com/login-page?op=localop&scope=openid%20idp_id%20roles%20email%20given_name%20family_name%20djid%20djUsername%20djStatus%20trackid%20tags%20prts%20suuid%20createTimestamp&client_id=5hssEAdMy0mJTICnJNvC9TXEw3Va7jfO&response_type=code&redirect_uri=https%3A%2F%2Faccounts.wsj.com%2Fauth%2Fsso%2Flogin&nonce=ab6a473a-cfa6-4714-8fad-b6dff98f5f18&ui_locales=en-us-x-wsj-223-2&mars=-1&ns=prod%2Faccounts-wsj&state=8rChOTDzC_Y_AK-i.TJAixN_XjsWxwUEEPoHg2OPCaX6qRBu4nGSk5fqLliY4H0B5F7gj_57-XH-YBWGS&protocol=oauth2&client=5hssEAdMy0mJTICnJNvC9TXEw3Va7jfO#!/signin\")\n",
    "\n",
    "    #put in user_name:\n",
    "    time.sleep(3) #makes sure field is fully loaded\n",
    "    username_field = driver_WSJ.find_element(By.XPATH, \"/html/body/div[1]/div[2]/div/div/div/div[1]/div[1]/form/div[2]/div[1]/div[2]/input\")\n",
    "    username_field.send_keys(username)\n",
    "\n",
    "    #continue to password:\n",
    "    driver_WSJ.find_element(By.XPATH, \"/html/body/div[1]/div[2]/div/div/div/div[1]/div[1]/form/div[2]/div[6]/div[1]/button[2]\").click()\n",
    "\n",
    "    #input password:\n",
    "    time.sleep(3) #makes sure field is fully loaded\n",
    "    password_field = driver_WSJ.find_element(By.XPATH, \"//*[@id='password-login-password']\")\n",
    "    password_field.send_keys(password)\n",
    "\n",
    "    #click on sign in:\n",
    "    driver_WSJ.find_element(By.XPATH, \"//*[@id='password-login']/div/form/div/div[5]/div[1]/button\").click()\n",
    "\n",
    "    try:\n",
    "        #accept cookies\n",
    "        time.sleep(10)\n",
    "        driver_WSJ.switch_to.frame(WebDriverWait(driver_WSJ,30).until(EC.presence_of_element_located((By.ID, 'sp_message_iframe_718122'))))\n",
    "        driver_WSJ.find_element(By.CSS_SELECTOR, \"button.message-component:nth-child(2)\").click()\n",
    "        driver_WSJ.switch_to.default_content() \n",
    "        time.sleep(3)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    scrape_time = time.time() #starting the timing\n",
    "\n",
    "    for single_date in daterange(start_date, end_date):\n",
    "        date = single_date.strftime(\"%d/%m/%Y\")\n",
    "        print(date)\n",
    "        year = \"%02d\" % (int(date.split('/')[2]),)\n",
    "        month = \"%02d\" % (int(date.split('/')[1]),)\n",
    "        day = \"%02d\" % (int(date.split('/')[0]),)\n",
    "        page = \"%02d\" % (1,) #we assume there is always at least one page\n",
    "        url = f\"https://www.wsj.com/news/archive/{year}/{month}/{day}?page={page}\"\n",
    "        test = 0\n",
    "        while(test < 100):\n",
    "            try:\n",
    "                driver_WSJ.get(url) #go to the page in the WSJ archive for the given date\n",
    "                break\n",
    "            except:\n",
    "                test += 1\n",
    "                time.sleep(10)\n",
    "\n",
    "        #check if there are multiple pages and if so, visit these as well\n",
    "        pages_load = 0\n",
    "        while(pages_load < 100):\n",
    "            try:\n",
    "                pages = WebDriverWait(driver_WSJ, 10).until(EC.presence_of_element_located((By.CLASS_NAME, \"WSJTheme--pagepicker-total--Kl350I1l \")))\n",
    "                page_total = re.findall(r'\\d+', pages.text)\n",
    "                nr_pages = int(page_total[0])\n",
    "                break\n",
    "            except:\n",
    "                pages_load += 1\n",
    "                driver_WSJ.get(url)\n",
    "\n",
    "        for p in range(1,nr_pages+1):\n",
    "            page = \"%02d\" % (p,)\n",
    "            url = f\"https://www.wsj.com/news/archive/{year}/{month}/{day}?page={page}\"\n",
    "            urls_load = 0\n",
    "            while(urls_load < 100):\n",
    "                try:\n",
    "                    driver_WSJ.get(url) #go to the page in the WSJ archive for the given date\n",
    "                    break\n",
    "                except:\n",
    "                    driver_WSJ.get(url)\n",
    "\n",
    "            #get the article urls for the articles on the other pages, if there are more than 1\n",
    "            articles = WebDriverWait(driver_WSJ,10).until(EC.presence_of_all_elements_located((By.TAG_NAME, \"article\")))\n",
    "            for a in range(0,len(articles)):\n",
    "                WSJ_link.append(articles[a].find_element(By.TAG_NAME, \"a\").get_attribute('href'))\n",
    "                WSJ_date.append(date)\n",
    "    \n",
    "    driver_WSJ.quit()\n",
    "    \n",
    "    WSJ = [\"Wall_Street_Journal\"] * len(WSJ_link)\n",
    "    data = {\"Date\" : WSJ_date, \"News Paper\" : WSJ, \"Link\" : WSJ_link}\n",
    "    Wall_Street_Journal = pd.DataFrame(data)\n",
    "\n",
    "    start_date = start_date.strftime(\"%d%m%Y\")\n",
    "    end_date = end_date.strftime(\"%d%m%Y\")\n",
    "    path = f\"Data/URLS/Wall_Street_Journal_{start_date}_{end_date}_URLS\"\n",
    "    Wall_Street_Journal.to_parquet(path) \n",
    "    \n",
    "    print(\"--- %s seconds ---\" % (time.time() - scrape_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217b3365",
   "metadata": {},
   "source": [
    "### 2.1.2. Function - Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f6db76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWallStreetJournalArticles(username, password, dataframe):\n",
    "    WSJ_article_content = []\n",
    "    WSJ_title = []\n",
    "\n",
    "    #FIREFOX\n",
    "    #Declare the driver and go to website\n",
    "    #fireFoxOptions = webdriver.FirefoxOptions()\n",
    "    #fireFoxOptions.add_argument('--ignore-certificate-errors')\n",
    "    #fireFoxOptions.add_argument('--allow-running-insecure-content')\n",
    "    #fireFoxOptions.add_argument(\"--headless\")\n",
    "    #fireFoxOptions.add_argument(\"--disable-gpu\")\n",
    "    #fireFoxOptions.add_argument(\"--no-sandbox\")\n",
    "    #user_agent = 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.50 Safari/537.36'\n",
    "    #fireFoxOptions.add_argument(f'user-agent={user_agent}')\n",
    "    #driver_WSJ = webdriver.Firefox(options=fireFoxOptions)\n",
    "    \n",
    "    #CHROME\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--window-size=1920x1080\")\n",
    "    chrome_options.add_argument(\"--disable-extensions\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\") # linux only\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.81 Safari/537.36\"\n",
    "    chrome_options.add_argument(f'user-agent={user_agent}')\n",
    "    driver_WSJ = webdriver.Chrome(options=chrome_options)\n",
    "    #driver_WSJ = webdriver.Chrome()\n",
    "    driver_WSJ.get(\"https://sso.accounts.dowjones.com/login-page?op=localop&scope=openid%20idp_id%20roles%20email%20given_name%20family_name%20djid%20djUsername%20djStatus%20trackid%20tags%20prts%20suuid%20updated_at&client_id=5hssEAdMy0mJTICnJNvC9TXEw3Va7jfO&response_type=code&redirect_uri=https%3A%2F%2Faccounts.wsj.com%2Fauth%2Fsso%2Flogin&nonce=5e93fe71-314a-44b9-bb2d-4a0e5a3167a4&ui_locales=en-us-x-wsj-223-2&mars=-1&ns=prod%2Faccounts-wsj&state=zJgHWictetlLHLG6.Uob1DvxnFA4kIkiqJAY80zp2N4FxS3WdBvV-VuTFdpM&protocol=oauth2&client=5hssEAdMy0mJTICnJNvC9TXEw3Va7jfO#!/signin\")\n",
    "\n",
    "    #put in user_name:\n",
    "    time.sleep(3) #makes sure field is fully loaded\n",
    "    username_field = driver_WSJ.find_element(By.XPATH, \"/html/body/div[1]/div[2]/div/div/div/div[1]/div[1]/form/div[2]/div[1]/div[2]/input\")\n",
    "    username_field.send_keys(username)\n",
    "\n",
    "    #continue to password:\n",
    "    driver_WSJ.find_element(By.XPATH, \"/html/body/div[1]/div[2]/div/div/div/div[1]/div[1]/form/div[2]/div[6]/div[1]/button[2]\").click()\n",
    "\n",
    "    #input password:\n",
    "    time.sleep(3) #makes sure field is fully loaded\n",
    "    password_field = driver_WSJ.find_element(By.XPATH, \"//*[@id='password-login-password']\")\n",
    "    password_field.send_keys(password)\n",
    "\n",
    "    #click on sign in:\n",
    "    driver_WSJ.find_element(By.XPATH, \"//*[@id='password-login']/div/form/div/div[5]/div[1]/button\").click()\n",
    "\n",
    "    try:\n",
    "        #accept cookies\n",
    "        time.sleep(20)\n",
    "        driver_WSJ.switch_to.default_content()\n",
    "        driver_WSJ.switch_to.frame(WebDriverWait(driver_WSJ,30).until(EC.presence_of_element_located((By.ID, 'sp_message_iframe_718122'))))\n",
    "        driver_WSJ.find_element(By.CSS_SELECTOR, \"button.message-component:nth-child(2)\").click()\n",
    "        print(\"accepted\")\n",
    "        time.sleep(3)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    scrape_time = time.time()\n",
    "    #go to all the articles and scrape the content\n",
    "    links  = list(dataframe[\"Link\"])\n",
    "    for u in range(0, len(links)):\n",
    "        trys = 0\n",
    "        while(trys < 1):\n",
    "            try:\n",
    "                driver_WSJ.get(links[u])\n",
    "                break\n",
    "            except:\n",
    "                trys += 1\n",
    "                time.sleep(10)\n",
    "\n",
    "        try:\n",
    "            content = driver_WSJ.find_element(By.TAG_NAME, 'article') \n",
    "            #extract the content and add to a variables\n",
    "            text = content.find_elements(By.TAG_NAME, 'p')\n",
    "            article_text = \"\"\n",
    "            for t in range(0,len(text)):\n",
    "                article_text += \" \" + text[t].text\n",
    "\n",
    "            WSJ_article_content.append(article_text) #add text to the list\n",
    "            #WSJ_article_content.append(content.text) #add text to the list\n",
    "\n",
    "        except: #we don't have access to the article\n",
    "            WSJ_article_content.append(\"NO ACCESS\")\n",
    "            pass #go back to page with all articles\n",
    "\n",
    "        try:\n",
    "            #collect the title as well\n",
    "            title = WebDriverWait(driver_WSJ,10).until(EC.presence_of_element_located((By.TAG_NAME, \"h1\"))).text\n",
    "            WSJ_title.append(title)\n",
    "        except:\n",
    "            try:\n",
    "                #there are 2 main formats in which the titles are present in the HTML\n",
    "                title = WebDriverWait(driver_WSJ,10).until(EC.presence_of_element_located((By.CLASS_NAME, \"bigTop__hed\"))).text\n",
    "                WSJ_title.append(title)\n",
    "\n",
    "            except:\n",
    "                #add error if there is some unexpected layout -> this way our arrays will be of the same length and we will be able\n",
    "                #to construct a dataframe in the end\n",
    "                WSJ_title.append(\"ERROR\")\n",
    "                pass\n",
    "\n",
    "    driver_WSJ.quit()\n",
    "\n",
    "    WSJ = [\"Wall_Street_Journal\"] * len(WSJ_article_content)\n",
    "    data = {\"Title\" : WSJ_title, \"Text\" : WSJ_article_content, \"Link\" : dataframe[\"Link\"]}\n",
    "    Wall_Street_Journal = pd.DataFrame(data)\n",
    "    Wall_Street_Journal = pd.merge(Wall_Street_Journal, dataframe, on = \"Link\")\n",
    "    \n",
    "    print(\"--- %s seconds ---\" % (time.time() - scrape_time))\n",
    "    return(Wall_Street_Journal)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2cf9e1",
   "metadata": {},
   "source": [
    "## 2.2. Washington Post"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a6082c",
   "metadata": {},
   "source": [
    "### 2.2.1. Function - Get URLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32e3729",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWashingtonPostURLS(start_date, end_date, username, password):\n",
    "    #put dates in date format\n",
    "    start_date = datetime.strptime(start_date, \"%d/%m/%Y\")   \n",
    "    end_date = datetime.strptime(end_date, \"%d/%m/%Y\")\n",
    "\n",
    "\n",
    "    #put them in correct format\n",
    "\n",
    "\n",
    "    #Declare all list variables for the output\n",
    "    WP_date = []\n",
    "    WP_link = []\n",
    "    \n",
    "    #FIREFOX\n",
    "    #Declare the driver and go to website\n",
    "    #fireFoxOptions = webdriver.FirefoxOptions()\n",
    "    #fireFoxOptions.add_argument('--ignore-certificate-errors')\n",
    "    #fireFoxOptions.add_argument('--allow-running-insecure-content')\n",
    "    #fireFoxOptions.add_argument(\"--headless\")\n",
    "    #fireFoxOptions.add_argument(\"--disable-gpu\")\n",
    "    #fireFoxOptions.add_argument(\"--no-sandbox\")\n",
    "    #user_agent = 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.50 Safari/537.36'\n",
    "    #fireFoxOptions.add_argument(f'user-agent={user_agent}')\n",
    "    #driver = webdriver.Firefox(options=fireFoxOptions)\n",
    "    \n",
    "    #CHROME\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--disable-extensions\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\") # linux only\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.190 Safari/537.36\"\n",
    "    chrome_options.add_argument(f'user-agent={user_agent}')\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    driver = webdriver.Chrome()\n",
    "    # Initialize the driver\n",
    "\n",
    "\n",
    "    #Go the the searchpage of the Washington Post\n",
    "    driver.get(\"https://www.washingtonpost.com/search/?query=+&facets=%7B%22time%22%3A%22all%22%2C%22sort%22%3A%22relevancy%22%2C%22section%22%3A%5B%5D%2C%22author%22%3A%5B%5D%7D\")\n",
    "    #Accept Cookies\n",
    "    try:\n",
    "        driver.save_screenshot(\"cookies.png\")\n",
    "        WebDriverWait(driver,20).until(EC.presence_of_element_located((By.ID, \"onetrust-accept-btn-handler\"))).click()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    #Log-in into Washington Post account\n",
    "    try: #it is possible that account is already logged in! \n",
    "        #Sign in into Washington Post account\n",
    "        #Click on Sign In\n",
    "        #WebDriverWait(driver,10).until(EC.presence_of_element_located((By.XPATH, \"//*[@id='__next']/div[1]/nav/div[4]/div[2]/a/p\"))).click()\n",
    "        driver.get(\"https://www.washingtonpost.com/subscribe/signin/?next_url=https%3A%2F%2Fwww.washingtonpost.com&nid=top_pb_signin&arcId=&itid=nav_sign_in\")\n",
    "        time.sleep(5)\n",
    "        driver.save_screenshot(\"Login.png\")\n",
    "        #located username field\n",
    "        time.sleep(2) #field needs to completely load\n",
    "        username_field = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.XPATH, \"//*[@id='username']\")))\n",
    "        username_field.send_keys(username) #send username\n",
    "        username_field.send_keys(Keys.RETURN) #press enter\n",
    "\n",
    "        #Located password field\n",
    "        time.sleep(2) #field needs to completely load\n",
    "        password_field = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.XPATH, \"//*[@id='password']\")))\n",
    "        password_field.send_keys(password)\n",
    "        password_field.send_keys(Keys.ENTER)\n",
    "\n",
    "        #Go back to the search page\n",
    "        WebDriverWait(driver,10).until(EC.presence_of_element_located((By.CLASS_NAME, \"wpds-c-jlBemH \"))).click()\n",
    "        go_to_search = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.ID, \"query\")))\n",
    "        go_to_search.send_keys(search_term)\n",
    "        go_to_search.send_keys(Keys.ENTER)\n",
    "\n",
    "    except:\n",
    "        #you already logged-in -> continue \n",
    "        pass\n",
    "    \n",
    "    search_page_load = 0\n",
    "    while(search_page_load < 100):\n",
    "        try:\n",
    "            driver.get(\"https://www.washingtonpost.com/search/?query=+&facets=%7B%22time%22%3A%22all%22%2C%22sort%22%3A%22relevancy%22%2C%22section%22%3A%5B%5D%2C%22author%22%3A%5B%5D%7D\")\n",
    "            break\n",
    "        except:\n",
    "            search_page_load += 1\n",
    "            \n",
    "    scrape_time = time.time()\n",
    "    for single_date in daterange(start_date, end_date):\n",
    "        date = single_date.strftime(\"%m/%d/%y\")\n",
    "\n",
    "        #Locate the searchbar, send searchterm and press enter\n",
    "        \n",
    "        time.sleep(10)\n",
    "        driver.save_screenshot(\"Searchbar.png\")\n",
    "        searchbar = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.CLASS_NAME, \"aa-Input\")))\n",
    "        searchbar.send_keys(\" \")\n",
    "        searchbar.send_keys(Keys.ENTER)\n",
    "\n",
    "        #Select time period\n",
    "        #select periode specification\n",
    "        WebDriverWait(driver,10).until(EC.presence_of_element_located((By.XPATH, \"//*[@id='main-content']/div[1]/section[2]/div[1]/button/div/div[2]/span\"))).click()\n",
    "\n",
    "        #select custom time\n",
    "        WebDriverWait(driver,10).until(EC.presence_of_element_located((By.XPATH, \"//*[@id='main-content']/div[1]/section[2]/div[1]/div/div/div[1]/button/div/div[2]\"))).click()\n",
    "        time.sleep(1)\n",
    "\n",
    "        #Send start date\n",
    "        WebDriverWait(driver,10).until(EC.presence_of_element_located((By.XPATH, \"//*[@id='main-content']/div[1]/section[2]/div[1]/div/div/div[2]/div[1]/div/div/div[1]/input\"))).send_keys(date)\n",
    "\n",
    "        #Empty automatic end date\n",
    "        WebDriverWait(driver,10).until(EC.presence_of_element_located((By.XPATH, \"//*[@id='main-content']/div[1]/section[2]/div[1]/div/div/div[2]/div[2]/div/div/div[1]/input\"))).clear()\n",
    "\n",
    "        #Send end date\n",
    "        WebDriverWait(driver,10).until(EC.presence_of_element_located((By.XPATH, \"//*[@id='main-content']/div[1]/section[2]/div[1]/div/div/div[2]/div[2]/div/div/div[1]/input\"))).send_keys(date)\n",
    "\n",
    "        #Press Apply\n",
    "        WebDriverWait(driver,10).until(EC.presence_of_element_located((By.XPATH, \"//*[@id='main-content']/div[1]/section[2]/div[1]/div/div/div[2]/button[2]\"))).click()\n",
    "        time.sleep(5)\n",
    "        driver.refresh()\n",
    "        #click to load more on the page untill no longer possible\n",
    "        loading = True\n",
    "        while(loading):\n",
    "            try:\n",
    "                driver.save_screenshot(\"loading.png\")\n",
    "                WebDriverWait(driver,10).until(EC.presence_of_element_located((By.XPATH, \"//*[@id='main-content']/div[1]/section[3]/button\"))).click()\n",
    "            except:\n",
    "                loading = False\n",
    "        \n",
    "        driver.save_screenshot(\"articles.png\")\n",
    "        articles = WebDriverWait(driver,10).until(EC.presence_of_all_elements_located((By.TAG_NAME, \"article\")))\n",
    "\n",
    "        for a in range(0,len(articles)):\n",
    "            WP_link.append(articles[a].find_element(By.TAG_NAME, \"a\").get_attribute('href'))\n",
    "            WP_date.append(single_date.strftime(\"%d/%m/%Y\"))\n",
    "    \n",
    "    driver.quit()\n",
    "    \n",
    "    WP = [\"Washington_Post\"] * len(WP_link)\n",
    "    data = {\"Date\" : WP_date, \"News Paper\" : WP, \"Link\" : WP_link}\n",
    "    WashingtonPost = pd.DataFrame(data)\n",
    "\n",
    "    start_date = start_date.strftime(\"%d%m%Y\")\n",
    "    end_date = end_date.strftime(\"%d%m%Y\")\n",
    "    path = f\"Data/URLS/Washington_Post_{start_date}_{end_date}_URLS\"\n",
    "    #WashingtonPost.to_parquet(path) \n",
    "    return(WashingtonPost)\n",
    "\n",
    "    print(\"--- %s seconds ---\" % (time.time() - scrape_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c850d03",
   "metadata": {},
   "source": [
    "### 2.2.2. Function - Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc19fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWashingtonPostArticles(username, password, dataframe):\n",
    "    WP_article_content = []\n",
    "    WP_title = []\n",
    "\n",
    "    #FIREFOX\n",
    "    #Declare the driver and go to website\n",
    "    #fireFoxOptions = webdriver.FirefoxOptions()\n",
    "    #fireFoxOptions.add_argument('--ignore-certificate-errors')\n",
    "    #fireFoxOptions.add_argument('--allow-running-insecure-content')\n",
    "    #fireFoxOptions.add_argument(\"--headless\")\n",
    "    #fireFoxOptions.add_argument(\"--disable-gpu\")\n",
    "    #fireFoxOptions.add_argument(\"--no-sandbox\")\n",
    "    #user_agent = 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.50 Safari/537.36'\n",
    "    #fireFoxOptions.add_argument(f'user-agent={user_agent}')\n",
    "    #driver_WSJ = webdriver.Firefox(options=fireFoxOptions)\n",
    "\n",
    "    #CHROME\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--disable-extensions\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\") # linux only\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.190 Safari/537.36\"\n",
    "    chrome_options.add_argument(f'user-agent={user_agent}')\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    #driver = webdriver.Chrome()\n",
    "\n",
    "    #Go the the searchpage of the Washington Post\n",
    "    driver.get(\"https://www.washingtonpost.com/search/?query=+&facets=%7B%22time%22%3A%22all%22%2C%22sort%22%3A%22relevancy%22%2C%22section%22%3A%5B%5D%2C%22author%22%3A%5B%5D%7D\")\n",
    "\n",
    "    try:\n",
    "        #Accept Cookies\n",
    "        WebDriverWait(driver,10).until(EC.presence_of_element_located((By.ID, \"onetrust-accept-btn-handler\"))).click()\n",
    "    except:\n",
    "        print(\"no cookies\")\n",
    "        pass\n",
    "    #Log-in into Washington Post account\n",
    "    try: #it is possible that account is already logged in! \n",
    "\n",
    "        #Sign in into Washington Post account\n",
    "        #Click on Sign In\n",
    "        WebDriverWait(driver,10).until(EC.presence_of_element_located((By.XPATH, \"//*[@id='__next']/div[1]/nav/div[4]/div[2]/a/p\"))).click()\n",
    "\n",
    "        #located username field\n",
    "        time.sleep(2) #field needs to completely load\n",
    "        username_field = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.XPATH, \"//*[@id='username']\")))\n",
    "        username_field.send_keys(username) #send username\n",
    "        username_field.send_keys(Keys.RETURN) #press enter\n",
    "\n",
    "        #Located password field\n",
    "        time.sleep(2) #field needs to completely load\n",
    "        password_field = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.XPATH, \"//*[@id='password']\")))\n",
    "        password_field.send_keys(password)\n",
    "        password_field.send_keys(Keys.ENTER)\n",
    "\n",
    "        time.sleep(10)\n",
    "\n",
    "    except:\n",
    "        #you already logged-in -> continue \n",
    "        pass\n",
    "\n",
    "    scrape_time = time.time()\n",
    "    links = list(dataframe[\"Link\"])\n",
    "    for u in range(0, len(links)):\n",
    "            trys = 0\n",
    "            while(trys < 100):\n",
    "                try:\n",
    "                    #go to every article\n",
    "                    driver.get(links[u])\n",
    "                    break\n",
    "                except:\n",
    "                    trys += 1\n",
    "            try:\n",
    "                text = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.TAG_NAME, \"article\"))).text\n",
    "            except:\n",
    "                text = \"\"\n",
    "                \n",
    "            try:\n",
    "                text = text[text.index(\"Share\")+len(\"Share\")+1:text.rindex(\"Gift Article\")]\n",
    "                WP_article_content.append(text)\n",
    "            except:\n",
    "                try:\n",
    "                    text = text[text.index(\"Share\")+len(\"Share\")+1:text.rindex(\"Comments\")]\n",
    "                    WP_article_content.append(text)\n",
    "                except:\n",
    "                    try:\n",
    "                        text = text[text.index(\"tb.boedttibo\")+len(\"tb.boedttibo\")+1:text.rindex(\"Comments\")]\n",
    "                        WP_article_content.append(text)\n",
    "                    except:\n",
    "                        WP_article_content.append(text)\n",
    "                        pass\n",
    "            #idea is: We take the full text and before we have the article content the last pice of text is \"share\" and immediatly after the content\n",
    "            #we have \"comments\" this way we are able to only extract the article content\n",
    "            #the reasons for this is that each article has a more or less different html dependent on their \"type\"\n",
    "\n",
    "            #there are different formats of titles, in this order all titles are located\n",
    "            try:\n",
    "                title = driver.find_element(By.TAG_NAME, \"h1\").text\n",
    "                WP_title.append(title)\n",
    "            except:\n",
    "                WP_title.append(\"ERROR\")\n",
    "                pass\n",
    "\n",
    "    #Close driver\n",
    "    driver.quit()\n",
    "\n",
    "    #How long did the scraping took? \n",
    "    print(\"--- %s seconds ---\" % (time.time() - scrape_time))\n",
    "\n",
    "    #Create output file\n",
    "    #Make the dataframe\n",
    "    WP = [\"Washington Post\"] * len(WP_article_content)\n",
    "    data = {\"Title\" : WP_title, \"Text\" : WP_article_content, \"Date\" : dataframe[\"Date\"], \"News Paper\" : dataframe[\"News Paper\"], \"Link\" : dataframe[\"Link\"]}\n",
    "    WashingtonPost = pd.DataFrame(data)\n",
    "    return(WashingtonPost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d699e9f",
   "metadata": {},
   "source": [
    "## 2.3. New York Times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c29e7e",
   "metadata": {},
   "source": [
    "### 2.3.1. Function - Get URLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8d98ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNewYorkTimesURLS(start_date, end_date, username, password):\n",
    "    #put dates in date format\n",
    "    start_date = datetime.strptime(start_date, \"%d/%m/%Y\") \n",
    "    end_date = datetime.strptime(end_date, \"%d/%m/%Y\")\n",
    "\n",
    "    #Declare all list variables for the output\n",
    "    NYT_date = []\n",
    "    NYT_link = []\n",
    "\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--window-size=1920x1080\")\n",
    "    chrome_options.add_argument(\"--disable-extensions\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\") # linux only\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.81 Safari/537.36\"\n",
    "    chrome_options.add_argument(f'user-agent={user_agent}')\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    driver.get(\"https://myaccount.nytimes.com/auth/login?response_type=cookie&client_id=vi&redirect_uri=https%3A%2F%2Fwww.nytimes.com%2Fsubscription%2Fonboarding-offer%3FcampaignId%3D7JFJX%26EXIT_URI%3Dhttps%253A%252F%252Fwww.nytimes.com%252F&asset=masthead\")\n",
    "    time.sleep(5)\n",
    "    driver.save_screenshot(\"test.png\")\n",
    "    #not a robot\n",
    "    input(\"Press Enter to continue...\")\n",
    "\n",
    "    #Select log-in field\n",
    "    login = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.ID, \"email\")))\n",
    "\n",
    "    #Send username\n",
    "    login.send_keys(username)\n",
    "\n",
    "    #Press continue\n",
    "    WebDriverWait(driver,10).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"#myAccountAuth > div > div > div > form > div > div.css-bho3kg-buttonWrapper-buttonStyles-Button > button\"))).click()\n",
    "\n",
    "    #Select password field\n",
    "    password_field = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.ID, \"password\")))\n",
    "\n",
    "    #send password\n",
    "    password_field.send_keys(password)\n",
    "\n",
    "    #Press continue\n",
    "    WebDriverWait(driver,10).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"#myAccountAuth > div > div > form > div > div.css-1nkv26b-buttonWrapper-buttonStyles-Button > button\"))).click()\n",
    "\n",
    "    time.sleep(1)\n",
    "\n",
    "    #go to search page\n",
    "    search_page_loaded = 0\n",
    "    while(search_page_loaded < 100):\n",
    "        try:\n",
    "            driver.get(\"https://www.nytimes.com/search?dropmab=false&query=&sort=best\")\n",
    "            break\n",
    "        except:\n",
    "            search_page_loaded += 1\n",
    "\n",
    "    #Accept Cookies\n",
    "    try:\n",
    "        WebDriverWait(driver,5).until(EC.presence_of_element_located((By.CLASS_NAME, \"banner__container__cta--accept\"))).click()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    scrape_time = time.time()\n",
    "    for single_date in daterange(start_date, end_date):\n",
    "        date = single_date.strftime(\"%Y%m%d\")\n",
    "\n",
    "        url = f\"https://www.nytimes.com/search?dropmab=false&endDate={date}&query=%20&sort=best&startDate={date}\"\n",
    "        url_loaded = 0\n",
    "        while(url_loaded < 100):\n",
    "            try:\n",
    "                driver.get(url)\n",
    "                break\n",
    "            except:\n",
    "                url_loaded += 1\n",
    "\n",
    "        #click to load more on the page untill no longer possible\n",
    "        loading = True\n",
    "        while(loading):\n",
    "            try:\n",
    "                WebDriverWait(driver,10).until(EC.presence_of_element_located((By.XPATH, \"//*[@id='site-content']/div/div[2]/div[2]/div/button\"))).click()\n",
    "            except:\n",
    "                loading = False\n",
    "\n",
    "        articles = WebDriverWait(driver,10).until(EC.presence_of_all_elements_located((By.CLASS_NAME, \"css-1l4w6pd\")))\n",
    "        dates = WebDriverWait(driver,10).until(EC.presence_of_all_elements_located((By.CLASS_NAME, \"css-17ubb9w\")))\n",
    "\n",
    "        date_list = WebDriverWait(driver,10).until(EC.presence_of_all_elements_located((By.CLASS_NAME, \"css-17ubb9w\")))\n",
    "        true_date = date_list[0].text\n",
    "        for a in range(0, len(articles)):\n",
    "            if(true_date == date_list[a].text):\n",
    "                NYT_link.append(articles[a].find_element(By.TAG_NAME, \"a\").get_attribute('href'))\n",
    "                NYT_date.append(single_date.strftime(\"%d/%m/%Y\"))\n",
    "    \n",
    "    driver.quit()\n",
    "    \n",
    "    NYT = [\"New_York_Times\"] * len(NYT_link)\n",
    "    data = {\"Date\" : NYT_date, \"News Paper\" : NYT, \"Link\" : NYT_link}\n",
    "    New_York_Times = pd.DataFrame(data)\n",
    "    \n",
    "    print(\"--- %s seconds ---\" % (time.time() - scrape_time))\n",
    "    \n",
    "    return(New_York_Times)\n",
    "    start_date = start_date.strftime(\"%d%m%Y\")\n",
    "    end_date = end_date.strftime(\"%d%m%Y\")\n",
    "    path = f\"Data/URLS/New_York_Times_{start_date}_{end_date}_URLS\"\n",
    "    New_York_Times.to_parquet(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce3650f",
   "metadata": {},
   "source": [
    "## 2.4. Politico"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f600994",
   "metadata": {},
   "source": [
    "### 2.4.1. Function - Get URLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050ba84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPolitico(start_date, end_date):\n",
    "    #put dates in date format\n",
    "    end_date = datetime.strptime(end_date, \"%d/%m/%Y\")\n",
    "    start_date = datetime.strptime(start_date, \"%d/%m/%Y\")\n",
    "\n",
    "    #Declare all list variables for the output\n",
    "    P_date = []\n",
    "    P_link = []\n",
    "\n",
    "    fireFoxOptions = webdriver.FirefoxOptions()\n",
    "    fireFoxOptions.add_argument('--ignore-certificate-errors')\n",
    "    fireFoxOptions.add_argument('--allow-running-insecure-content')\n",
    "    fireFoxOptions.add_argument(\"--headless\")\n",
    "    fireFoxOptions.add_argument(\"--disable-gpu\")\n",
    "    fireFoxOptions.add_argument(\"--no-sandbox\")\n",
    "    user_agent = 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.50 Safari/537.36'\n",
    "    fireFoxOptions.add_argument(f'user-agent={user_agent}')\n",
    "    driver = webdriver.Firefox(options=fireFoxOptions)\n",
    "\n",
    "    #go to the website\n",
    "    driver.get(\"https://www.politico.com/search?adv=true\")\n",
    "\n",
    "    #accept cookies\n",
    "    try:\n",
    "        WebDriverWait(driver,10).until(EC.presence_of_element_located((By.ID, \"onetrust-accept-btn-handler\"))).click()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    scrape_time = time.time()\n",
    "    for single_date in daterange(start_date, end_date):\n",
    "        date = single_date.strftime(\"%m/%d/%Y\")\n",
    "        test = 0\n",
    "        while(test < 100):\n",
    "            try:\n",
    "                driver.get(f\"https://www.politico.com/search/1?adv=true&start={date}&end={date}\")\n",
    "                break\n",
    "            except:\n",
    "                test += 1\n",
    "                time.sleep(10)\n",
    "        articles = WebDriverWait(driver,10).until(EC.presence_of_all_elements_located((By.TAG_NAME, \"article\")))\n",
    "        for a in range(0, len(articles)):\n",
    "            P_link.append(articles[a].find_element(By.TAG_NAME, \"a\").get_attribute('href'))\n",
    "            P_date.append(date)\n",
    "\n",
    "        #get the number of pages to know how much you should click .pagination > ol:nth-child(2) > li:nth-child(3) > a:nth-child(1)\n",
    "        try:\n",
    "            nr_pages = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.CSS_SELECTOR, \".pagination > ol:nth-child(2) > li:nth-child(6) > a:nth-child(1)\"))).text\n",
    "        except:\n",
    "            nr_pages = 1\n",
    "            pass\n",
    "\n",
    "        for p in range(1, int(nr_pages)):\n",
    "            url = f\"https://www.politico.com/search/{p+1}?adv=true&start={date}&end={date}\"\n",
    "            driver.get(url)\n",
    "\n",
    "        articles = WebDriverWait(driver,10).until(EC.presence_of_all_elements_located((By.TAG_NAME, \"article\")))\n",
    "        for a in range(0, len(articles)):\n",
    "            P_link.append(articles[a].find_element(By.TAG_NAME, \"a\").get_attribute('href'))\n",
    "            P_date.append(single_date.strftime(\"%d/%m/%Y\"))\n",
    "\n",
    "    driver.quit()\n",
    "    print(\"--- %s seconds ---\" % (time.time() - scrape_time))\n",
    "\n",
    "    P = [\"Politico\"] * len(P_link)\n",
    "    data = {\"Date\" : P_date, \"News Paper\" : P, \"Link\" : P_link}\n",
    "    Politico = pd.DataFrame(data)\n",
    "\n",
    "    start_date = start_date.strftime(\"%d%m%Y\")\n",
    "    end_date = end_date.strftime(\"%d%m%Y\")\n",
    "    path = f\"Data/URLS/Politico_{start_date}_{end_date}_URLS\"\n",
    "    Politico.to_parquet(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0928dfb",
   "metadata": {},
   "source": [
    "# 3. Running Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc7e763",
   "metadata": {},
   "source": [
    "## 3.1. Wall Street Journal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa89ce63",
   "metadata": {},
   "source": [
    "## 3.2. Washington Post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5515224c",
   "metadata": {},
   "outputs": [],
   "source": [
    "WP_URLS = pd.read_parquet(\"Washington_Post_2017urls_to_scrape\").reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d40067a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(WP_URLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaf1e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_2017 = getWashingtonPostArticles(\"tb.boedttibo@gmail.com\", \"ThesisR&T\", WP_URLS[1:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c344bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.reset_index(drop = True)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb21e2bd",
   "metadata": {},
   "source": [
    "## 3.3. New York Times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc311562",
   "metadata": {},
   "source": [
    "## 3.4. Politico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70da76b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b04c4769",
   "metadata": {},
   "source": [
    "# 4. Analysing Scraping Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d476da0",
   "metadata": {},
   "source": [
    "## 4.1. Load in all Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffcb105",
   "metadata": {},
   "source": [
    "### Wall Street Journal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8784dbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the directory path\n",
    "directory = \"C:/Users/Boedt/OneDrive/Bureaublad/Scraped_Articles/Wall Street Journal\"\n",
    "\n",
    "# Get a list of filenames in the directory\n",
    "filenames = os.listdir(directory)\n",
    "\n",
    "# Loop through the filenames and read each Parquet file into a DataFrame\n",
    "dfs = []\n",
    "for filename in filenames:\n",
    "    filepath = os.path.join(directory, filename)\n",
    "    df = pd.read_parquet(filepath)\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "WSJ_df = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f06205",
   "metadata": {},
   "outputs": [],
   "source": [
    "WSJ_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80915afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set type of the date column to date\n",
    "WSJ_df[\"Date\"] = pd.to_datetime(WSJ_df['Date'], format='%d/%m/%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b34d3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove the index column\n",
    "WSJ_df = WSJ_df.drop(\"index\", axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ab407e",
   "metadata": {},
   "source": [
    "### Washington Post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "20bc424d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the directory path\n",
    "directory = \"C:/Users/Boedt/OneDrive/Bureaublad/Scraped_Articles/Washington Post\"\n",
    "\n",
    "# Get a list of filenames in the directory\n",
    "filenames = os.listdir(directory)\n",
    "\n",
    "# Loop through the filenames and read each Parquet file into a DataFrame\n",
    "dfs = []\n",
    "for filename in filenames:\n",
    "    filepath = os.path.join(directory, filename)\n",
    "    df = pd.read_parquet(filepath)\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "WP_df = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8cf68cc2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "      <th>Date</th>\n",
       "      <th>News Paper</th>\n",
       "      <th>Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Daniel Winnik was ‘shocked’ to be traded to Ca...</td>\n",
       "      <td>Around 10:45 p.m., a Toronto number Daniel Win...</td>\n",
       "      <td>29/02/2016</td>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>https://www.washingtonpost.com/news/capitals-i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JSOC commander tapped to lead Special Operatio...</td>\n",
       "      <td>Lt. Gen. Raymond “Tony” Thomas has been offici...</td>\n",
       "      <td>29/02/2016</td>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>https://www.washingtonpost.com/news/checkpoint...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chris Christie is setting himself up to be Rep...</td>\n",
       "      <td>Chris Christie is getting savaged for becoming...</td>\n",
       "      <td>29/02/2016</td>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>https://www.washingtonpost.com/news/the-fix/wp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lawsuit takes aim at asset forfeiture in Indiana</td>\n",
       "      <td>More here. Under the law, law enforcement agen...</td>\n",
       "      <td>29/02/2016</td>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>https://www.washingtonpost.com/news/the-watch/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Holly Holm says she won’t chase money with Ron...</td>\n",
       "      <td>Holly Holm fully understands that the most pro...</td>\n",
       "      <td>29/02/2016</td>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>https://www.washingtonpost.com/news/early-lead...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0  Daniel Winnik was ‘shocked’ to be traded to Ca...   \n",
       "1  JSOC commander tapped to lead Special Operatio...   \n",
       "2  Chris Christie is setting himself up to be Rep...   \n",
       "3   Lawsuit takes aim at asset forfeiture in Indiana   \n",
       "4  Holly Holm says she won’t chase money with Ron...   \n",
       "\n",
       "                                                Text        Date  \\\n",
       "0  Around 10:45 p.m., a Toronto number Daniel Win...  29/02/2016   \n",
       "1  Lt. Gen. Raymond “Tony” Thomas has been offici...  29/02/2016   \n",
       "2  Chris Christie is getting savaged for becoming...  29/02/2016   \n",
       "3  More here. Under the law, law enforcement agen...  29/02/2016   \n",
       "4  Holly Holm fully understands that the most pro...  29/02/2016   \n",
       "\n",
       "        News Paper                                               Link  \n",
       "0  Washington_Post  https://www.washingtonpost.com/news/capitals-i...  \n",
       "1  Washington_Post  https://www.washingtonpost.com/news/checkpoint...  \n",
       "2  Washington_Post  https://www.washingtonpost.com/news/the-fix/wp...  \n",
       "3  Washington_Post  https://www.washingtonpost.com/news/the-watch/...  \n",
       "4  Washington_Post  https://www.washingtonpost.com/news/early-lead...  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WP_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "87acc593",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set type of the date column to date\n",
    "WP_df[\"Date\"] = pd.to_datetime(WP_df['Date'], format='%d/%m/%Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036473c3",
   "metadata": {},
   "source": [
    "## 4.2. Clean URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4cc52b",
   "metadata": {},
   "source": [
    "### Wall Street Journal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52ac3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "WSJ_duplicate_URLs = WSJ_df['Link'].value_counts()\n",
    "WSJ_duplicate_URLs = WSJ_duplicate_URLs[WSJ_duplicate_URLs > 1].reset_index()\n",
    "WSJ_duplicate_URLs.columns = ['Link', 'Count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae5d1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "WSJ_duplicate_URLs[\"Link\"][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccd8eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "WSJ_df[WSJ_df[\"Link\"] == WSJ_duplicate_URLs[\"Link\"][5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24c32da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop all the duplicate rows\n",
    "WSJ_df = WSJ_df.drop_duplicates().reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c68cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some URLs of the Wall Street Journal contain the string \"mod=error_page\". Once we look into this we see that these URLs lead us\n",
    "#to articles that where published on a date outside the scope. We assume that these were URLs where something went wrong during\n",
    "#the scraping process and thus we received an error URL instead. We first remove these URLs from the dataframe\n",
    "mask = ~ WSJ_df['Link'].str.contains('mod=error_page')\n",
    "WSJ_df = WSJ_df[mask].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510897c6",
   "metadata": {},
   "source": [
    "### Washington Post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9c110848",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop all duplicates\n",
    "WP_df = WP_df.drop_duplicates().reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5c17c4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "WP_duplicate_URLs = WP_df['Text'].value_counts()\n",
    "WP_duplicate_URLs = WP_duplicate_URLs[WP_duplicate_URLs > 1].reset_index()\n",
    "WP_duplicate_URLs.columns = ['Link', 'Count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a23138ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Link</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>33779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Photo</td>\n",
       "      <td>3714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0\\nComments\\n</td>\n",
       "      <td>1668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1\\nComments\\n</td>\n",
       "      <td>683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2\\nComments\\n</td>\n",
       "      <td>556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3\\nComments\\n</td>\n",
       "      <td>454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Opinions to start the day, in your inbox. Sign...</td>\n",
       "      <td>416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2021 Election: Complete coverage and analysis\\n</td>\n",
       "      <td>401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4\\nComments\\n</td>\n",
       "      <td>339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5\\nComments\\n</td>\n",
       "      <td>297</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Link  Count\n",
       "0                                                     33779\n",
       "1                                              Photo   3714\n",
       "2                                      0\\nComments\\n   1668\n",
       "3                                      1\\nComments\\n    683\n",
       "4                                      2\\nComments\\n    556\n",
       "5                                      3\\nComments\\n    454\n",
       "6  Opinions to start the day, in your inbox. Sign...    416\n",
       "7    2021 Election: Complete coverage and analysis\\n    401\n",
       "8                                      4\\nComments\\n    339\n",
       "9                                      5\\nComments\\n    297"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WP_duplicate_URLs[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "27b50be5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['', 'Photo', 'HAWAII', ..., '889\\nComments\\n', '1617\\nComments\\n',\n",
       "       '1415\\nComments\\n'], dtype=object)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WP_df[WP_df[\"Text\"].str.split(\" \").str.len() == 1][\"Text\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e33de16",
   "metadata": {},
   "source": [
    "## 4.3. Unique URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7b339e",
   "metadata": {},
   "source": [
    "### Wall Street Journal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3d422d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we create a two new columns with the year, month of the articles -> this way we can look at the amount of URLs\n",
    "#per year and per month\n",
    "\n",
    "WSJ_df[\"Year\"] = WSJ_df['Date'].dt.year\n",
    "WSJ_df[\"Month\"] = WSJ_df[\"Date\"].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d430379",
   "metadata": {},
   "outputs": [],
   "source": [
    "WSJ_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d363bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unique URLs per year -> same article on different date is unique URL\n",
    "WSJ_URLs_Year = WSJ_df.groupby([\"News Paper\",\"Year\", \"Date\"])[\"Link\"].nunique().reset_index(name='Unique_URLs_Count')\n",
    "WSJ_URLs_Year.groupby(\"Year\").sum(\"Unique_URLs_Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2346b1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unique URLs per year -> same article on different date is no unique URL\n",
    "WSJ_URLs_Year = WSJ_df.groupby([\"News Paper\",\"Year\"])[\"Link\"].nunique().reset_index(name='Unique_URLs_Count')\n",
    "WSJ_URLs_Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edf9b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unique URLs per year\n",
    "WSJ_URLs_Month = WSJ_df.groupby([\"News Paper\",\"Year\", \"Month\"])[\"Link\"].nunique().reset_index(name='Unique_URLs_Count')\n",
    "#WSJ_URLs_Month"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2e0e8e",
   "metadata": {},
   "source": [
    "### Washington Post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "3c448fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we create a two new columns with the year, month of the articles -> this way we can look at the amount of URLs\n",
    "#per year and per month\n",
    "\n",
    "WP_df[\"Year\"] = WP_df['Date'].dt.year\n",
    "WP_df[\"Month\"] = WP_df[\"Date\"].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f20af619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "      <th>Date</th>\n",
       "      <th>News Paper</th>\n",
       "      <th>Link</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Daniel Winnik was ‘shocked’ to be traded to Ca...</td>\n",
       "      <td>Around 10:45 p.m., a Toronto number Daniel Win...</td>\n",
       "      <td>2016-02-29</td>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>https://www.washingtonpost.com/news/capitals-i...</td>\n",
       "      <td>2016</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JSOC commander tapped to lead Special Operatio...</td>\n",
       "      <td>Lt. Gen. Raymond “Tony” Thomas has been offici...</td>\n",
       "      <td>2016-02-29</td>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>https://www.washingtonpost.com/news/checkpoint...</td>\n",
       "      <td>2016</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chris Christie is setting himself up to be Rep...</td>\n",
       "      <td>Chris Christie is getting savaged for becoming...</td>\n",
       "      <td>2016-02-29</td>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>https://www.washingtonpost.com/news/the-fix/wp...</td>\n",
       "      <td>2016</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0  Daniel Winnik was ‘shocked’ to be traded to Ca...   \n",
       "1  JSOC commander tapped to lead Special Operatio...   \n",
       "2  Chris Christie is setting himself up to be Rep...   \n",
       "\n",
       "                                                Text       Date  \\\n",
       "0  Around 10:45 p.m., a Toronto number Daniel Win... 2016-02-29   \n",
       "1  Lt. Gen. Raymond “Tony” Thomas has been offici... 2016-02-29   \n",
       "2  Chris Christie is getting savaged for becoming... 2016-02-29   \n",
       "\n",
       "        News Paper                                               Link  Year  \\\n",
       "0  Washington_Post  https://www.washingtonpost.com/news/capitals-i...  2016   \n",
       "1  Washington_Post  https://www.washingtonpost.com/news/checkpoint...  2016   \n",
       "2  Washington_Post  https://www.washingtonpost.com/news/the-fix/wp...  2016   \n",
       "\n",
       "   Month  \n",
       "0      2  \n",
       "1      2  \n",
       "2      2  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WP_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "40f8e5fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>News Paper</th>\n",
       "      <th>Year</th>\n",
       "      <th>Unique_URLs_Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>2016</td>\n",
       "      <td>49664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>2017</td>\n",
       "      <td>49800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>2018</td>\n",
       "      <td>27888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>2019</td>\n",
       "      <td>10233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Washington_Post</td>\n",
       "      <td>2020</td>\n",
       "      <td>13697</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        News Paper  Year  Unique_URLs_Count\n",
       "0  Washington_Post  2016              49664\n",
       "1  Washington_Post  2017              49800\n",
       "2  Washington_Post  2018              27888\n",
       "3  Washington_Post  2019              10233\n",
       "4  Washington_Post  2020              13697"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Unique URLs per year\n",
    "WP_URLs_Year = WP_df.groupby([\"News Paper\",\"Year\"])[\"Link\"].nunique().reset_index(name='Unique_URLs_Count')\n",
    "WP_URLs_Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca20ddc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unique URLs per year\n",
    "WP_URLs_Month = WP_df.groupby([\"News Paper\",\"Year\", \"Month\"])[\"Link\"].nunique().reset_index(name='Unique_URLs_Count')\n",
    "#WP_URLs_Month"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47e8194",
   "metadata": {},
   "source": [
    "## 4.4. Clean Articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd72e2ed",
   "metadata": {},
   "source": [
    "### Wall Street Journal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ad049b",
   "metadata": {},
   "source": [
    "#### Remove all articles which we don't have access to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06af56a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove all articles that have the text \"NO ACCESS\" -> we didn't have access to these\n",
    "remove_NO_ACCESS = ~ WSJ_df[\"Text\"].str.contains(\"NO ACCESS\")\n",
    "WSJ_df = WSJ_df[remove_NO_ACCESS].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f7ce5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "WSJ_df.drop_duplicates().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500fb79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "WSJ_df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5190b1d",
   "metadata": {},
   "source": [
    "#### Empty text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a683ed",
   "metadata": {},
   "source": [
    "Articles where we didn't collect text -> was either rescraped, or there was no text to begin with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9853a611",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove all blank space in the text\n",
    "remove_blank_text_df = WSJ_df\n",
    "remove_blank_text_df[\"Text\"] = remove_blank_text_df['Text'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b8aabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "WSJ_df = WSJ_df.drop(remove_blank_text_df[remove_blank_text_df[\"Text\"] == \"\"].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a543071",
   "metadata": {},
   "outputs": [],
   "source": [
    "WSJ_df.drop_duplicates().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835ef5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "WSJ_df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4770c8",
   "metadata": {},
   "source": [
    "#### Other text that failed to be collected or is not usefull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15603be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#These articles text only contains the end of the article, but we didn't scrape the article itself\n",
    "WSJ_df = WSJ_df[~WSJ_df[\"Text\"].str.startswith('Write to')].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e78a15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "WSJ_df.drop_duplicates().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc21502",
   "metadata": {},
   "outputs": [],
   "source": [
    "WSJ_df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3c518d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some articles had a PDF you had to download\n",
    "WSJ_df = WSJ_df[WSJ_df[\"Text\"] != \"Download PDF\"].reset_index(drop = True)\n",
    "WSJ_df = WSJ_df[WSJ_df[\"Text\"] != \"See Solution Download PDF\"].reset_index(drop = True)\n",
    "WSJ_df = WSJ_df[WSJ_df[\"Text\"] != \"Download PDF See Solution\"].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142ff060",
   "metadata": {},
   "source": [
    "### Washington Post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279e963b",
   "metadata": {},
   "outputs": [],
   "source": [
    "WSJ_df.groupby([\"News Paper\",\"Year\", \"Date\"])[\"Link\"].nunique().reset_index(name='Unique_URLs_Count').groupby(\"Year\").sum(\"Unique_URLs_Count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5713ac6a",
   "metadata": {},
   "source": [
    "## 4.5. Articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d01bef7",
   "metadata": {},
   "source": [
    "### Wall Street Journal"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
