{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b21dbe2",
   "metadata": {},
   "source": [
    "# 0. Packages & Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0154ec9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "from nltk import ngrams\n",
    "from collections import Counter\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, pipeline, AutoModelForSequenceClassification, AutoModelForSeq2SeqLM\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pyarrow\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d4be568",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre-process text for lexicon based approaches\n",
    "def preprocess_text(text):\n",
    "    # remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # convert to lower case\n",
    "    text = text.lower()\n",
    "    # remove blank spaces\n",
    "    text = ' '.join(text.split())\n",
    "    # remove newline characters\n",
    "    text = text.replace('\\n', '')\n",
    "    return text\n",
    "\n",
    "def text_cleaning_final(text):\n",
    "    # remove blank spaces\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    # remove newline characters\n",
    "    text = text.replace('\\n', '')\n",
    "    \n",
    "    # Use regular expressions to match punctuation marks\n",
    "    pattern = r'([!\"#$%&\\'()*+,-./:;<=>?@\\[\\\\\\]^_`{|}~])'\n",
    "    # Replace punctuation marks with whitespaces before and after them\n",
    "    text = re.sub(pattern, r' \\1 ', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25fa7ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_lexicon_words(text_df, lexicon):\n",
    "    lexicon = lexicon[\"Lexicon\"]\n",
    "    frequency = []\n",
    "    present = []\n",
    "    rfrequency = []\n",
    "    rpresent = []\n",
    "\n",
    "    for text in tqdm(text_df[\"Text\"]):\n",
    "        lexicon_counts = 0\n",
    "        present_count = 0\n",
    "        for word in lexicon:\n",
    "            lexicon_counts += text.lower().count(word.lower())\n",
    "            if(text.lower().count(word.lower()) > 0):\n",
    "                present_count += 1\n",
    "        \n",
    "        word_list = text.split() \n",
    "        word_count = len(word_list)\n",
    "\n",
    "        frequency.append(lexicon_counts)\n",
    "        present.append(present_count)\n",
    "        rfrequency.append((lexicon_counts/word_count)*100)\n",
    "        rpresent.append((present_count/word_count)*100)\n",
    "        \n",
    "        \n",
    "    text_df[\"Absolute Frequency\"] = frequency\n",
    "    text_df[\"Absolute Present\"] = present\n",
    "    text_df[\"Relative Frequency\"] = rfrequency\n",
    "    text_df[\"Relative Present\"] = rpresent\n",
    "    \n",
    "    return(text_df)\n",
    "\n",
    "def get_metrics(df, colname, threshold):\n",
    "    target = []\n",
    "    values = df[colname]\n",
    "    \n",
    "    for v in values:\n",
    "        if v >= threshold:\n",
    "            target.append(\"Yes\")\n",
    "        else:\n",
    "            target.append(\"No\")\n",
    "    \n",
    "    df[\"Estimate\"] = target\n",
    "    \n",
    "    cross_table = pd.crosstab(df['Target'], df['Estimate'], margins=True)\n",
    "    accuracy = (cross_table.iloc[0, 0] + cross_table.iloc[1, 1]) / cross_table.loc['All', 'All'] if cross_table.shape == (3,3) else cross_table.iloc[1,0] / (cross_table.iloc[1, 0] + cross_table.iloc[0,0]) \n",
    "    precision = cross_table.iloc[1,1] / (cross_table.iloc[0,1] + cross_table.iloc[1,1]) if cross_table.shape == (3,3) else 0\n",
    "    recall = cross_table.iloc[1,1] / (cross_table.iloc[1,0] + cross_table.iloc[1,1]) if cross_table.shape == (3,3) != 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "    return([accuracy, precision, recall, f1_score])\n",
    "\n",
    "def get_gross_table_data(df, colname, threshold, binary):\n",
    "    target = []\n",
    "    values = df[colname]\n",
    "    \n",
    "    for v in values:\n",
    "        if v >= threshold:\n",
    "            target.append(\"Yes\")\n",
    "        else:\n",
    "            target.append(\"No\")\n",
    "    \n",
    "    df[\"Estimate\"] = target\n",
    "    \n",
    "    if(binary):\n",
    "        cross_table = pd.crosstab(df['Target'], df['Estimate'], margins=True)\n",
    "    else:\n",
    "        cross_table = pd.crosstab(df['Final_Climate_Change_Level_Label'], df['Estimate'], margins=True)\n",
    "    return(cross_table)\n",
    "    \n",
    "def find_optimal_threshold(df, lexicon, lexicon_name):\n",
    "    df = count_lexicon_words(df, lexicon)\n",
    "    \n",
    "    #absolute frequency\n",
    "    af_accuracy = 0\n",
    "    af_th = 0\n",
    "    found = False\n",
    "    while(found == False):\n",
    "        metrics = get_metrics(df, \"Absolute Frequency\", af_th)\n",
    "        if metrics[0] > af_accuracy:\n",
    "            af_accuracy = metrics[0]\n",
    "            af_precision = metrics[1]\n",
    "            af_recall = metrics[2]\n",
    "            af_f1 = metrics[3]\n",
    "            af_th += 1\n",
    "        else:\n",
    "            found = True\n",
    "            \n",
    "    #absolute present\n",
    "    ap_accuracy = 0\n",
    "    ap_th = 0\n",
    "    found = False\n",
    "    while(found == False):\n",
    "        metrics = get_metrics(df, \"Absolute Present\", ap_th)\n",
    "        if metrics[0] > ap_accuracy:\n",
    "            ap_accuracy = metrics[0]\n",
    "            ap_precision = metrics[1]\n",
    "            ap_recall = metrics[2]\n",
    "            ap_f1 = metrics[3]\n",
    "            ap_th += 1\n",
    "        else:\n",
    "            found = True\n",
    "            \n",
    "    #relative frequency\n",
    "    rf_accuracy = 0\n",
    "    rf_th = 0\n",
    "    found = False\n",
    "    while(found == False):\n",
    "        metrics = get_metrics(df, \"Relative Frequency\", rf_th)\n",
    "        if metrics[0] > rf_accuracy:\n",
    "            rf_accuracy = metrics[0]\n",
    "            rf_precision = metrics[1]\n",
    "            rf_recall = metrics[2]\n",
    "            rf_f1 = metrics[3]\n",
    "            rf_th += 0.1\n",
    "        else:\n",
    "            found = True\n",
    "            \n",
    "    #relative present\n",
    "    rp_accuracy = 0\n",
    "    rp_th = 0\n",
    "    found = False\n",
    "    while(found == False):\n",
    "        metrics = get_metrics(df, \"Relative Present\", rp_th)\n",
    "        if metrics[0] > rp_accuracy:\n",
    "            rp_accuracy = metrics[0]\n",
    "            rp_precision = metrics[1]\n",
    "            rp_recall = metrics[2]\n",
    "            rp_f1 = metrics[3]\n",
    "            rp_th += 0.1\n",
    "        else:\n",
    "            found = True\n",
    "    \n",
    "    return(pd.DataFrame({\"Lexicon\" : [lexicon_name] * 4, \n",
    "                         \"Technique\" : [\"Absolute Frequency\", \"Absolute Present\", \"Relative Frequency\", \"Relative Present\"],\n",
    "                         \"Threshold\" : [af_th - 1, ap_th - 1, rf_th - 0.1, rp_th - 0.1], \n",
    "                 \"Accuracy\" : [af_accuracy, ap_accuracy, rf_accuracy, rp_accuracy], \n",
    "                        \"Precision\" : [af_precision, ap_precision, rf_precision, rp_precision], \n",
    "                        \"Recall\" : [af_recall, ap_recall, rf_recall, rp_recall], \n",
    "                        \"F1 Score\" : [af_f1, ap_f1, rf_f1, rp_f1]}))\n",
    "    \n",
    "def test_lexicon(test_df, results_df, lexicon, lexicon_name):\n",
    "    techniques = [\"Absolute Frequency\", \"Absolute Present\", \"Relative Frequency\", \"Relative Present\"]\n",
    "    df = count_lexicon_words(test_df, lexicon)\n",
    "    accuracy = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1 = []\n",
    "    th_df = []\n",
    "    for t in range(len(techniques)):\n",
    "        th = results_df[results_df[\"Technique\"] == techniques[t]][\"Threshold\"].iloc[0]\n",
    "        th_df.append(th)\n",
    "        accuracy.append(get_metrics(df, techniques[t], th)[0])\n",
    "        precision.append(get_metrics(df, techniques[t], th)[1])\n",
    "        recall.append(get_metrics(df, techniques[t], th)[2])\n",
    "        f1.append(get_metrics(df, techniques[t], th)[3])\n",
    "    \n",
    "    return(pd.DataFrame({\"Lexicon\" : [lexicon_name] * 4, \"Technique\" : techniques, \"Threshold\" : th_df ,\n",
    "                         \"Test Accuracy\" : accuracy, \n",
    "                        \"Test Precision\" : precision, \n",
    "                        \"Test Recall\" : recall, \n",
    "                        \"Test F1 Score\" : f1}))\n",
    "\n",
    "\n",
    "\n",
    "def get_cross_table(text_df, lexicon, threshold, colname, binary):\n",
    "    df = count_lexicon_words(text_df, lexicon)\n",
    "    return(get_gross_table_data(df, colname, threshold, binary))\n",
    "\n",
    "def get_most_words_used(text_df, lexicon):\n",
    "    lexicon = lexicon[\"Lexicon\"]\n",
    "    output = {}\n",
    "\n",
    "    for text in tqdm(text_df[\"Text\"]):\n",
    "        for word in lexicon:\n",
    "            if(text.lower().count(word.lower()) > 0):\n",
    "                if word in output:\n",
    "                    # Increment the value by 1\n",
    "                    output[word] += 1\n",
    "                else:\n",
    "                    # Add the value to the dictionary with an initial count of 1\n",
    "                    output[word] = 1\n",
    "\n",
    "    \n",
    "    return(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25c89c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary(df_with_text, name, model_name, max_lenght_input=-1):\n",
    "    data_in_list = df_with_text[name].tolist()\n",
    "    tokenizer_sum = AutoTokenizer.from_pretrained(model_name)\n",
    "    model_sum = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "    summarizer = pipeline('summarization', model=model_sum, tokenizer = tokenizer_sum) \n",
    "\n",
    "    if max_lenght_input>=0:\n",
    "        df_with_text['summary'] = summarizer(data_in_list, max_length=max_lenght_input)\n",
    "\n",
    "    else:\n",
    "        df_with_text['summary'] = summarizer(data_in_list)\n",
    "\n",
    "def classification(df_with_text, name, model_name, max_lenght_input=-1):\n",
    "    data_in_list = df_with_text[name].tolist()\n",
    "    tokenizer_clas = AutoTokenizer.from_pretrained(model_name)\n",
    "    model_clas = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "    classification = pipeline('text-classification', model=model_clas, tokenizer = tokenizer_clas) \n",
    "\n",
    "    if max_lenght_input>=0:\n",
    "        df_with_text['classification'] = classification(data_in_list, max_length=max_lenght_input, truncation=True)\n",
    "\n",
    "    else:\n",
    "        df_with_text['classification'] = classification(data_in_list)\n",
    "        \n",
    "    return(df_with_text)\n",
    "\n",
    "def get_metrics_hugging_face(text_df, text_column, model, tokens):\n",
    "    \n",
    "    df = classification(text_df, text_column, model, tokens)\n",
    "    label_list = list(df[\"classification\"])\n",
    "    labels = [entry['label'] for entry in label_list]\n",
    "    df[\"Label_Hugging\"] = labels\n",
    "    \n",
    "    cross_table = pd.crosstab(df['Target'], df['Label_Hugging'], margins=True)\n",
    "    \n",
    "    # calculate classification metrics using scikit-learn\n",
    "    accuracy = (cross_table.iloc[0, 0] + cross_table.iloc[1, 1]) / cross_table.loc['All', 'All'] if cross_table.shape == (3,3) else cross_table.iloc[1,0] / (cross_table.iloc[1, 0] + cross_table.iloc[0,0]) \n",
    "    precision = cross_table.iloc[1,1] / (cross_table.iloc[0,1] + cross_table.iloc[1,1]) if cross_table.shape == (3,3) else 0\n",
    "    recall = cross_table.iloc[1,1] / (cross_table.iloc[1,0] + cross_table.iloc[1,1]) if cross_table.shape == (3,3) != 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "\n",
    "\n",
    "    # print the metrics\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1 score:\", f1_score)\n",
    "    print(cross_table)\n",
    "    print(\"\\n\")\n",
    "    print(pd.crosstab(df['Final_Climate_Change_Level_Label'], df['Label_Hugging'], margins=True))\n",
    "    \n",
    "\n",
    "def get_metrics_df_hugging_face(text_df, text_column, model, tokens, model_name):\n",
    "    df = classification(text_df, text_column, model, tokens)\n",
    "    label_list = list(df[\"classification\"])\n",
    "    labels = [entry['label'] for entry in label_list]\n",
    "    df[\"Label_Hugging\"] = labels\n",
    "    \n",
    "    cross_table = pd.crosstab(df['Target'], df['Label_Hugging'], margins=True)\n",
    "    \n",
    "    # calculate classification metrics using scikit-learn\n",
    "    accuracy = (cross_table.iloc[0, 0] + cross_table.iloc[1, 1]) / cross_table.loc['All', 'All'] if cross_table.shape == (3,3) else cross_table.iloc[1,0] / (cross_table.iloc[1, 0] + cross_table.iloc[0,0]) \n",
    "    precision = cross_table.iloc[1,1] / (cross_table.iloc[0,1] + cross_table.iloc[1,1]) if cross_table.shape == (3,3) else 0\n",
    "    recall = cross_table.iloc[1,1] / (cross_table.iloc[1,0] + cross_table.iloc[1,1]) if cross_table.shape == (3,3) != 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "    \n",
    "    return(pd.DataFrame({\"Model\" : [model_name], \"Accuracy\" : [accuracy], \"Precision\" : [precision], \"Recall\" : [recall],\n",
    "                         \"F1 Score\" : [f1_score]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfa34b3",
   "metadata": {},
   "source": [
    "# 1. Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2bf7db0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Link</th>\n",
       "      <th>Final_Climate_Change_Level_Label</th>\n",
       "      <th>Final_Sentiment_Label</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>On a Train trip north toward Aberdeen, the Sc...</td>\n",
       "      <td>https://www.wsj.com/articles/surfacing-review-...</td>\n",
       "      <td>Medium</td>\n",
       "      <td>-1</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A container area at the Yangshan Deep Water Po...</td>\n",
       "      <td>https://www.washingtonpost.com/news/monkey-cag...</td>\n",
       "      <td>No Climate</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This week, New York City observed an annual r...</td>\n",
       "      <td>https://www.wsj.com/articles/lets-get-the-un-o...</td>\n",
       "      <td>No Climate</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>During a visit to Detroit last year, Presiden...</td>\n",
       "      <td>https://www.wsj.com/articles/make-cars-great-a...</td>\n",
       "      <td>High</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Investors holding more than $5 billion in Exx...</td>\n",
       "      <td>http://www.wsj.com/articles/calpers-pushes-exx...</td>\n",
       "      <td>High</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>No spending cuts to Medicaid? Then no tax cuts...</td>\n",
       "      <td>https://www.washingtonpost.com/news/fact-check...</td>\n",
       "      <td>No Climate</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>The U.S. dollar edged higher Tuesday, maintai...</td>\n",
       "      <td>https://www.wsj.com/articles/u-s-dollar-edges-...</td>\n",
       "      <td>No Climate</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>Lots of today’s board games are just jazzed-u...</td>\n",
       "      <td>http://www.wsj.com/articles/the-many-guises-of...</td>\n",
       "      <td>No Climate</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>The new $1,000 iPhone 11 Pro and the $1,100 iP...</td>\n",
       "      <td>https://www.washingtonpost.com/technology/2019...</td>\n",
       "      <td>No Climate</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>Oil driller Transocean Ltd. swung to a first-...</td>\n",
       "      <td>http://www.wsj.com/articles/transocean-swings-...</td>\n",
       "      <td>No Climate</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Text  \\\n",
       "0     On a Train trip north toward Aberdeen, the Sc...   \n",
       "1    A container area at the Yangshan Deep Water Po...   \n",
       "2     This week, New York City observed an annual r...   \n",
       "3     During a visit to Detroit last year, Presiden...   \n",
       "4     Investors holding more than $5 billion in Exx...   \n",
       "..                                                 ...   \n",
       "495  No spending cuts to Medicaid? Then no tax cuts...   \n",
       "496   The U.S. dollar edged higher Tuesday, maintai...   \n",
       "497   Lots of today’s board games are just jazzed-u...   \n",
       "498  The new $1,000 iPhone 11 Pro and the $1,100 iP...   \n",
       "499   Oil driller Transocean Ltd. swung to a first-...   \n",
       "\n",
       "                                                  Link  \\\n",
       "0    https://www.wsj.com/articles/surfacing-review-...   \n",
       "1    https://www.washingtonpost.com/news/monkey-cag...   \n",
       "2    https://www.wsj.com/articles/lets-get-the-un-o...   \n",
       "3    https://www.wsj.com/articles/make-cars-great-a...   \n",
       "4    http://www.wsj.com/articles/calpers-pushes-exx...   \n",
       "..                                                 ...   \n",
       "495  https://www.washingtonpost.com/news/fact-check...   \n",
       "496  https://www.wsj.com/articles/u-s-dollar-edges-...   \n",
       "497  http://www.wsj.com/articles/the-many-guises-of...   \n",
       "498  https://www.washingtonpost.com/technology/2019...   \n",
       "499  http://www.wsj.com/articles/transocean-swings-...   \n",
       "\n",
       "    Final_Climate_Change_Level_Label  Final_Sentiment_Label Target  \n",
       "0                             Medium                     -1    Yes  \n",
       "1                         No Climate                      0     No  \n",
       "2                         No Climate                      0     No  \n",
       "3                               High                      1    Yes  \n",
       "4                               High                      1    Yes  \n",
       "..                               ...                    ...    ...  \n",
       "495                       No Climate                      0     No  \n",
       "496                       No Climate                      0     No  \n",
       "497                       No Climate                      0     No  \n",
       "498                       No Climate                      0     No  \n",
       "499                       No Climate                      0     No  \n",
       "\n",
       "[500 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_climate_df = pd.read_parquet(\"Climate_Labels_Dataset.parquet\")\n",
    "\n",
    "#Clean the tabel\n",
    "tag_climate_df['Final_Climate_Change_Level_Label'] = tag_climate_df['Final_Climate_Change_Level_Label'].str.strip()\n",
    "tag_climate_df.loc[tag_climate_df[\"Final_Climate_Change_Level_Label\"] == \"NA\", \"Final_Climate_Change_Level_Label\"] = \"Na\"\n",
    "tag_climate_df.loc[tag_climate_df[\"Final_Climate_Change_Level_Label\"] == \"0\", \"Final_Climate_Change_Level_Label\"] = \"Na\"\n",
    "tag_climate_df.loc[tag_climate_df[\"Final_Climate_Change_Level_Label\"] == \"Na\", \"Final_Climate_Change_Level_Label\"] = \"No Climate\"\n",
    "tag_climate_df[\"Target\"] = tag_climate_df[\"Final_Climate_Change_Level_Label\"].apply(lambda x: \"Yes\" if x in [\"High\", \"Medium\"] else \"No\")\n",
    "tag_climate_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5418e92f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Final_Climate_Change_Level_Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>High</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Medium</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No Climate</td>\n",
       "      <td>307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Small</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Final_Climate_Change_Level_Label  Text\n",
       "0                             High    57\n",
       "1                           Medium    42\n",
       "2                       No Climate   307\n",
       "3                            Small    94"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overview_labels_hms = tag_climate_df.groupby(\"Final_Climate_Change_Level_Label\")[\"Text\"].count().reset_index()\n",
    "overview_labels_hms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b25c3c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No</td>\n",
       "      <td>401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yes</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Target  Text\n",
       "0     No   401\n",
       "1    Yes    99"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overview_labels = tag_climate_df.groupby(\"Target\")[\"Text\"].count().reset_index()\n",
    "overview_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d5bef2",
   "metadata": {},
   "source": [
    "## Splits in Train en Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5931a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataframe into two sets\n",
    "df_train, df_test = train_test_split(tag_climate_df, test_size = 0.3, random_state = 23)\n",
    "df_train = df_train.reset_index(drop = True)\n",
    "df_test = df_test.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0765752f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Final_Climate_Change_Level_Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>High</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Medium</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No Climate</td>\n",
       "      <td>213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Small</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Final_Climate_Change_Level_Label  Text\n",
       "0                             High    38\n",
       "1                           Medium    35\n",
       "2                       No Climate   213\n",
       "3                            Small    64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.groupby(\"Final_Climate_Change_Level_Label\")[\"Text\"].count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6a621a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No</td>\n",
       "      <td>277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yes</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Target  Text\n",
       "0     No   277\n",
       "1    Yes    73"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.groupby(\"Target\")[\"Text\"].count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd3722a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Final_Climate_Change_Level_Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>High</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Medium</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No Climate</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Small</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Final_Climate_Change_Level_Label  Text\n",
       "0                             High    19\n",
       "1                           Medium     7\n",
       "2                       No Climate    94\n",
       "3                            Small    30"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.groupby(\"Final_Climate_Change_Level_Label\")[\"Text\"].count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3320d1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yes</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Target  Text\n",
       "0     No   124\n",
       "1    Yes    26"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.groupby(\"Target\")[\"Text\"].count().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ad56cb",
   "metadata": {},
   "source": [
    "# 2. Lexicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fc2163",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the lexicon\n",
    "UNDP_Lexicon = pd.read_csv(\"Lexicons/UNDP_Lexicon\")\n",
    "UNDP_Lexicon = UNDP_Lexicon.drop_duplicates().reset_index(drop = True)\n",
    "UNDP_Lexicon[\"Lexicon\"] = UNDP_Lexicon[\"Lexicon\"].str.lower()\n",
    "UNDP_Lexicon = pd.DataFrame(UNDP_Lexicon[\"Lexicon\"])\n",
    "UNDP_Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabba077",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the lexicon\n",
    "IPCC_Lexicon = pd.read_csv(\"Lexicons/IPCC_Lexicon\")\n",
    "IPCC_Lexicon = IPCC_Lexicon.drop_duplicates().reset_index(drop = True)\n",
    "IPCC_Lexicon[\"Lexicon\"] = IPCC_Lexicon[\"Lexicon\"].str.lower()\n",
    "IPCC_Lexicon = pd.DataFrame(IPCC_Lexicon[\"Lexicon\"])\n",
    "IPCC_Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a69ff34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the lexicon\n",
    "EPA_Lexicon = pd.read_csv(\"Lexicons/EPA_Lexicon\")\n",
    "EPA_Lexicon = EPA_Lexicon.drop_duplicates().reset_index(drop = True)\n",
    "EPA_Lexicon[\"Lexicon\"] = EPA_Lexicon[\"Lexicon\"].str.lower()\n",
    "EPA_Lexicon = pd.DataFrame(EPA_Lexicon[\"Lexicon\"])\n",
    "\n",
    "EPA_Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e55e2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the lexicon\n",
    "Wikipedia_Lexicon = pd.read_csv(\"Lexicons/Wikipedia_Lexicon\")\n",
    "Wikipedia_Lexicon = Wikipedia_Lexicon.drop_duplicates().reset_index(drop = True)\n",
    "Wikipedia_Lexicon[\"Lexicon\"] = Wikipedia_Lexicon[\"Lexicon\"].str.lower()\n",
    "Wikipedia_Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2627bec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the lexicon\n",
    "Global_Change_Lexicon = pd.read_csv(\"Lexicons/Global_Change_Lexicon\")\n",
    "Global_Change_Lexicon = Global_Change_Lexicon.drop_duplicates().reset_index(drop = True)\n",
    "Global_Change_Lexicon[\"Lexicon\"] = Global_Change_Lexicon[\"Lexicon\"].str.lower()\n",
    "Global_Change_Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bc5049",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the lexicon\n",
    "BBC_Lexicon = pd.read_csv(\"Lexicons/BBC_Lexicon\")\n",
    "BBC_Lexicon = BBC_Lexicon.drop_duplicates().reset_index(drop = True)\n",
    "BBC_Lexicon[\"Lexicon\"] = BBC_Lexicon[\"Lexicon\"].str.lower()\n",
    "\n",
    "BBC_Lexicon = pd.DataFrame(BBC_Lexicon[\"Lexicon\"])\n",
    "BBC_Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca15ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([EPA_Lexicon, BBC_Lexicon]).drop_duplicates().reset_index(drop = True).to_csv(\"Lexicons/EPA_BBC_Lexicon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62316160",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an empty dataframe and write a function to fill with the values\n",
    "\n",
    "common_words_df = pd.DataFrame({\"Lexicon\" : [\"Global Change\", \"IPCC\", \"Wikipedia\", \"EPA\", \"BBC\", \"UNDP\"], \n",
    "                               \"Global Change\": [0, 0, 0, 0, 0, 0], \"IPCC\" : [0, 0, 0, 0, 0, 0], \n",
    "                               \"Wikipedia\" : [0, 0, 0, 0, 0, 0], \"EPA\" : [0, 0, 0, 0, 0, 0], \n",
    "                               \"BBC\" : [0, 0, 0, 0, 0, 0], \"UNDP\" : [0, 0, 0, 0, 0, 0]})\n",
    "\n",
    "dfs = [Global_Change_Lexicon, IPCC_Lexicon, Wikipedia_Lexicon, EPA_Lexicon, BBC_Lexicon, UNDP_Lexicon]\n",
    "\n",
    "for r in range(0, len(dfs)):\n",
    "    for c in range(0, len(dfs)):\n",
    "        # Get the common values between the two columns\n",
    "        common_words = set(dfs[r]['Lexicon']).intersection(set(dfs[c]['Lexicon']))\n",
    "        common_words_df.loc[r, common_words_df.columns[c +1]] = len(common_words)/len(dfs[c]['Lexicon'])\n",
    "\n",
    "common_words_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b01b5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an empty dataframe and write a function to fill with the values\n",
    "\n",
    "common_words_df = pd.DataFrame({\"Lexicon\" : [\"Global Change\", \"IPCC\", \"Wikipedia\", \"EPA\", \"BBC\", \"UNDP\"], \n",
    "                               \"Global Change\": [0, 0, 0, 0, 0, 0], \"IPCC\" : [0, 0, 0, 0, 0, 0], \n",
    "                               \"Wikipedia\" : [0, 0, 0, 0, 0, 0], \"EPA\" : [0, 0, 0, 0, 0, 0], \n",
    "                               \"BBC\" : [0, 0, 0, 0, 0, 0], \"UNDP\" : [0, 0, 0, 0, 0, 0]})\n",
    "\n",
    "dfs = [Global_Change_Lexicon, IPCC_Lexicon, Wikipedia_Lexicon, EPA_Lexicon, BBC_Lexicon, UNDP_Lexicon]\n",
    "\n",
    "for r in range(0, len(dfs)):\n",
    "    for c in range(0, len(dfs)):\n",
    "        # Get the common values between the two columns\n",
    "        common_words = set(dfs[r]['Lexicon']).intersection(set(dfs[c]['Lexicon']))\n",
    "        common_words_df.loc[r, common_words_df.columns[c +1]] = len(common_words)\n",
    "\n",
    "common_words_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1106d880",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [Global_Change_Lexicon, IPCC_Lexicon, Wikipedia_Lexicon, EPA_Lexicon, BBC_Lexicon, UNDP_Lexicon]\n",
    "non_unique_words = []\n",
    "unique_words = []\n",
    "total_words = []\n",
    "for r in range(len(dfs)):\n",
    "    common_words = []\n",
    "    for c in range(len(dfs)):\n",
    "        if c != r:\n",
    "            # Get the common values between the two columns\n",
    "            common_words.extend(list(set(dfs[r]['Lexicon']).intersection(set(dfs[c]['Lexicon']))))\n",
    "    common_words = list(set(common_words))  # Remove duplicates by converting to a set and back to a list\n",
    "    total_words.append(len(dfs[r][\"Lexicon\"]))\n",
    "    unique_words.append(len(dfs[r][\"Lexicon\"]) - len(common_words))\n",
    "    non_unique_words.append(len(common_words))\n",
    "\n",
    "non_unique_words\n",
    "\n",
    "unique_words_df = pd.DataFrame({\"Lexicon\" : [\"Global Change\", \"IPCC\", \"Wikipedia\", \"EPA\", \"BBC\", \"UNDP\"], \n",
    "                                \"Non unique words\" : non_unique_words, \"unique words\" : unique_words, \n",
    "                               \"total_words\" : total_words})\n",
    "\n",
    "unique_words_df[\"Richness\"] = unique_words_df[\"unique words\"] / unique_words_df[\"total_words\"]\n",
    "\n",
    "unique_words_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fb7b13",
   "metadata": {},
   "source": [
    "# 3. Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6340f21",
   "metadata": {},
   "source": [
    "## 3.1. Lexicons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518f9459",
   "metadata": {},
   "source": [
    "### 3.1.1. Train Lexicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42af9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Text_df = df_train.copy()\n",
    "Text_df[\"Text\"] = Text_df[\"Text\"].apply(preprocess_text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebadcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "names = [\"IPCC\", \"Global_Change\", \"UNDP\", \"EPA\", \"Wikipedia\", \"BBC\"]\n",
    "All_names = []\n",
    "# Generate all combinations\n",
    "all_combinations = []\n",
    "for r in range(1, len(names) + 1):\n",
    "    combinations_r = combinations(names, r)\n",
    "    all_combinations.extend(combinations_r)\n",
    "\n",
    "# Print all combinations\n",
    "for combination in all_combinations:\n",
    "    combined_string = \"_\".join(combination)\n",
    "    All_names.append(combined_string)\n",
    "    \n",
    "Lexicons = [IPCC_Lexicon, Global_Change_Lexicon, UNDP_Lexicon, EPA_Lexicon, Wikipedia_Lexicon, BBC_Lexicon]\n",
    "All_Lexicons = []\n",
    "# Generate all combinations\n",
    "all_combinations = []\n",
    "for r in range(1, len(Lexicons) + 1):\n",
    "    combinations_r = combinations(Lexicons, r)\n",
    "    all_combinations.extend(combinations_r)\n",
    "\n",
    "# Concatenate and print all combinations\n",
    "for combination in all_combinations:\n",
    "    combined_df = pd.concat(combination, axis=0).drop_duplicates().reset_index()\n",
    "    All_Lexicons.append(combined_df)\n",
    "    \n",
    "results_df = pd.DataFrame()\n",
    "\n",
    "start_results = time.time()\n",
    "\n",
    "print(\"--- %s seconds ---\" % (start_results - start))\n",
    "\n",
    "for i in range(len(All_Lexicons)):\n",
    "    r_df = find_optimal_threshold(Text_df, All_Lexicons[i], All_names[i])\n",
    "    results_df = pd.concat([results_df, r_df])\n",
    "\n",
    "results_df = results_df.reset_index(drop = True)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c26fee",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results_df.sort_values(by = \"Accuracy\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e93d6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_parquet(\"Classification_Results\\Lexicon_Tagging_Train_Results.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68434da",
   "metadata": {},
   "source": [
    "### 3.1.2. Test Lexicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f74ff59",
   "metadata": {},
   "outputs": [],
   "source": [
    "Text_df_test = df_test.copy()\n",
    "Text_df_test[\"Text\"] = Text_df_test[\"Text\"].apply(preprocess_text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cd1c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "names = [\"IPCC\", \"Global_Change\", \"UNDP\", \"EPA\", \"Wikipedia\", \"BBC\"]\n",
    "All_names = []\n",
    "# Generate all combinations\n",
    "all_combinations = []\n",
    "for r in range(1, len(names) + 1):\n",
    "    combinations_r = combinations(names, r)\n",
    "    all_combinations.extend(combinations_r)\n",
    "\n",
    "# Print all combinations\n",
    "for combination in all_combinations:\n",
    "    combined_string = \"_\".join(combination)\n",
    "    All_names.append(combined_string)\n",
    "    \n",
    "Lexicons = [IPCC_Lexicon, Global_Change_Lexicon, UNDP_Lexicon, EPA_Lexicon, Wikipedia_Lexicon, BBC_Lexicon]\n",
    "All_Lexicons = []\n",
    "# Generate all combinations\n",
    "all_combinations = []\n",
    "for r in range(1, len(Lexicons) + 1):\n",
    "    combinations_r = combinations(Lexicons, r)\n",
    "    all_combinations.extend(combinations_r)\n",
    "\n",
    "# Concatenate and print all combinations\n",
    "for combination in all_combinations:\n",
    "    combined_df = pd.concat(combination, axis=0).drop_duplicates().reset_index()\n",
    "    All_Lexicons.append(combined_df)\n",
    "    \n",
    "test_result_df = pd.DataFrame()\n",
    "\n",
    "start_results = time.time()\n",
    "\n",
    "print(\"--- %s seconds ---\" % (start_results - start))\n",
    "\n",
    "for n in range(0, len(All_names)):\n",
    "    test_lexicon_result = test_lexicon(Text_df_test, results_df[results_df[\"Lexicon\"] == All_names[n]], All_Lexicons[All_names.index(All_names[n])], All_names[n])\n",
    "    test_result_df = pd.concat([test_result_df, test_lexicon_result])\n",
    "    \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fdeef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result_df.sort_values(by = \"Test Accuracy\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86b1827",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result_df.to_parquet(\"Classification_Results\\Lexicon_Tagging_Test_Results.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9ec1e5",
   "metadata": {},
   "source": [
    "## 3.2. Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84af67e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_test_df = df_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7964bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert1 = get_metrics_df_hugging_face(bert_test_df, 'Text',\"climatebert/environmental-claims\",512, \"climatebert/environmental-claims\")\n",
    "bert2 = get_metrics_df_hugging_face(bert_test_df, \"Text\", \"climatebert/distilroberta-base-climate-detector\", 512, \"climatebert/distilroberta-base-climate-detector\")\n",
    "bert_df = pd.concat([bert1, bert2]).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a8f4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bf9786",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_df.to_parquet(\"Classification_Results\\Bert_Tagging_Test_Results.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cedaec8",
   "metadata": {},
   "source": [
    "# 4. Comparing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "61d2e135",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lexicon_train_result_df = pd.read_parquet(\"Classification_Results\\Lexicon_Tagging_Train_Results.parquet\")\n",
    "Lexicon_test_result_df = pd.read_parquet(\"Classification_Results\\Lexicon_Tagging_Test_Results.parquet\")\n",
    "Bert_test_result_df = pd.read_parquet(\"Classification_Results\\Bert_Tagging_Test_Results.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e17a2237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lexicon</th>\n",
       "      <th>Technique</th>\n",
       "      <th>Threshold</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EPA_BBC</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.739726</td>\n",
       "      <td>0.812030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EPA_Wikipedia_BBC</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.739726</td>\n",
       "      <td>0.812030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EPA_BBC</td>\n",
       "      <td>Relative Frequency</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.925714</td>\n",
       "      <td>0.790123</td>\n",
       "      <td>0.876712</td>\n",
       "      <td>0.831169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UNDP_EPA_Wikipedia</td>\n",
       "      <td>Relative Frequency</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.925714</td>\n",
       "      <td>0.813333</td>\n",
       "      <td>0.835616</td>\n",
       "      <td>0.824324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EPA</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.925714</td>\n",
       "      <td>0.861538</td>\n",
       "      <td>0.767123</td>\n",
       "      <td>0.811594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>UNDP_EPA</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.925714</td>\n",
       "      <td>0.861538</td>\n",
       "      <td>0.767123</td>\n",
       "      <td>0.811594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>UNDP_EPA_BBC</td>\n",
       "      <td>Relative Frequency</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.925714</td>\n",
       "      <td>0.783133</td>\n",
       "      <td>0.890411</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>UNDP_EPA</td>\n",
       "      <td>Relative Frequency</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.925714</td>\n",
       "      <td>0.821918</td>\n",
       "      <td>0.821918</td>\n",
       "      <td>0.821918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>UNDP_EPA_BBC</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.925714</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.780822</td>\n",
       "      <td>0.814286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>EPA_Wikipedia</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.925714</td>\n",
       "      <td>0.898305</td>\n",
       "      <td>0.726027</td>\n",
       "      <td>0.803030</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Lexicon           Technique  Threshold  Accuracy  Precision  \\\n",
       "0             EPA_BBC  Absolute Frequency        7.0  0.928571   0.900000   \n",
       "1   EPA_Wikipedia_BBC  Absolute Frequency        7.0  0.928571   0.900000   \n",
       "2             EPA_BBC  Relative Frequency        0.5  0.925714   0.790123   \n",
       "3  UNDP_EPA_Wikipedia  Relative Frequency        0.6  0.925714   0.813333   \n",
       "4                 EPA  Absolute Frequency        6.0  0.925714   0.861538   \n",
       "5            UNDP_EPA  Absolute Frequency        6.0  0.925714   0.861538   \n",
       "6        UNDP_EPA_BBC  Relative Frequency        0.5  0.925714   0.783133   \n",
       "7            UNDP_EPA  Relative Frequency        0.6  0.925714   0.821918   \n",
       "8        UNDP_EPA_BBC  Absolute Frequency        6.0  0.925714   0.850746   \n",
       "9       EPA_Wikipedia  Absolute Frequency        7.0  0.925714   0.898305   \n",
       "\n",
       "     Recall  F1 Score  \n",
       "0  0.739726  0.812030  \n",
       "1  0.739726  0.812030  \n",
       "2  0.876712  0.831169  \n",
       "3  0.835616  0.824324  \n",
       "4  0.767123  0.811594  \n",
       "5  0.767123  0.811594  \n",
       "6  0.890411  0.833333  \n",
       "7  0.821918  0.821918  \n",
       "8  0.780822  0.814286  \n",
       "9  0.726027  0.803030  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Lexicon_train_result_df.sort_values(by = \"Accuracy\", ascending = False).head(10).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "31f13431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lexicon</th>\n",
       "      <th>Technique</th>\n",
       "      <th>Threshold</th>\n",
       "      <th>Test Precision</th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Test Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.653846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EPA_BBC</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.884615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UNDP_Wikipedia</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.769231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wikipedia_BBC</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.769231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UNDP_Wikipedia_BBC</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.769231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>UNDP_Wikipedia_BBC</td>\n",
       "      <td>Relative Present</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.692308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>BBC</td>\n",
       "      <td>Relative Present</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.926667</td>\n",
       "      <td>0.653846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Wikipedia_BBC</td>\n",
       "      <td>Relative Present</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.926667</td>\n",
       "      <td>0.653846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Global_Change_UNDP_Wikipedia</td>\n",
       "      <td>Relative Frequency</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.926667</td>\n",
       "      <td>0.653846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>EPA_Wikipedia_BBC</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.884615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Lexicon           Technique  Threshold  \\\n",
       "0                     Wikipedia  Absolute Frequency        7.0   \n",
       "1                       EPA_BBC  Absolute Frequency        7.0   \n",
       "2                UNDP_Wikipedia  Absolute Frequency        7.0   \n",
       "3                 Wikipedia_BBC  Absolute Frequency        7.0   \n",
       "4            UNDP_Wikipedia_BBC  Absolute Frequency        7.0   \n",
       "5            UNDP_Wikipedia_BBC    Relative Present        0.4   \n",
       "6                           BBC    Relative Present        0.2   \n",
       "7                 Wikipedia_BBC    Relative Present        0.4   \n",
       "8  Global_Change_UNDP_Wikipedia  Relative Frequency        1.1   \n",
       "9             EPA_Wikipedia_BBC  Absolute Frequency        7.0   \n",
       "\n",
       "   Test Precision  Test Accuracy  Test Recall  \n",
       "0        0.944444       0.933333     0.653846  \n",
       "1        0.920000       0.966667     0.884615  \n",
       "2        0.909091       0.946667     0.769231  \n",
       "3        0.909091       0.946667     0.769231  \n",
       "4        0.909091       0.946667     0.769231  \n",
       "5        0.900000       0.933333     0.692308  \n",
       "6        0.894737       0.926667     0.653846  \n",
       "7        0.894737       0.926667     0.653846  \n",
       "8        0.894737       0.926667     0.653846  \n",
       "9        0.884615       0.960000     0.884615  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Lexicon_test_result_df.sort_values(by = [\"Test Precision\", \"Test Accuracy\"], ascending = False).head(10).reset_index(drop = True)[[\"Lexicon\", \"Technique\", \"Threshold\", \"Test Precision\", \"Test Accuracy\", \"Test Recall\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0eef69c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lexicon</th>\n",
       "      <th>Technique</th>\n",
       "      <th>Threshold</th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Test Precision</th>\n",
       "      <th>Test Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EPA_BBC</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.884615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EPA_Wikipedia_BBC</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UNDP_EPA_Wikipedia_BBC</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EPA</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UNDP_EPA</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>UNDP_EPA_Wikipedia</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>UNDP_EPA_BBC</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.961538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>EPA_Wikipedia</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.953333</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.846154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>BBC</td>\n",
       "      <td>Relative Frequency</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.953333</td>\n",
       "      <td>0.827586</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>UNDP_Wikipedia</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.769231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Lexicon           Technique  Threshold  Test Accuracy  \\\n",
       "0                 EPA_BBC  Absolute Frequency        7.0       0.966667   \n",
       "1       EPA_Wikipedia_BBC  Absolute Frequency        7.0       0.960000   \n",
       "2  UNDP_EPA_Wikipedia_BBC  Absolute Frequency        7.0       0.960000   \n",
       "3                     EPA  Absolute Frequency        6.0       0.960000   \n",
       "4                UNDP_EPA  Absolute Frequency        6.0       0.960000   \n",
       "5      UNDP_EPA_Wikipedia  Absolute Frequency        6.0       0.960000   \n",
       "6            UNDP_EPA_BBC  Absolute Frequency        6.0       0.960000   \n",
       "7           EPA_Wikipedia  Absolute Frequency        7.0       0.953333   \n",
       "8                     BBC  Relative Frequency        0.2       0.953333   \n",
       "9          UNDP_Wikipedia  Absolute Frequency        7.0       0.946667   \n",
       "\n",
       "   Test Precision  Test Recall  \n",
       "0        0.920000     0.884615  \n",
       "1        0.884615     0.884615  \n",
       "2        0.884615     0.884615  \n",
       "3        0.857143     0.923077  \n",
       "4        0.857143     0.923077  \n",
       "5        0.857143     0.923077  \n",
       "6        0.833333     0.961538  \n",
       "7        0.880000     0.846154  \n",
       "8        0.827586     0.923077  \n",
       "9        0.909091     0.769231  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Lexicon_test_result_df.sort_values(by = [\"Test Accuracy\", \"Test Precision\"], ascending = False).head(10).reset_index(drop = True)[[\"Lexicon\", \"Technique\", \"Threshold\", \"Test Accuracy\", \"Test Precision\", \"Test Recall\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7508a2cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lexicon</th>\n",
       "      <th>Technique</th>\n",
       "      <th>Threshold</th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Test Precision</th>\n",
       "      <th>Test Recall</th>\n",
       "      <th>Test F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EPA_BBC</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.901961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UNDP_EPA_BBC</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.892857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EPA</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UNDP_EPA</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UNDP_EPA_Wikipedia</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>EPA_Wikipedia_BBC</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>UNDP_EPA_Wikipedia_BBC</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.884615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>BBC</td>\n",
       "      <td>Relative Frequency</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.953333</td>\n",
       "      <td>0.827586</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.872727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>EPA_Wikipedia</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.953333</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.862745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>EPA</td>\n",
       "      <td>Relative Frequency</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.862069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Lexicon           Technique  Threshold  Test Accuracy  \\\n",
       "0                 EPA_BBC  Absolute Frequency        7.0       0.966667   \n",
       "1            UNDP_EPA_BBC  Absolute Frequency        6.0       0.960000   \n",
       "2                     EPA  Absolute Frequency        6.0       0.960000   \n",
       "3                UNDP_EPA  Absolute Frequency        6.0       0.960000   \n",
       "4      UNDP_EPA_Wikipedia  Absolute Frequency        6.0       0.960000   \n",
       "5       EPA_Wikipedia_BBC  Absolute Frequency        7.0       0.960000   \n",
       "6  UNDP_EPA_Wikipedia_BBC  Absolute Frequency        7.0       0.960000   \n",
       "7                     BBC  Relative Frequency        0.2       0.953333   \n",
       "8           EPA_Wikipedia  Absolute Frequency        7.0       0.953333   \n",
       "9                     EPA  Relative Frequency        0.5       0.946667   \n",
       "\n",
       "   Test Precision  Test Recall  Test F1 Score  \n",
       "0        0.920000     0.884615       0.901961  \n",
       "1        0.833333     0.961538       0.892857  \n",
       "2        0.857143     0.923077       0.888889  \n",
       "3        0.857143     0.923077       0.888889  \n",
       "4        0.857143     0.923077       0.888889  \n",
       "5        0.884615     0.884615       0.884615  \n",
       "6        0.884615     0.884615       0.884615  \n",
       "7        0.827586     0.923077       0.872727  \n",
       "8        0.880000     0.846154       0.862745  \n",
       "9        0.781250     0.961538       0.862069  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Lexicon_test_result_df.sort_values(by = [\"Test F1 Score\", \"Test Accuracy\", \"Test Precision\"], ascending = False).head(10).reset_index(drop = True)[[\"Lexicon\", \"Technique\", \"Threshold\", \"Test Accuracy\", \"Test Precision\", \"Test Recall\", \"Test F1 Score\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fae941",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lexicons_names = [\"BBC\", \"EPA\", \"Wikipedia\", \"UNDP\", \"IPCC\", \"Global_Change\"]\n",
    "Solo_Lexicons_df = Lexicon_test_result_df[Lexicon_test_result_df[\"Lexicon\"].isin(Lexicons_names)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fecb9654",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(Solo_Lexicons_df.groupby(\"Lexicon\")[\"Test Accuracy\"].max()).sort_values(by = \"Test Accuracy\", ascending = False).reset_index(\"Lexicon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "68b6afd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = Solo_Lexicons_df.groupby([\"Lexicon\"])['Test Accuracy'].transform(max) == Solo_Lexicons_df['Test Accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "355de7eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lexicon</th>\n",
       "      <th>Technique</th>\n",
       "      <th>Threshold</th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Test Precision</th>\n",
       "      <th>Test Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>EPA</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>BBC</td>\n",
       "      <td>Relative Frequency</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.953333</td>\n",
       "      <td>0.827586</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>UNDP</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.774194</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>Relative Frequency</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.774194</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IPCC</td>\n",
       "      <td>Relative Frequency</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.807692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Global_Change</td>\n",
       "      <td>Absolute Frequency</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.853333</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.538462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Global_Change</td>\n",
       "      <td>Relative Present</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.853333</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.269231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Lexicon           Technique  Threshold  Test Accuracy  \\\n",
       "12            EPA  Absolute Frequency        6.0       0.960000   \n",
       "22            BBC  Relative Frequency        0.2       0.953333   \n",
       "8            UNDP  Absolute Frequency        4.0       0.940000   \n",
       "18      Wikipedia  Relative Frequency        0.5       0.940000   \n",
       "2            IPCC  Relative Frequency        1.3       0.920000   \n",
       "4   Global_Change  Absolute Frequency        5.0       0.853333   \n",
       "7   Global_Change    Relative Present        0.5       0.853333   \n",
       "\n",
       "    Test Precision  Test Recall  \n",
       "12        0.857143     0.923077  \n",
       "22        0.827586     0.923077  \n",
       "8         0.774194     0.923077  \n",
       "18        0.774194     0.923077  \n",
       "2         0.750000     0.807692  \n",
       "4         0.583333     0.538462  \n",
       "7         0.700000     0.269231  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Solo_Lexicons_df[idx].sort_values(by = \"Test Accuracy\", ascending = False).drop(\"Test F1 Score\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d8b8ada1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Solo_Lexicons_df = Solo_Lexicons_df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a1d2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lexicon_test_result_df.groupby(\"Technique\")[\"Test Accuracy\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "58729d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 150/150 [00:01<00:00, 139.78it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 150/150 [00:00<00:00, 371.36it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 150/150 [00:00<00:00, 495.21it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 150/150 [00:01<00:00, 138.88it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 150/150 [00:02<00:00, 71.40it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 150/150 [00:00<00:00, 227.32it/s]\n"
     ]
    }
   ],
   "source": [
    "data = get_most_words_used(Text_df_test, EPA_Lexicon)\n",
    "EPA_words_used = pd.DataFrame.from_dict(data, orient='index', columns=['Count']).reset_index().rename(columns={'index': 'Word'})\n",
    "\n",
    "data = get_most_words_used(Text_df_test, BBC_Lexicon)\n",
    "BBC_words_used = pd.DataFrame.from_dict(data, orient='index', columns=['Count']).reset_index().rename(columns={'index': 'Word'})\n",
    "\n",
    "data = get_most_words_used(Text_df_test, UNDP_Lexicon)\n",
    "UNDP_words_used = pd.DataFrame.from_dict(data, orient='index', columns=['Count']).reset_index().rename(columns={'index': 'Word'})\n",
    "\n",
    "data = get_most_words_used(Text_df_test, Wikipedia_Lexicon)\n",
    "Wikipedia_words_used = pd.DataFrame.from_dict(data, orient='index', columns=['Count']).reset_index().rename(columns={'index': 'Word'})\n",
    "\n",
    "data = get_most_words_used(Text_df_test, IPCC_Lexicon)\n",
    "IPCC_words_used = pd.DataFrame.from_dict(data, orient='index', columns=['Count']).reset_index().rename(columns={'index': 'Word'})\n",
    "\n",
    "data = get_most_words_used(Text_df_test, Global_Change_Lexicon)\n",
    "Global_Change_words_used = pd.DataFrame.from_dict(data, orient='index', columns=['Count']).reset_index().rename(columns={'index': 'Word'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a2687333",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPA_top = EPA_words_used.sort_values(\"Count\", ascending = False).head(10).reset_index(drop = True)\n",
    "EPA_top.columns = [\"EPA\", \"Count\"]\n",
    "BBC_top = BBC_words_used.sort_values(\"Count\", ascending = False).head(10).reset_index(drop = True)\n",
    "BBC_top.columns = [\"BBC\", \"Count\"]\n",
    "Wikipedia_top = Wikipedia_words_used.sort_values(\"Count\", ascending = False).head(10).reset_index(drop = True)\n",
    "Wikipedia_top.columns = [\"Wikipedia\", \"Count\"]\n",
    "UNDP_top = UNDP_words_used.sort_values(\"Count\", ascending = False).head(10).reset_index(drop = True)\n",
    "UNDP_top.columns = [\"UNDP\", \"Count\"]\n",
    "IPCC_top = IPCC_words_used.sort_values(\"Count\", ascending = False).head(10).reset_index(drop = True)\n",
    "IPCC_top.columns = [\"IPCC\", \"Count\"]\n",
    "Global_Change_top = Global_Change_words_used.sort_values(\"Count\", ascending = False).head(10).reset_index(drop = True)\n",
    "Global_Change_top.columns = [\"Global Change\", \"Count\"]\n",
    "\n",
    "top_df = pd.concat([EPA_top, BBC_top, Wikipedia_top, UNDP_top, IPCC_top, Global_Change_top], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f9e27fa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EPA</th>\n",
       "      <th>Count</th>\n",
       "      <th>BBC</th>\n",
       "      <th>Count</th>\n",
       "      <th>Wikipedia</th>\n",
       "      <th>Count</th>\n",
       "      <th>UNDP</th>\n",
       "      <th>Count</th>\n",
       "      <th>IPCC</th>\n",
       "      <th>Count</th>\n",
       "      <th>Global Change</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>climate</td>\n",
       "      <td>62</td>\n",
       "      <td>climate change</td>\n",
       "      <td>59</td>\n",
       "      <td>climate</td>\n",
       "      <td>62</td>\n",
       "      <td>climate</td>\n",
       "      <td>62</td>\n",
       "      <td>climate</td>\n",
       "      <td>62</td>\n",
       "      <td>climate change</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>climate change</td>\n",
       "      <td>59</td>\n",
       "      <td>fossil fuels</td>\n",
       "      <td>7</td>\n",
       "      <td>climate change</td>\n",
       "      <td>59</td>\n",
       "      <td>climate change</td>\n",
       "      <td>59</td>\n",
       "      <td>climate change</td>\n",
       "      <td>59</td>\n",
       "      <td>risk</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>emissions</td>\n",
       "      <td>13</td>\n",
       "      <td>global warming</td>\n",
       "      <td>7</td>\n",
       "      <td>weather</td>\n",
       "      <td>10</td>\n",
       "      <td>weather</td>\n",
       "      <td>10</td>\n",
       "      <td>risk</td>\n",
       "      <td>38</td>\n",
       "      <td>value</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>weather</td>\n",
       "      <td>10</td>\n",
       "      <td>renewable energy</td>\n",
       "      <td>5</td>\n",
       "      <td>fossil fuel</td>\n",
       "      <td>8</td>\n",
       "      <td>global warming</td>\n",
       "      <td>7</td>\n",
       "      <td>agreement</td>\n",
       "      <td>30</td>\n",
       "      <td>evolution</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fossil fuel</td>\n",
       "      <td>8</td>\n",
       "      <td>methane</td>\n",
       "      <td>3</td>\n",
       "      <td>global warming</td>\n",
       "      <td>7</td>\n",
       "      <td>renewable energy</td>\n",
       "      <td>5</td>\n",
       "      <td>social</td>\n",
       "      <td>28</td>\n",
       "      <td>uncertainty</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>global warming</td>\n",
       "      <td>7</td>\n",
       "      <td>greenhouse gases</td>\n",
       "      <td>2</td>\n",
       "      <td>greenhouse gas</td>\n",
       "      <td>5</td>\n",
       "      <td>transparency</td>\n",
       "      <td>3</td>\n",
       "      <td>region</td>\n",
       "      <td>24</td>\n",
       "      <td>global warming</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>atmosphere</td>\n",
       "      <td>6</td>\n",
       "      <td>mitigation</td>\n",
       "      <td>2</td>\n",
       "      <td>ozone</td>\n",
       "      <td>4</td>\n",
       "      <td>adaptation</td>\n",
       "      <td>2</td>\n",
       "      <td>model</td>\n",
       "      <td>24</td>\n",
       "      <td>feedback</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>renewable energy</td>\n",
       "      <td>5</td>\n",
       "      <td>adaptation</td>\n",
       "      <td>2</td>\n",
       "      <td>argo</td>\n",
       "      <td>4</td>\n",
       "      <td>mitigation</td>\n",
       "      <td>2</td>\n",
       "      <td>policies</td>\n",
       "      <td>23</td>\n",
       "      <td>scenario</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>greenhouse gas</td>\n",
       "      <td>5</td>\n",
       "      <td>carbon dioxide</td>\n",
       "      <td>2</td>\n",
       "      <td>methane</td>\n",
       "      <td>3</td>\n",
       "      <td>climate crisis</td>\n",
       "      <td>2</td>\n",
       "      <td>institution</td>\n",
       "      <td>17</td>\n",
       "      <td>sink</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ozone</td>\n",
       "      <td>4</td>\n",
       "      <td>hockey stick</td>\n",
       "      <td>1</td>\n",
       "      <td>climate crisis</td>\n",
       "      <td>2</td>\n",
       "      <td>decarbonization</td>\n",
       "      <td>2</td>\n",
       "      <td>evidence</td>\n",
       "      <td>16</td>\n",
       "      <td>ozone</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                EPA  Count               BBC  Count       Wikipedia  Count  \\\n",
       "0           climate     62    climate change     59         climate     62   \n",
       "1    climate change     59      fossil fuels      7  climate change     59   \n",
       "2         emissions     13    global warming      7         weather     10   \n",
       "3           weather     10  renewable energy      5     fossil fuel      8   \n",
       "4       fossil fuel      8           methane      3  global warming      7   \n",
       "5    global warming      7  greenhouse gases      2  greenhouse gas      5   \n",
       "6        atmosphere      6        mitigation      2           ozone      4   \n",
       "7  renewable energy      5        adaptation      2            argo      4   \n",
       "8    greenhouse gas      5    carbon dioxide      2         methane      3   \n",
       "9             ozone      4      hockey stick      1  climate crisis      2   \n",
       "\n",
       "               UNDP  Count            IPCC  Count   Global Change  Count  \n",
       "0           climate     62         climate     62  climate change     59  \n",
       "1    climate change     59  climate change     59            risk     38  \n",
       "2           weather     10            risk     38           value     26  \n",
       "3    global warming      7       agreement     30       evolution     13  \n",
       "4  renewable energy      5          social     28     uncertainty     12  \n",
       "5      transparency      3          region     24  global warming      7  \n",
       "6        adaptation      2           model     24        feedback      5  \n",
       "7        mitigation      2        policies     23        scenario      5  \n",
       "8    climate crisis      2     institution     17            sink      4  \n",
       "9   decarbonization      2        evidence     16           ozone      4  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "62826592",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 26/26 [00:00<00:00, 156.07it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 26/26 [00:00<00:00, 397.29it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 26/26 [00:00<00:00, 579.35it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 26/26 [00:00<00:00, 152.84it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 26/26 [00:00<00:00, 79.72it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 26/26 [00:00<00:00, 248.38it/s]\n"
     ]
    }
   ],
   "source": [
    "data = get_most_words_used(Text_df_test[Text_df_test[\"Target\"] == \"Yes\"], EPA_Lexicon)\n",
    "EPA_words_used_Yes = pd.DataFrame.from_dict(data, orient='index', columns=['Count']).reset_index().rename(columns={'index': 'Word'})\n",
    "\n",
    "data = get_most_words_used(Text_df_test[Text_df_test[\"Target\"] == \"Yes\"], BBC_Lexicon)\n",
    "BBC_words_used_Yes = pd.DataFrame.from_dict(data, orient='index', columns=['Count']).reset_index().rename(columns={'index': 'Word'})\n",
    "\n",
    "data = get_most_words_used(Text_df_test[Text_df_test[\"Target\"] == \"Yes\"], UNDP_Lexicon)\n",
    "UNDP_words_used_Yes = pd.DataFrame.from_dict(data, orient='index', columns=['Count']).reset_index().rename(columns={'index': 'Word'})\n",
    "\n",
    "data = get_most_words_used(Text_df_test[Text_df_test[\"Target\"] == \"Yes\"], Wikipedia_Lexicon)\n",
    "Wikipedia_words_used_Yes = pd.DataFrame.from_dict(data, orient='index', columns=['Count']).reset_index().rename(columns={'index': 'Word'})\n",
    "\n",
    "data = get_most_words_used(Text_df_test[Text_df_test[\"Target\"] == \"Yes\"], IPCC_Lexicon)\n",
    "IPCC_words_used_Yes = pd.DataFrame.from_dict(data, orient='index', columns=['Count']).reset_index().rename(columns={'index': 'Word'})\n",
    "\n",
    "data = get_most_words_used(Text_df_test[Text_df_test[\"Target\"] == \"Yes\"], Global_Change_Lexicon)\n",
    "Global_Change_words_used_Yes = pd.DataFrame.from_dict(data, orient='index', columns=['Count']).reset_index().rename(columns={'index': 'Word'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4d34b855",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPA_top_Yes = EPA_words_used_Yes.sort_values(\"Count\", ascending = False).head(10).reset_index(drop = True)\n",
    "EPA_top_Yes.columns = [\"EPA\", \"Count\"]\n",
    "BBC_top_Yes = BBC_words_used_Yes.sort_values(\"Count\", ascending = False).head(10).reset_index(drop = True)\n",
    "BBC_top_Yes.columns = [\"BBC\", \"Count\"]\n",
    "Wikipedia_top_Yes = Wikipedia_words_used_Yes.sort_values(\"Count\", ascending = False).head(10).reset_index(drop = True)\n",
    "Wikipedia_top_Yes.columns = [\"Wikipedia\", \"Count\"]\n",
    "UNDP_top_Yes = UNDP_words_used_Yes.sort_values(\"Count\", ascending = False).head(10).reset_index(drop = True)\n",
    "UNDP_top_Yes.columns = [\"UNDP\", \"Count\"]\n",
    "IPCC_top_Yes = IPCC_words_used_Yes.sort_values(\"Count\", ascending = False).head(10).reset_index(drop = True)\n",
    "IPCC_top_Yes.columns = [\"IPCC\", \"Count\"]\n",
    "Global_Change_top_Yes = Global_Change_words_used_Yes.sort_values(\"Count\", ascending = False).head(10).reset_index(drop = True)\n",
    "Global_Change_top_Yes.columns = [\"Global Change\", \"Count\"]\n",
    "\n",
    "top_df_Yes = pd.concat([EPA_top_Yes, BBC_top_Yes, Wikipedia_top_Yes, UNDP_top_Yes, IPCC_top_Yes, Global_Change_top_Yes], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8bf6cad4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EPA</th>\n",
       "      <th>Count</th>\n",
       "      <th>BBC</th>\n",
       "      <th>Count</th>\n",
       "      <th>Wikipedia</th>\n",
       "      <th>Count</th>\n",
       "      <th>UNDP</th>\n",
       "      <th>Count</th>\n",
       "      <th>IPCC</th>\n",
       "      <th>Count</th>\n",
       "      <th>Global Change</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>climate</td>\n",
       "      <td>25</td>\n",
       "      <td>climate change</td>\n",
       "      <td>25</td>\n",
       "      <td>climate</td>\n",
       "      <td>25</td>\n",
       "      <td>climate</td>\n",
       "      <td>25</td>\n",
       "      <td>climate change</td>\n",
       "      <td>25</td>\n",
       "      <td>climate change</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>climate change</td>\n",
       "      <td>25</td>\n",
       "      <td>fossil fuels</td>\n",
       "      <td>6</td>\n",
       "      <td>climate change</td>\n",
       "      <td>25</td>\n",
       "      <td>climate change</td>\n",
       "      <td>25</td>\n",
       "      <td>climate</td>\n",
       "      <td>25</td>\n",
       "      <td>risk</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>emissions</td>\n",
       "      <td>13</td>\n",
       "      <td>global warming</td>\n",
       "      <td>6</td>\n",
       "      <td>fossil fuel</td>\n",
       "      <td>7</td>\n",
       "      <td>global warming</td>\n",
       "      <td>6</td>\n",
       "      <td>agreement</td>\n",
       "      <td>10</td>\n",
       "      <td>value</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fossil fuel</td>\n",
       "      <td>7</td>\n",
       "      <td>renewable energy</td>\n",
       "      <td>4</td>\n",
       "      <td>global warming</td>\n",
       "      <td>6</td>\n",
       "      <td>weather</td>\n",
       "      <td>6</td>\n",
       "      <td>risk</td>\n",
       "      <td>10</td>\n",
       "      <td>global warming</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>global warming</td>\n",
       "      <td>6</td>\n",
       "      <td>methane</td>\n",
       "      <td>3</td>\n",
       "      <td>weather</td>\n",
       "      <td>6</td>\n",
       "      <td>renewable energy</td>\n",
       "      <td>4</td>\n",
       "      <td>region</td>\n",
       "      <td>8</td>\n",
       "      <td>scenario</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>weather</td>\n",
       "      <td>6</td>\n",
       "      <td>greenhouse gases</td>\n",
       "      <td>2</td>\n",
       "      <td>greenhouse gas</td>\n",
       "      <td>5</td>\n",
       "      <td>greenhouse gases</td>\n",
       "      <td>2</td>\n",
       "      <td>policies</td>\n",
       "      <td>7</td>\n",
       "      <td>uncertainty</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>greenhouse gas</td>\n",
       "      <td>5</td>\n",
       "      <td>mitigation</td>\n",
       "      <td>2</td>\n",
       "      <td>methane</td>\n",
       "      <td>3</td>\n",
       "      <td>decarbonization</td>\n",
       "      <td>2</td>\n",
       "      <td>fossil fuels</td>\n",
       "      <td>6</td>\n",
       "      <td>evolution</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>renewable energy</td>\n",
       "      <td>4</td>\n",
       "      <td>carbon dioxide</td>\n",
       "      <td>2</td>\n",
       "      <td>argo</td>\n",
       "      <td>2</td>\n",
       "      <td>mitigation</td>\n",
       "      <td>2</td>\n",
       "      <td>model</td>\n",
       "      <td>6</td>\n",
       "      <td>greenhouse gases</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>natural gas</td>\n",
       "      <td>4</td>\n",
       "      <td>hockey stick</td>\n",
       "      <td>1</td>\n",
       "      <td>ozone</td>\n",
       "      <td>2</td>\n",
       "      <td>paris agreement</td>\n",
       "      <td>2</td>\n",
       "      <td>global warming</td>\n",
       "      <td>6</td>\n",
       "      <td>mitigation</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>atmosphere</td>\n",
       "      <td>4</td>\n",
       "      <td>carbon neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>carbon dioxide</td>\n",
       "      <td>2</td>\n",
       "      <td>tipping point</td>\n",
       "      <td>1</td>\n",
       "      <td>greenhouse gas</td>\n",
       "      <td>5</td>\n",
       "      <td>ozone</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                EPA  Count               BBC  Count       Wikipedia  Count  \\\n",
       "0           climate     25    climate change     25         climate     25   \n",
       "1    climate change     25      fossil fuels      6  climate change     25   \n",
       "2         emissions     13    global warming      6     fossil fuel      7   \n",
       "3       fossil fuel      7  renewable energy      4  global warming      6   \n",
       "4    global warming      6           methane      3         weather      6   \n",
       "5           weather      6  greenhouse gases      2  greenhouse gas      5   \n",
       "6    greenhouse gas      5        mitigation      2         methane      3   \n",
       "7  renewable energy      4    carbon dioxide      2            argo      2   \n",
       "8       natural gas      4      hockey stick      1           ozone      2   \n",
       "9        atmosphere      4    carbon neutral      1  carbon dioxide      2   \n",
       "\n",
       "               UNDP  Count            IPCC  Count     Global Change  Count  \n",
       "0           climate     25  climate change     25    climate change     25  \n",
       "1    climate change     25         climate     25              risk     10  \n",
       "2    global warming      6       agreement     10             value      8  \n",
       "3           weather      6            risk     10    global warming      6  \n",
       "4  renewable energy      4          region      8          scenario      4  \n",
       "5  greenhouse gases      2        policies      7       uncertainty      3  \n",
       "6   decarbonization      2    fossil fuels      6         evolution      3  \n",
       "7        mitigation      2           model      6  greenhouse gases      2  \n",
       "8   paris agreement      2  global warming      6        mitigation      2  \n",
       "9     tipping point      1  greenhouse gas      5             ozone      2  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_df_Yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c8b6eb53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 124/124 [00:00<00:00, 150.21it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 124/124 [00:00<00:00, 371.74it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 124/124 [00:00<00:00, 500.52it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 124/124 [00:00<00:00, 143.70it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 124/124 [00:01<00:00, 70.85it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 124/124 [00:00<00:00, 227.80it/s]\n"
     ]
    }
   ],
   "source": [
    "data = get_most_words_used(Text_df_test[Text_df_test[\"Target\"] == \"No\"], EPA_Lexicon)\n",
    "EPA_words_used_No = pd.DataFrame.from_dict(data, orient='index', columns=['Count']).reset_index().rename(columns={'index': 'Word'})\n",
    "\n",
    "data = get_most_words_used(Text_df_test[Text_df_test[\"Target\"] == \"No\"], BBC_Lexicon)\n",
    "BBC_words_used_No = pd.DataFrame.from_dict(data, orient='index', columns=['Count']).reset_index().rename(columns={'index': 'Word'})\n",
    "\n",
    "data = get_most_words_used(Text_df_test[Text_df_test[\"Target\"] == \"No\"], UNDP_Lexicon)\n",
    "UNDP_words_used_No = pd.DataFrame.from_dict(data, orient='index', columns=['Count']).reset_index().rename(columns={'index': 'Word'})\n",
    "\n",
    "data = get_most_words_used(Text_df_test[Text_df_test[\"Target\"] == \"No\"], Wikipedia_Lexicon)\n",
    "Wikipedia_words_used_No = pd.DataFrame.from_dict(data, orient='index', columns=['Count']).reset_index().rename(columns={'index': 'Word'})\n",
    "\n",
    "data = get_most_words_used(Text_df_test[Text_df_test[\"Target\"] == \"No\"], IPCC_Lexicon)\n",
    "IPCC_words_used_No = pd.DataFrame.from_dict(data, orient='index', columns=['Count']).reset_index().rename(columns={'index': 'Word'})\n",
    "\n",
    "data = get_most_words_used(Text_df_test[Text_df_test[\"Target\"] == \"No\"], Global_Change_Lexicon)\n",
    "Global_Change_words_used_No = pd.DataFrame.from_dict(data, orient='index', columns=['Count']).reset_index().rename(columns={'index': 'Word'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5a9207b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPA_top_No = EPA_words_used_No.sort_values(\"Count\", ascending = False).head(10).reset_index(drop = True)\n",
    "EPA_top_No.columns = [\"EPA\", \"Count\"]\n",
    "BBC_top_No = BBC_words_used_No.sort_values(\"Count\", ascending = False).head(10).reset_index(drop = True)\n",
    "BBC_top_No.columns = [\"BBC\", \"Count\"]\n",
    "Wikipedia_top_No = Wikipedia_words_used_No.sort_values(\"Count\", ascending = False).head(10).reset_index(drop = True)\n",
    "Wikipedia_top_No.columns = [\"Wikipedia\", \"Count\"]\n",
    "UNDP_top_No = UNDP_words_used_No.sort_values(\"Count\", ascending = False).head(10).reset_index(drop = True)\n",
    "UNDP_top_No.columns = [\"UNDP\", \"Count\"]\n",
    "IPCC_top_No = IPCC_words_used_No.sort_values(\"Count\", ascending = False).head(10).reset_index(drop = True)\n",
    "IPCC_top_No.columns = [\"IPCC\", \"Count\"]\n",
    "Global_Change_top_No = Global_Change_words_used_No.sort_values(\"Count\", ascending = False).head(10).reset_index(drop = True)\n",
    "Global_Change_top_No.columns = [\"Global Change\", \"Count\"]\n",
    "\n",
    "top_df_No = pd.concat([EPA_top_No, BBC_top_No, Wikipedia_top_No, UNDP_top_No, IPCC_top_No, Global_Change_top_No], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "bd20d8b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EPA</th>\n",
       "      <th>Count</th>\n",
       "      <th>BBC</th>\n",
       "      <th>Count</th>\n",
       "      <th>Wikipedia</th>\n",
       "      <th>Count</th>\n",
       "      <th>UNDP</th>\n",
       "      <th>Count</th>\n",
       "      <th>IPCC</th>\n",
       "      <th>Count</th>\n",
       "      <th>Global Change</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>climate</td>\n",
       "      <td>37</td>\n",
       "      <td>climate change</td>\n",
       "      <td>34.0</td>\n",
       "      <td>climate</td>\n",
       "      <td>37</td>\n",
       "      <td>climate</td>\n",
       "      <td>37</td>\n",
       "      <td>climate</td>\n",
       "      <td>37</td>\n",
       "      <td>climate change</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>climate change</td>\n",
       "      <td>34</td>\n",
       "      <td>adaptation</td>\n",
       "      <td>2.0</td>\n",
       "      <td>climate change</td>\n",
       "      <td>34</td>\n",
       "      <td>climate change</td>\n",
       "      <td>34</td>\n",
       "      <td>climate change</td>\n",
       "      <td>34</td>\n",
       "      <td>risk</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>weather</td>\n",
       "      <td>4</td>\n",
       "      <td>renewable energy</td>\n",
       "      <td>1.0</td>\n",
       "      <td>weather</td>\n",
       "      <td>4</td>\n",
       "      <td>weather</td>\n",
       "      <td>4</td>\n",
       "      <td>risk</td>\n",
       "      <td>28</td>\n",
       "      <td>value</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>concentration</td>\n",
       "      <td>3</td>\n",
       "      <td>feedback loop</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ozone</td>\n",
       "      <td>2</td>\n",
       "      <td>transparency</td>\n",
       "      <td>3</td>\n",
       "      <td>social</td>\n",
       "      <td>24</td>\n",
       "      <td>evolution</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sink</td>\n",
       "      <td>3</td>\n",
       "      <td>global warming</td>\n",
       "      <td>1.0</td>\n",
       "      <td>argo</td>\n",
       "      <td>2</td>\n",
       "      <td>adaptation</td>\n",
       "      <td>2</td>\n",
       "      <td>agreement</td>\n",
       "      <td>20</td>\n",
       "      <td>uncertainty</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ozone</td>\n",
       "      <td>2</td>\n",
       "      <td>fossil fuels</td>\n",
       "      <td>1.0</td>\n",
       "      <td>adaptation</td>\n",
       "      <td>2</td>\n",
       "      <td>resilience</td>\n",
       "      <td>1</td>\n",
       "      <td>model</td>\n",
       "      <td>18</td>\n",
       "      <td>feedback</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>vulnerability</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>climate crisis</td>\n",
       "      <td>1</td>\n",
       "      <td>climate crisis</td>\n",
       "      <td>1</td>\n",
       "      <td>policies</td>\n",
       "      <td>16</td>\n",
       "      <td>sink</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>metric ton</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>proxy</td>\n",
       "      <td>1</td>\n",
       "      <td>reforestation</td>\n",
       "      <td>1</td>\n",
       "      <td>region</td>\n",
       "      <td>16</td>\n",
       "      <td>fitness</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>adaptation</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>carbon tax</td>\n",
       "      <td>1</td>\n",
       "      <td>renewable energy</td>\n",
       "      <td>1</td>\n",
       "      <td>justice</td>\n",
       "      <td>13</td>\n",
       "      <td>ozone</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>energy efficiency</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>global warming</td>\n",
       "      <td>1</td>\n",
       "      <td>feedback loop</td>\n",
       "      <td>1</td>\n",
       "      <td>evidence</td>\n",
       "      <td>13</td>\n",
       "      <td>vulnerability</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 EPA  Count               BBC  Count       Wikipedia  Count  \\\n",
       "0            climate     37    climate change   34.0         climate     37   \n",
       "1     climate change     34        adaptation    2.0  climate change     34   \n",
       "2            weather      4  renewable energy    1.0         weather      4   \n",
       "3      concentration      3     feedback loop    1.0           ozone      2   \n",
       "4               sink      3    global warming    1.0            argo      2   \n",
       "5              ozone      2      fossil fuels    1.0      adaptation      2   \n",
       "6      vulnerability      2               NaN    NaN  climate crisis      1   \n",
       "7         metric ton      2               NaN    NaN           proxy      1   \n",
       "8         adaptation      2               NaN    NaN      carbon tax      1   \n",
       "9  energy efficiency      2               NaN    NaN  global warming      1   \n",
       "\n",
       "               UNDP  Count            IPCC  Count   Global Change  Count  \n",
       "0           climate     37         climate     37  climate change     34  \n",
       "1    climate change     34  climate change     34            risk     28  \n",
       "2           weather      4            risk     28           value     18  \n",
       "3      transparency      3          social     24       evolution     10  \n",
       "4        adaptation      2       agreement     20     uncertainty      9  \n",
       "5        resilience      1           model     18        feedback      5  \n",
       "6    climate crisis      1        policies     16            sink      3  \n",
       "7     reforestation      1          region     16         fitness      3  \n",
       "8  renewable energy      1         justice     13           ozone      2  \n",
       "9     feedback loop      1        evidence     13   vulnerability      2  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_df_No"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209ff76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IPCC_UNDP_BBC\n",
    "Lexicon_cross = pd.concat([EPA_Lexicon, BBC_Lexicon]).drop_duplicates()\n",
    "get_cross_table(Text_df_test, Lexicon_cross, 7, \"Absolute Frequency\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5913769",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_cross_table(Text_df_test, EPA_Lexicon, 0.6, \"Relative Frequency\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6a6ca1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7a57a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bert_test_result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602d8cad",
   "metadata": {},
   "source": [
    "# 5. Classify Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461db6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "WP = pd.read_parquet(\"C:/Users/Boedt/OneDrive/Bureaublad/Scraped_Articles/Final/WP_Final_Articles.parquet\")\n",
    "WP_clean = WP.copy()\n",
    "WP_clean[\"Text\"] = WP_clean[\"Text\"].apply(preprocess_text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5367f0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "WSJ = pd.read_parquet(\"C:/Users/Boedt/OneDrive/Bureaublad/Scraped_Articles/Final/WSJ_Final\")\n",
    "WSJ_clean = WSJ.copy()\n",
    "WSJ_clean[\"Text\"] = WSJ_clean[\"Text\"].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bf495e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lexicon = pd.concat([EPA_Lexicon, BBC_Lexicon]).drop_duplicates()\n",
    "test = count_lexicon_words(WSJ_clean, Lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7463261",
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"Climate\"] = \"No\"\n",
    "test.loc[test[\"Absolute Frequency\"] >= 7, \"Climate\"] = \"Yes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af3ce84",
   "metadata": {},
   "outputs": [],
   "source": [
    "WSJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37143101",
   "metadata": {},
   "outputs": [],
   "source": [
    "WSJ[\"Text\"] = WSJ[\"Text\"].apply(text_cleaning_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0785b0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "WSJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22f9518",
   "metadata": {},
   "outputs": [],
   "source": [
    "WSJ[WSJ[\"Year\"] == 2020].reset_index(drop = True).to_parquet(\"C:/Users/Boedt/OneDrive/Bureaublad/Scraped_Articles/Final/WSJ_Final_2020.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1167d365",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
